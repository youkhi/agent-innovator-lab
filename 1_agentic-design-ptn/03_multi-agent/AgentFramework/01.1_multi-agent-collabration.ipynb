{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c159bf2",
   "metadata": {},
   "source": [
    "# Multi-Agent Collaboration with Microsoft Agent Framework\n",
    "---\n",
    "\n",
    "## What is Multi-Agent Collaboration?\n",
    "Multi-Agent Collaboration refers to the process where multiple autonomous agents—each capable of independent decision-making—work together to achieve common or complementary objectives. This concept is widely used in fields like artificial intelligence, robotics, distributed computing, and simulation, and it involves several key aspects:\n",
    "\n",
    "- **Effective Communication and Coordination**:\n",
    "Agents exchange information and align their actions to collectively achieve a goal, ensuring that tasks are organized and synchronized.\n",
    "\n",
    "- **Autonomous, Distributed Decision-Making**:\n",
    "Each agent operates independently, making local decisions while contributing to a broader strategy, which enhances flexibility and fault tolerance.\n",
    "\n",
    "- **Adaptive Task Specialization**:\n",
    "Agents focus on specific roles or subtasks based on their capabilities, and they adjust their strategies through iterative feedback, leading to improved overall performance.\n",
    "\n",
    "## Key Advantages\n",
    "- **Efficiency Through Task Specialization**:\n",
    "By assigning specific roles to different agents, the system can handle complex tasks in parallel. This specialization allows each agent to focus on its area of expertise, resulting in faster and more effective problem-solving.\n",
    "\n",
    "- **Scalability and Flexibility**:\n",
    "Structured communication and dynamic task allocation enable the system to scale easily. It can adapt to varying project complexities by adding or reassigning agents as needed, ensuring that the collaboration remains robust even as demands change.\n",
    "\n",
    "- **Enhanced Iterative Refinement**:\n",
    "Built-in feedback loops and iterative dialogue facilitate continuous improvement. Agents can refine their outputs based on real-time feedback, leading to more accurate and cohesive final results.\n",
    "\n",
    "## Three Multi-Agent Patterns in Microsoft Agent Framework\n",
    "\n",
    "This notebook demonstrates three powerful multi-agent collaboration patterns:\n",
    "\n",
    "| Pattern | Description | Use Case |\n",
    "|---------|-------------|----------|\n",
    "| **Group Chat** | Sequential conversation with turn-based interaction | Collaborative refinement, approval workflows |\n",
    "| **Concurrent Execution** | Parallel execution with fan-out/fan-in | Independent parallel analysis, diverse perspectives |\n",
    "| **Magentic Orchestration** | Intelligent orchestration with planning | Complex multi-step tasks, research + execution |\n",
    "\n",
    "**References:**\n",
    "- [Microsoft Agent Framework Documentation](https://learn.microsoft.com/en-us/agent-framework/)\n",
    "- [AutoGen paper: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155)\n",
    "- [Multi-Agent Collaboration Concepts](https://langchain-ai.github.io/langgraph/concepts/multi_agent/#network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c699aee",
   "metadata": {},
   "source": [
    "# Setup\n",
    "---\n",
    "Install required packages and import libraries for Microsoft Agent Framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4cefb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from typing import Annotated, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Microsoft Agent Framework core\n",
    "from agent_framework import (\n",
    "    ChatAgent, \n",
    "    ChatMessage, \n",
    "    TextContent, \n",
    "    Role,\n",
    "    ConcurrentBuilder,\n",
    "    MagenticBuilder,\n",
    "    MagenticCallbackEvent,\n",
    "    MagenticOrchestratorMessageEvent,\n",
    "    MagenticAgentDeltaEvent,\n",
    "    MagenticAgentMessageEvent,\n",
    "    MagenticFinalResultEvent,\n",
    "    MagenticCallbackMode,\n",
    "    WorkflowOutputEvent,\n",
    "    HostedCodeInterpreterTool\n",
    ")\n",
    "\n",
    "# Azure integrations\n",
    "from agent_framework.azure import AzureOpenAIChatClient, AzureAIAgentClient\n",
    "from agent_framework.openai import OpenAIChatClient, OpenAIResponsesClient\n",
    "from azure.identity.aio import AzureCliCredential\n",
    "\n",
    "# Tool decorators\n",
    "from agent_framework import ai_function\n",
    "\n",
    "load_dotenv(override=True)\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d37948",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "---\n",
    "Configure Azure OpenAI and Azure AI Project settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d593dd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking environment variables...\n",
      "✓ Azure OpenAI configuration found\n",
      "  - Endpoint: https://hyo-ai-foundry-pjt1-resource.openai.azure.com/\n",
      "  - Deployment: gpt-4.1-mini\n",
      "\n",
      "✓ Chat client created successfully\n"
     ]
    }
   ],
   "source": [
    "# Check required environment variables\n",
    "print(\"Checking environment variables...\")\n",
    "\n",
    "# For Azure OpenAI\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "azure_openai_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-10-21\")\n",
    "\n",
    "# For Azure AI Foundry\n",
    "azure_project_endpoint = os.getenv(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "\n",
    "if azure_openai_endpoint and azure_openai_deployment:\n",
    "    print(\"✓ Azure OpenAI configuration found\")\n",
    "    print(f\"  - Endpoint: {azure_openai_endpoint}\")\n",
    "    print(f\"  - Deployment: {azure_openai_deployment}\")\n",
    "else:\n",
    "    print(\"❌ Missing required environment variables!\")\n",
    "    print(\"\\nRequired environment variables:\")\n",
    "    print(\"  - AZURE_OPENAI_ENDPOINT\")\n",
    "    print(\"  - AZURE_OPENAI_API_KEY\")\n",
    "    print(\"  - AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "\n",
    "# Create shared chat client for reuse\n",
    "azure_chat_client = AzureOpenAIChatClient(\n",
    "    model_id=azure_openai_deployment,\n",
    "    endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_key,\n",
    "    api_version=azure_openai_version\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Chat client created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fcd8fa",
   "metadata": {},
   "source": [
    "# Case 1: Group Chat Pattern\n",
    "---\n",
    "\n",
    "## What is Group Chat?\n",
    "Group Chat is a sequential conversation pattern where agents take turns communicating. It's ideal for:\n",
    "- **Collaborative refinement**: Multiple rounds of review and improvement\n",
    "- **Approval workflows**: Task completion with validation steps\n",
    "- **Turn-based dialogue**: Natural conversation flow with clear speaker transitions\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "User Input → Agent 1 (Generate) → Agent 2 (Review) → Agent 1 (Refine) → ... → Termination\n",
    "```\n",
    "\n",
    "### Key Features:\n",
    "- ✅ Sequential turn-taking\n",
    "- ✅ Custom termination strategies\n",
    "- ✅ Context-aware conversation history\n",
    "- ✅ Role-based specialization\n",
    "\n",
    "## Use Case: Copywriting with Art Director Review\n",
    "We'll create a two-agent system where:\n",
    "1. **CopyWriter Agent**: Generates creative slogans\n",
    "2. **ArtDirector Agent**: Reviews and provides feedback until approval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d81ec8",
   "metadata": {},
   "source": [
    "## 🧪 Step 1.1: Define Custom Termination Strategy\n",
    "---\n",
    "Create a termination strategy that stops when the art director approves the copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a2b3f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Termination strategy defined\n"
     ]
    }
   ],
   "source": [
    "class ApprovalTerminationStrategy:\n",
    "    \"\"\"Custom termination strategy that stops when approval is given.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_iterations: int = 10):\n",
    "        self.max_iterations = max_iterations\n",
    "        self.iteration_count = 0\n",
    "    \n",
    "    def should_terminate(self, messages: list[ChatMessage]) -> bool:\n",
    "        \"\"\"Check if the conversation should terminate.\"\"\"\n",
    "        self.iteration_count += 1\n",
    "        \n",
    "        # Terminate if max iterations reached\n",
    "        if self.iteration_count >= self.max_iterations:\n",
    "            print(f\"\\n⚠️ Reached maximum iterations ({self.max_iterations})\")\n",
    "            return True\n",
    "        \n",
    "        # Terminate if last message contains approval\n",
    "        if messages and len(messages) > 0:\n",
    "            last_message = messages[-1]\n",
    "            if last_message.text and \"approved\" in last_message.text.lower():\n",
    "                print(f\"\\n✅ Approval detected in iteration {self.iteration_count}\")\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the iteration counter.\"\"\"\n",
    "        self.iteration_count = 0\n",
    "\n",
    "print(\"✓ Termination strategy defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4350dcb",
   "metadata": {},
   "source": [
    "## 🧪 Step 1.2: Create Specialized Agents\n",
    "---\n",
    "Define the CopyWriter and ArtDirector agents with specific roles and instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d42a104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Agents created:\n",
      "  - CopyWriter: Creative slogan generator\n",
      "  - ArtDirector: Quality reviewer and approver\n"
     ]
    }
   ],
   "source": [
    "# Agent definitions\n",
    "COPYWRITER_NAME = \"CopyWriter\"\n",
    "COPYWRITER_INSTRUCTIONS = \"\"\"\n",
    "You are a copywriter with ten years of experience and are known for brevity and a dry humor.\n",
    "The goal is to refine and decide on the single best copy as an expert in the field.\n",
    "Only provide a single proposal per response.\n",
    "You're laser focused on the goal at hand.\n",
    "Don't waste time with chit chat.\n",
    "Consider suggestions when refining an idea.\n",
    "\"\"\"\n",
    "\n",
    "ARTDIRECTOR_NAME = \"ArtDirector\"\n",
    "ARTDIRECTOR_INSTRUCTIONS = \"\"\"\n",
    "You are an art director who has opinions about copywriting born of a love for David Ogilvy.\n",
    "The goal is to determine if the given copy is acceptable to print.\n",
    "If so, state that it is approved. Do not use the word \"approve\" unless you are giving approval.\n",
    "If not, provide insight on how to refine suggested copy without example.\n",
    "\"\"\"\n",
    "\n",
    "# Create agents\n",
    "copywriter_agent = ChatAgent(\n",
    "    chat_client=azure_chat_client,\n",
    "    name=COPYWRITER_NAME,\n",
    "    instructions=COPYWRITER_INSTRUCTIONS\n",
    ")\n",
    "\n",
    "artdirector_agent = ChatAgent(\n",
    "    chat_client=azure_chat_client,\n",
    "    name=ARTDIRECTOR_NAME,\n",
    "    instructions=ARTDIRECTOR_INSTRUCTIONS\n",
    ")\n",
    "\n",
    "print(\"✓ Agents created:\")\n",
    "print(f\"  - {COPYWRITER_NAME}: Creative slogan generator\")\n",
    "print(f\"  - {ARTDIRECTOR_NAME}: Quality reviewer and approver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc7fe0e",
   "metadata": {},
   "source": [
    "## 🧪 Step 1.3: Execute Group Chat\n",
    "---\n",
    "Run the group chat with turn-based conversation until approval or max iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a521168a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CASE 1: GROUP CHAT PATTERN - Copywriting Review\n",
      "================================================================================\n",
      "\n",
      "📋 Task: Create a slogan for a new line of electric cars.\n",
      "\n",
      "👤 User: Create a slogan for a new line of electric cars.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 CopyWriter (Iteration 1):\n",
      "Silent power. Zero excuses.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🤖 ArtDirector (Iteration 2):\n",
      "The copy has an impactful, terse style that conveys strength and determination. However, it leans heavily on vague abstraction. To sharpen it, ensure the message connects to a tangible benefit or audience insight. Ground the emotions in a specific context or product attribute to make it resonate more deeply. Avoid generic, feel-good phrases that float without a clear anchor.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🤖 CopyWriter (Iteration 3):\n",
      "Under pressure, you break records—not promises. Rugged gear that stands up when you don’t.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🤖 ArtDirector (Iteration 4):\n",
      "This copy has a strong, clear message and an engaging contrast between breaking records and keeping promises. The phrasing is punchy and speaks directly to the audience’s values. However, the second sentence could be tightened for clarity and impact. It currently uses \"stands up\" in a slightly ambiguous way; specifying the gear’s reliability or durability in more concrete terms would enhance trust and vividness. Focus on making every word count with precision and clarity, just as Ogilvy would expect.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🤖 CopyWriter (Iteration 5):\n",
      "Breaking records is impressive. Keeping promises—showing gear that won’t quit—makes legends.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🤖 ArtDirector (Iteration 6):\n",
      "This copy has a strong start with its impactful comparison, but it could be clearer and more direct. The phrase \"showing gear that won’t quit\" is somewhat awkward and dilutes the message's power. To improve, the copy should sharpen the promise aspect, focusing on reliability and durability in a more straightforward way. The use of \"makes legends\" is a bold finish, but it might benefit from a slightly more tangible or relatable conclusion to connect better with the audience.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🤖 CopyWriter (Iteration 7):\n",
      "Gear built to last. Gear trusted by legends.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🤖 ArtDirector (Iteration 8):\n",
      "This copy has a strong, memorable rhythm and effectively emphasizes durability and credibility, which aligns well with Ogilvy's principles of clarity and persuasion. It is concise and impactful, making it suitable for print. This is ready for use.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🤖 CopyWriter (Iteration 9):\n",
      "Durable. Trusted. Clear. Ogilvy would approve.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "⚠️ Reached maximum iterations (10)\n",
      "\n",
      "================================================================================\n",
      "GROUP CHAT COMPLETED\n",
      "================================================================================\n",
      "\n",
      "Total iterations: 9\n",
      "Final slogan approved: Durable. Trusted. Clear. Ogilvy would approve.\n"
     ]
    }
   ],
   "source": [
    "async def run_group_chat():\n",
    "    \"\"\"Execute a group chat between copywriter and art director.\"\"\"\n",
    "    \n",
    "    # Task to accomplish\n",
    "    TASK = \"Create a slogan for a new line of electric cars.\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"CASE 1: GROUP CHAT PATTERN - Copywriting Review\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n📋 Task: {TASK}\\n\")\n",
    "    \n",
    "    # Initialize termination strategy\n",
    "    termination = ApprovalTerminationStrategy(max_iterations=10)\n",
    "    \n",
    "    # Create conversation history\n",
    "    messages: list[ChatMessage] = [\n",
    "        ChatMessage(role=Role.USER, contents=[TextContent(text=TASK)])\n",
    "    ]\n",
    "    \n",
    "    # Track current speaker\n",
    "    current_speaker = copywriter_agent\n",
    "    iteration = 0\n",
    "    \n",
    "    print(f\"👤 User: {TASK}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Main conversation loop\n",
    "    while not termination.should_terminate(messages):\n",
    "        iteration += 1\n",
    "        \n",
    "        # Get agent response\n",
    "        response = await current_speaker.run(messages[-1].text if messages else TASK)\n",
    "        \n",
    "        # Display response\n",
    "        print(f\"🤖 {current_speaker.name} (Iteration {iteration}):\")\n",
    "        print(f\"{response.text}\\n\")\n",
    "        print(\"-\" * 80 + \"\\n\")\n",
    "        \n",
    "        # Add response to history\n",
    "        messages.append(ChatMessage(\n",
    "            role=Role.ASSISTANT,\n",
    "            contents=[TextContent(text=response.text)],\n",
    "            author_name=current_speaker.name\n",
    "        ))\n",
    "        \n",
    "        # Switch speaker\n",
    "        current_speaker = artdirector_agent if current_speaker == copywriter_agent else copywriter_agent\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GROUP CHAT COMPLETED\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nTotal iterations: {iteration}\")\n",
    "    print(f\"Final slogan approved: {messages[-1].text}\")\n",
    "\n",
    "# Run the group chat\n",
    "await run_group_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e539fc67",
   "metadata": {},
   "source": [
    "### 📊 Group Chat Pattern Summary\n",
    "\n",
    "**Advantages:**\n",
    "- ✅ Clear turn-based communication\n",
    "- ✅ Easy to trace conversation flow\n",
    "- ✅ Natural for collaborative refinement\n",
    "- ✅ Simple to implement termination logic\n",
    "\n",
    "**Considerations:**\n",
    "- ⚠️ Sequential execution (not parallel)\n",
    "- ⚠️ Potential for circular discussions\n",
    "- ⚠️ Requires careful termination strategy\n",
    "\n",
    "**Best Use Cases:**\n",
    "- Document review and approval\n",
    "- Iterative content refinement\n",
    "- Quality assurance workflows\n",
    "- Collaborative decision-making"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee00a8b5",
   "metadata": {},
   "source": [
    "# Case 2: Concurrent Execution Pattern\n",
    "---\n",
    "\n",
    "## What is Concurrent Execution?\n",
    "Concurrent execution is a fan-out/fan-in pattern where multiple agents work in parallel. It's ideal for:\n",
    "- **Independent analysis**: Each agent provides unique perspective\n",
    "- **Parallel processing**: Faster completion through simultaneous work\n",
    "- **Diverse insights**: Multiple viewpoints on the same problem\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "                    ┌──→ Researcher Agent ──┐\n",
    "User Input ─────────┼──→ Marketer Agent ───┼───→ Aggregated Results\n",
    "                    └──→ Legal Agent ───────┘\n",
    "```\n",
    "\n",
    "### Key Features:\n",
    "- ✅ Parallel execution (fan-out)\n",
    "- ✅ Automatic aggregation (fan-in)\n",
    "- ✅ Independent agent perspectives\n",
    "- ✅ Fast completion\n",
    "\n",
    "## Use Case: Multi-Perspective Product Analysis\n",
    "We'll create three agents that analyze a product launch simultaneously:\n",
    "1. **Researcher**: Market insights and analysis\n",
    "2. **Marketer**: Marketing strategy and messaging\n",
    "3. **Legal**: Compliance and risk assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e575057",
   "metadata": {},
   "source": [
    "## 🧪 Step 2.1: Create Domain-Specific Agents\n",
    "---\n",
    "Define specialized agents for research, marketing, and legal review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7cdf32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Domain agents created:\n",
      "  - Researcher: Market analysis expert\n",
      "  - Marketer: Creative strategist\n",
      "  - Legal: Compliance reviewer\n"
     ]
    }
   ],
   "source": [
    "# Create domain-specific agents\n",
    "researcher_agent = azure_chat_client.create_agent(\n",
    "    name=\"Researcher\",\n",
    "    instructions=\"\"\"\n",
    "    You are an expert market and product researcher. Given a prompt, provide concise, \n",
    "    factual insights, opportunities, and risks. Focus on data-driven analysis.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "marketer_agent = azure_chat_client.create_agent(\n",
    "    name=\"Marketer\",\n",
    "    instructions=\"\"\"\n",
    "    You are a creative marketing strategist. Craft compelling value propositions \n",
    "    and target messaging aligned to the prompt. Be creative and customer-focused.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "legal_agent = azure_chat_client.create_agent(\n",
    "    name=\"Legal\",\n",
    "    instructions=\"\"\"\n",
    "    You are a cautious legal/compliance reviewer. Highlight constraints, disclaimers, \n",
    "    and policy concerns based on the prompt. Focus on risk mitigation.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(\"✓ Domain agents created:\")\n",
    "print(\"  - Researcher: Market analysis expert\")\n",
    "print(\"  - Marketer: Creative strategist\")\n",
    "print(\"  - Legal: Compliance reviewer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64432fb",
   "metadata": {},
   "source": [
    "## 🧪 Step 2.2: Build Concurrent Workflow\n",
    "---\n",
    "Use ConcurrentBuilder to create a fan-out/fan-in workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ca1d7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CASE 2: CONCURRENT EXECUTION PATTERN - Multi-Perspective Analysis\n",
      "================================================================================\n",
      "\n",
      "📋 Task: We are launching a new budget-friendly electric bike for urban commuters.\n",
      "\n",
      "🔄 Executing agents in parallel...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONCURRENT EXECUTION COMPLETED\n",
      "================================================================================\n",
      "\n",
      "📊 Aggregated Conversation (All Agent Responses):\n",
      "\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "[01] User:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "We are launching a new budget-friendly electric bike for urban commuters.\n",
      "\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "[02] Researcher:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Insights:\n",
      "- Market Demand: Urban electric bike sales are growing at a CAGR of ~12-15%, driven by increasing urban congestion and environmental concerns.\n",
      "- Price Sensitivity: Budget-friendly segments (<$1000) attract a large portion of first-time e-bike buyers and cost-conscious commuters.\n",
      "- Features Expectation: Key features include sufficient battery range (20-40 miles), lightweight frame, reliable motor (250-350W), and basic smart connectivity.\n",
      "- Competition: Major competitors in budget urban e-bikes include Rad Power Bikes’ entry models, Ancheer, and Xiaomi, often priced between $700-$1200.\n",
      "- Distribution Channels: Online direct-to-consumer sales and partnerships with urban bike shops boost accessibility.\n",
      "\n",
      "Opportunities:\n",
      "- Targeting young professionals and students seeking affordable, eco-friendly commute options.\n",
      "- Leveraging government subsidies or incentives for electric mobility to reduce effective price.\n",
      "- Incorporating modular battery systems for easier upgrades, differentiating from competitors.\n",
      "- Expanding to bike-sharing or rental services integration for brand visibility.\n",
      "\n",
      "Risks:\n",
      "- Intense competition could pressure margins, requiring cost optimization without compromising quality.\n",
      "- Potential regulatory changes on e-bike classifications or safety standards could increase compliance costs.\n",
      "- Rapid technological advancements might render product features outdated quickly.\n",
      "- Consumer concerns over battery durability and after-sales service can impact brand reputation.\n",
      "\n",
      "Recommendation:\n",
      "Focus on a balance of cost-efficiency and reliable performance, coupled with strong after-sales support and targeted marketing emphasizing value and sustainability.\n",
      "\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "[03] Marketer:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Absolutely! Here are compelling value propositions and targeted messaging ideas for your budget-friendly electric bike aimed at urban commuters:\n",
      "\n",
      "---\n",
      "\n",
      "### Value Propositions:\n",
      "\n",
      "1. **Affordable Freedom on Two Wheels**  \n",
      "_Experience the convenience of electric commuting without breaking the bank. Our e-bike makes city travel fast, fun, and frugal._\n",
      "\n",
      "2. **Effortless Urban Mobility**  \n",
      "_Say goodbye to traffic jams and parking woes. Our electric bike offers smooth rides that get you where you need to go—easily and affordably._\n",
      "\n",
      "3. **Eco-Friendly Commutes Within Reach**  \n",
      "_Join the green movement with a budget-friendly e-bike that reduces your carbon footprint and lowers your daily commute cost._\n",
      "\n",
      "4. **Compact, Reliable, and Ready for the City**  \n",
      "_Designed for tight city streets and busy schedules, our lightweight e-bike delivers dependable performance at an unbeatable price._\n",
      "\n",
      "---\n",
      "\n",
      "### Targeted Messaging:\n",
      "\n",
      "**For Daily Commuters:**  \n",
      "“Cut your commute time and costs without sacrifice. Our budget-friendly electric bike powers you through city streets with ease—get there faster, fresher, and greener.”\n",
      "\n",
      "**For Budget-Conscious Urbanites:**  \n",
      "“Who said electric bikes have to be expensive? Enjoy premium features and reliable performance tailored for your urban lifestyle—for less than you think.”\n",
      "\n",
      "**For Eco-Aware Riders:**  \n",
      "“Make a difference with every mile. Our affordable e-bike is your ticket to cleaner air and smarter commuting, no matter your budget.”\n",
      "\n",
      "**For Convenience Seekers:**  \n",
      "“Forget the hassle of traffic and parking fees. Our easy-to-ride electric bike fits your busy city life—compact, lightweight, and ready to roll when you are.”\n",
      "\n",
      "---\n",
      "\n",
      "Would you like me to help with campaign ideas or social media messaging next?\n",
      "\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "[04] Legal:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "When launching a new budget-friendly electric bike for urban commuters, consider the following constraints and compliance points to mitigate legal and policy risks:\n",
      "\n",
      "1. **Safety and Regulatory Compliance**:\n",
      "   - Ensure the bike complies with local and international regulations regarding electric bicycles (e.g., maximum motor power, speed limits, classification as e-bike vs. motor vehicle).\n",
      "   - Conduct thorough safety testing to meet standards such as UL (Underwriters Laboratories) or equivalent certifications.\n",
      "   - Include clear warnings and user instructions about safe operation, battery handling, and maintenance.\n",
      "\n",
      "2. **Product Liability and Warranty**:\n",
      "   - Clearly disclose warranty terms and any limitations, especially considering the budget-friendly positioning which may affect component quality.\n",
      "   - Implement robust quality control to reduce risk of defects that could lead to injury or property damage.\n",
      "\n",
      "3. **Battery and Environmental Compliance**:\n",
      "   - Ensure batteries comply with transportation regulations (e.g., IATA for shipping lithium-ion batteries).\n",
      "   - Address environmental compliance such as recycling or disposal instructions for batteries and electronic components.\n",
      "\n",
      "4. **Marketing and Advertising Claims**:\n",
      "   - Avoid making unsubstantiated claims about performance, range, or cost savings.\n",
      "   - Comply with advertising standards to prevent misleading consumers.\n",
      "\n",
      "5. **Data Privacy (if applicable)**:\n",
      "   - If the e-bike includes digital features collecting user data (GPS, usage), comply with relevant data privacy laws (e.g., GDPR, CCPA).\n",
      "   - Clearly inform users about data collection and obtain necessary consents.\n",
      "\n",
      "6. **Consumer Protection**:\n",
      "   - Provide clear information on return policies, customer support, and contact details.\n",
      "   - Consider potential need for after-sales service capabilities.\n",
      "\n",
      "Mitigating risks by ensuring compliance with these points will help prevent legal exposure and enhance consumer trust.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def run_concurrent_workflow():\n",
    "    \"\"\"Execute concurrent workflow with multiple agents.\"\"\"\n",
    "    \n",
    "    # Task to accomplish\n",
    "    TASK = \"We are launching a new budget-friendly electric bike for urban commuters.\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"CASE 2: CONCURRENT EXECUTION PATTERN - Multi-Perspective Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n📋 Task: {TASK}\\n\")\n",
    "    print(\"🔄 Executing agents in parallel...\\n\")\n",
    "    \n",
    "    # Build concurrent workflow\n",
    "    workflow = (\n",
    "        ConcurrentBuilder()\n",
    "        .participants([researcher_agent, marketer_agent, legal_agent])\n",
    "        .build()\n",
    "    )\n",
    "    \n",
    "    # Run workflow\n",
    "    events = await workflow.run(TASK)\n",
    "    outputs = events.get_outputs()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"CONCURRENT EXECUTION COMPLETED\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Display aggregated results\n",
    "    if outputs:\n",
    "        print(\"\\n📊 Aggregated Conversation (All Agent Responses):\\n\")\n",
    "        for output in outputs:\n",
    "            messages: list[ChatMessage] | Any = output\n",
    "            for i, msg in enumerate(messages, start=1):\n",
    "                name = msg.author_name if msg.author_name else \"User\"\n",
    "                print(f\"\\n{'─' * 80}\")\n",
    "                print(f\"[{i:02d}] {name}:\")\n",
    "                print(f\"{'─' * 80}\")\n",
    "                print(f\"{msg.text}\\n\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ No outputs received from workflow\")\n",
    "\n",
    "# Run concurrent workflow\n",
    "await run_concurrent_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa87464",
   "metadata": {},
   "source": [
    "### 📊 Concurrent Execution Pattern Summary\n",
    "\n",
    "**Advantages:**\n",
    "- ✅ Fast parallel execution\n",
    "- ✅ Multiple independent perspectives\n",
    "- ✅ Efficient resource utilization\n",
    "- ✅ Scalable to many agents\n",
    "\n",
    "**Considerations:**\n",
    "- ⚠️ No inter-agent communication during execution\n",
    "- ⚠️ Requires aggregation strategy\n",
    "- ⚠️ May produce redundant information\n",
    "\n",
    "**Best Use Cases:**\n",
    "- Multi-perspective analysis\n",
    "- Independent task execution\n",
    "- Rapid information gathering\n",
    "- Parallel research tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e649e",
   "metadata": {},
   "source": [
    "# Case 3: Magentic Orchestration Pattern\n",
    "---\n",
    "\n",
    "## What is Magentic Orchestration?\n",
    "Magentic is an intelligent orchestration pattern that uses a manager agent to coordinate specialized agents. It's ideal for:\n",
    "- **Complex multi-step tasks**: Planning and execution coordination\n",
    "- **Dynamic agent selection**: Manager chooses the right agent for each step\n",
    "- **Adaptive workflows**: Adjusts based on progress and results\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "                    ┌──→ Plan → Select Agent → Execute ──┐\n",
    "User Task ──→ Manager │                                    │──→ Final Result\n",
    "                    └──→ Monitor Progress → Adjust ──────┘\n",
    "                         ↓\n",
    "                    [Researcher, Coder, Analyst...]\n",
    "```\n",
    "\n",
    "### Key Features:\n",
    "- ✅ Intelligent planning and orchestration\n",
    "- ✅ Dynamic agent selection\n",
    "- ✅ Progress monitoring\n",
    "- ✅ Adaptive execution\n",
    "\n",
    "## Use Case: Complex Research with Code Analysis\n",
    "We'll create a Magentic workflow with:\n",
    "1. **Manager Agent**: Orchestrates and plans\n",
    "2. **Researcher Agent**: Finds information (with web search)\n",
    "3. **Coder Agent**: Performs computational analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa0099a",
   "metadata": {},
   "source": [
    "## 🧪 Step 3.1: Create Specialized Agents for Magentic\n",
    "---\n",
    "Create agents with specific capabilities for research and coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f767f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Magentic agents created:\n",
      "  - ResearcherAgent: Information gathering with search\n",
      "  - CoderAgent: Computational analysis with code execution\n"
     ]
    }
   ],
   "source": [
    "# Create researcher agent with search capability\n",
    "magentic_researcher = ChatAgent(\n",
    "    name=\"ResearcherAgent\",\n",
    "    description=\"Specialist in research and information gathering\",\n",
    "    instructions=\"\"\"\n",
    "    You are a Researcher. You find information without additional computation \n",
    "    or quantitative analysis. Provide factual, well-researched information.\n",
    "    \"\"\",\n",
    "    chat_client=azure_chat_client\n",
    ")\n",
    "\n",
    "# Create coder agent with code interpreter\n",
    "magentic_coder = ChatAgent(\n",
    "    name=\"CoderAgent\",\n",
    "    description=\"A helpful assistant that evaluates, writes and executes code to process and analyze data.\",\n",
    "    instructions=\"\"\"\n",
    "    You solve questions using code. Please provide detailed analysis and \n",
    "    computation process. Always explain your approach and results clearly.\n",
    "    \"\"\",\n",
    "    chat_client=azure_chat_client,\n",
    "    tools=HostedCodeInterpreterTool()\n",
    ")\n",
    "\n",
    "print(\"✓ Magentic agents created:\")\n",
    "print(\"  - ResearcherAgent: Information gathering with search\")\n",
    "print(\"  - CoderAgent: Computational analysis with code execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6878aa6",
   "metadata": {},
   "source": [
    "## 🧪 Step 3.2: Setup Streaming Callbacks\n",
    "---\n",
    "Define callback functions to monitor Magentic workflow progress in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7c81773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Callback functions defined\n"
     ]
    }
   ],
   "source": [
    "# State for tracking streaming\n",
    "last_stream_agent_id: str | None = None\n",
    "stream_line_open: bool = False\n",
    "\n",
    "async def on_magentic_event(event: MagenticCallbackEvent) -> None:\n",
    "    \"\"\"\n",
    "    Callback to process events emitted by the Magentic workflow.\n",
    "    Handles orchestrator messages, agent deltas, agent messages, and final results.\n",
    "    \"\"\"\n",
    "    global last_stream_agent_id, stream_line_open\n",
    "    \n",
    "    if isinstance(event, MagenticOrchestratorMessageEvent):\n",
    "        # Display orchestrator planning and coordination messages\n",
    "        print(f\"\\n🎯 [ORCHESTRATOR: {event.kind}]\")\n",
    "        print(f\"{'─' * 60}\")\n",
    "        print(f\"{getattr(event.message, 'text', '')}\")\n",
    "        print(f\"{'─' * 60}\\n\")\n",
    "        \n",
    "    elif isinstance(event, MagenticAgentDeltaEvent):\n",
    "        # Stream agent responses token by token\n",
    "        if last_stream_agent_id != event.agent_id or not stream_line_open:\n",
    "            if stream_line_open:\n",
    "                print()\n",
    "            print(f\"\\n🤖 [{event.agent_id}]: \", end=\"\", flush=True)\n",
    "            last_stream_agent_id = event.agent_id\n",
    "            stream_line_open = True\n",
    "        print(event.text, end=\"\", flush=True)\n",
    "        \n",
    "    elif isinstance(event, MagenticAgentMessageEvent):\n",
    "        # Display complete agent messages\n",
    "        if stream_line_open:\n",
    "            print(\" (complete)\")\n",
    "            stream_line_open = False\n",
    "            print()\n",
    "        if event.message is not None:\n",
    "            response_text = (event.message.text or \"\").replace(\"\\n\", \" \")[:200]\n",
    "            print(f\"\\n✅ [AGENT: {event.agent_id}] {event.message.role.value}\")\n",
    "            print(f\"   {response_text}...\")\n",
    "            print(f\"{'─' * 60}\\n\")\n",
    "            \n",
    "    elif isinstance(event, MagenticFinalResultEvent):\n",
    "        # Display final orchestration result\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🎉 MAGENTIC ORCHESTRATION - FINAL RESULT\")\n",
    "        print(\"=\" * 80)\n",
    "        if event.message is not None:\n",
    "            print(f\"\\n{event.message.text}\\n\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "print(\"✓ Callback functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32eafb",
   "metadata": {},
   "source": [
    "## 🧪 Step 3.3: Build and Execute Magentic Workflow\n",
    "---\n",
    "Build the Magentic workflow with streaming callbacks and execute a complex task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "414bf3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-14 09:08:27 - /anaconda/envs/venv_agentlab/lib/python3.12/site-packages/agent_framework/_workflows/_validation.py:522 - WARNING] Cycle detected in the workflow graph involving: agent_coder -> agent_researcher -> magentic_orchestrator -> agent_coder. Ensure termination or iteration limits exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CASE 3: MAGENTIC ORCHESTRATION PATTERN - Intelligent Agent Coordination\n",
      "================================================================================\n",
      "\n",
      "📋 Complex Task: \n",
      "            Compare the total 24-hour operating cost and efficiency of two language model setups:\n",
      "            DeepSeek-7B deployed on an NVIDIA H100 ...\n",
      "\n",
      "🔧 Building Magentic workflow...\n",
      "\n",
      "🚀 Starting Magentic orchestration...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "🎯 [ORCHESTRATOR: user_task]\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "            Compare the total 24-hour operating cost and efficiency of two language model setups:\n",
      "            DeepSeek-7B deployed on an NVIDIA H100 GPU instance (self-hosted)\n",
      "            GPT-4.1-mini accessed through the Azure OpenAI Service\n",
      "            Include the following points:\n",
      "                24-hour cost estimation – instance hourly rate vs. API token cost\n",
      "                Performance – average response time and throughput for 1K-token prompts\n",
      "                Operational factors – maintenance effort and scalability\n",
      "                Recommendation – which option offers better cost-to-value for continuous workloads\n",
      "                Save Results:\n",
      "                - saved to results/cost_summary.csv \n",
      "                - plot and save results/tokens_per_dollar.png\n",
      "            Present the comparison in a simple table.\n",
      "    \n",
      "────────────────────────────────────────────────────────────\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 [ORCHESTRATOR: task_ledger]\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "We are working to address the following user request:\n",
      "\n",
      "\n",
      "            Compare the total 24-hour operating cost and efficiency of two language model setups:\n",
      "            DeepSeek-7B deployed on an NVIDIA H100 GPU instance (self-hosted)\n",
      "            GPT-4.1-mini accessed through the Azure OpenAI Service\n",
      "            Include the following points:\n",
      "                24-hour cost estimation – instance hourly rate vs. API token cost\n",
      "                Performance – average response time and throughput for 1K-token prompts\n",
      "                Operational factors – maintenance effort and scalability\n",
      "                Recommendation – which option offers better cost-to-value for continuous workloads\n",
      "                Save Results:\n",
      "                - saved to results/cost_summary.csv \n",
      "                - plot and save results/tokens_per_dollar.png\n",
      "            Present the comparison in a simple table.\n",
      "    \n",
      "\n",
      "\n",
      "To answer this request we have assembled the following team:\n",
      "\n",
      "- researcher: Specialist in research and information gathering\n",
      "- coder: A helpful assistant that evaluates, writes and executes code to process and analyze data.\n",
      "\n",
      "\n",
      "Here is an initial fact sheet to consider:\n",
      "\n",
      "1. GIVEN OR VERIFIED FACTS  \n",
      "- Two language model setups to compare:  \n",
      "  - DeepSeek-7B on an NVIDIA H100 GPU instance (self-hosted)  \n",
      "  - GPT-4.1-mini via Azure OpenAI Service  \n",
      "- Comparison points: 24-hour cost estimation (hourly instance rate vs. API token cost), performance (average response time and throughput for 1K-token prompts), operational factors (maintenance effort and scalability), recommendation for cost-to-value in continuous workloads  \n",
      "- Output requirement: save results to results/cost_summary.csv and results/tokens_per_dollar.png  \n",
      "- Results to be presented in a simple comparison table  \n",
      "\n",
      "2. FACTS TO LOOK UP  \n",
      "- NVIDIA H100 GPU instance hourly rates on cloud providers (e.g., AWS, Azure, Google Cloud) or on-prem costs  \n",
      "- API pricing for GPT-4.1-mini from the Azure OpenAI Service, specifically cost per 1K tokens  \n",
      "- Typical or benchmarked average response times and throughput for 1K-token prompts for DeepSeek-7B on H100 and GPT-4.1-mini via Azure OpenAI  \n",
      "- Maintenance effort and scalability considerations for self-hosting a large model vs. using managed API services (may be available in whitepapers, cloud provider docs, or model provider documentation)  \n",
      "\n",
      "3. FACTS TO DERIVE  \n",
      "- Total 24-hour cost for DeepSeek-7B instance = hourly rate * 24  \n",
      "- Total 24-hour cost for GPT-4.1-mini = (number of tokens processed in 24 hours/1000) * API cost per 1K tokens (assuming continuous usage, may require assumption on throughput)  \n",
      "- Throughput (tokens per second or per minute) and number of prompts feasible in 24 hours for both setups based on average response time (e.g., 24h * 3600 seconds / response time per 1K-token prompt)  \n",
      "- Tokens processed per dollar for each setup (throughput divided by cost)  \n",
      "- Comparison table values and plotting tokens per dollar versus cost for visualization  \n",
      "\n",
      "4. EDUCATED GUESSES  \n",
      "- NVIDIA H100 cloud instances likely cost between $3 to $10 per hour depending on provider and configuration  \n",
      "- GPT-4.1-mini API cost per 1K tokens may range roughly from $0.03 to $0.10 based on typical Azure pricing patterns for smaller GPT-4 variants  \n",
      "- DeepSeek-7B latency for 1K-token prompt on H100 could be on the order of a few seconds (e.g., 2-5 seconds), with high throughput for batch processing  \n",
      "- GPT-4.1-mini API response time may be longer per request due to network latency and service overhead, possibly 5-10 seconds per 1K-token prompt  \n",
      "- Self-hosting incurs higher maintenance effort (infrastructure management, updates, monitoring), whereas Azure API offers managed service scalability with minimal operational overhead  \n",
      "- For continuous, high-throughput workloads, self-hosting may have better cost efficiency but higher operational complexity; API offers better scalability and lower management burden at higher per-token cost\n",
      "\n",
      "\n",
      "Here is the plan to follow as best as possible:\n",
      "\n",
      "- Researcher:  \n",
      "  - Gather up-to-date pricing data for NVIDIA H100 GPU instances from major cloud providers (e.g., AWS, Azure, GCP)  \n",
      "  - Retrieve pricing details for GPT-4.1-mini API usage on Azure OpenAI Service (cost per 1K tokens)  \n",
      "  - Find benchmark or reported performance metrics (response time and throughput) for both DeepSeek-7B on H100 and GPT-4.1-mini via API  \n",
      "  - Collect qualitative information on operational factors: maintenance effort and scalability for self-hosted versus managed API services  \n",
      "\n",
      "- Coder:  \n",
      "  - Calculate total 24-hour operating costs for each setup using researched prices and expected usage patterns  \n",
      "  - Compute throughput and tokens-per-dollar metrics based on performance data and cost figures  \n",
      "  - Create a simple comparison table summarizing cost, performance, operational factors, and recommendation  \n",
      "  - Save the table data as CSV file (`results/cost_summary.csv`)  \n",
      "  - Generate a plot of tokens per dollar for both setups, save as `results/tokens_per_dollar.png`  \n",
      "\n",
      "- Collaboration:  \n",
      "  - Researcher provides verified data and qualitative insights to Coder  \n",
      "  - Coder performs calculations, generates outputs, and presents final comparison table and graphics\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "🎯 [ORCHESTRATOR: instruction]\n",
      "────────────────────────────────────────────────────────────\n",
      "Please gather current hourly cost estimates for an NVIDIA H100 GPU instance, Azure OpenAI Service pricing for GPT-4.1-mini per 1,000 tokens, average response time and throughput benchmarks for both models processing 1K-token prompts, plus qualitative notes on maintenance and scalability factors.\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "🤖 [researcher]: Certainly, here is the compiled information based on the most recent available data as of mid-2024:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Hourly Cost Estimates for NVIDIA H100 GPU Instances\n",
      "\n",
      "- **Cloud Providers Offering NVIDIA H100 Instances:**\n",
      "  - **AWS (Amazon Web Services)**: Offers NVIDIA H100 GPUs under the EC2 “p5” instance family.\n",
      "    - Example: *p5.24xlarge* instance with 8× NVIDIA H100 GPUs.\n",
      "    - **Approximate cost:** ~$40–45 USD per hour (varies by region and reserved vs. on-demand pricing).\n",
      "\n",
      "  - **Azure**: Provides H100 GPUs in their *ND H100 v5* series.\n",
      "    - Example: *ND96asr_v5* with 8× NVIDIA H100 GPUs.\n",
      "    - **Approximate cost:** ~$38–42 USD per hour.\n",
      "\n",
      "  - **Google Cloud Platform (GCP)**: Introduced H100 GPUs available as accelerator attachments.\n",
      "    - GCP H100 GPU per hour: Approximately $8.60 per GPU (varies based on machine type and region).\n",
      "    - So, instances with 8 GPUs cost around $68–70 per hour.\n",
      "\n",
      "- **Summary**: \n",
      "  - Hourly prices for instances with a **single NVIDIA H100 GPU** vary but typically range between **$7 to $10 per hour** on major public clouds.\n",
      "  - Larger multi-GPU instances aggregate accordingly.\n",
      "  \n",
      "  These numbers are approximate and depend on region, commitment (on-demand vs reserved), and cloud provider discounts.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Azure OpenAI Service Pricing for GPT-4.1-mini\n",
      "\n",
      "- **Azure OpenAI Service GPT-4.1-mini Pricing (approximate and subject to change):**\n",
      "\n",
      "  Azure’s pricing model for GPT models often charges per 1,000 tokens processed.\n",
      "\n",
      "  - For \"GPT-4.1-mini\" (a smaller, cost-optimized variant of GPT-4.1):\n",
      "    - **Prompt Tokens:** ~$0.003–0.004 per 1,000 tokens\n",
      "    - **Completion Tokens:** ~$0.006–0.008 per 1,000 tokens\n",
      "\n",
      "  - Note: Pricing is split between prompt tokens (input) and completion tokens (output). The above represents a rough average.\n",
      "\n",
      "  For example, for a 1,000-token prompt and a 1,000-token completion, total cost would be in the range of **$0.009–0.012 per 1,000 tokens total** processed.\n",
      "\n",
      "- See Azure’s official pricing page for OpenAI:  \n",
      "  https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Average Response Time and Throughput Benchmarks for NVIDIA H100 and GPT-4.1-mini\n",
      "\n",
      "- **NVIDIA H100 GPU Model Processing Benchmarks**\n",
      "\n",
      "  The NVIDIA H100, being a latest-gen GPU optimized for AI workloads, drastically reduces inference latency and improves throughput compared to previous GPUs (A100, etc.).\n",
      "\n",
      "  - **Latency for 1K-token prompts using transformer models (GPT-like):**\n",
      "    - Reported latencies on H100 (single GPU) are around **~50–150 milliseconds** for a 1K-token input with moderate batch sizes.\n",
      "    - Larger batch sizes and optimized implementations reduce latency further.\n",
      "\n",
      "  - **Throughput:**\n",
      "    - H100 allows for high throughput, typically in the range of **several thousand tokens per second (kTPS)** for autoregressive models when using efficient implementations and batching.\n",
      "\n",
      "- **Azure OpenAI GPT-4.1-mini Model Performance**\n",
      "\n",
      "  - Azure does not publicly disclose exact average response times, but user-reported experience and service level objectives indicate:\n",
      "    - **Average response times:** Between **1 to 2 seconds** for 1K-token prompts.\n",
      "    - **Throughput:** Limited by Azure’s managed service design to balance multi-tenant performance and scale; approximate throughput is in the range of **500–1,000 tokens per second** per request, but actual performance may vary.\n",
      "\n",
      "- **Summary:**\n",
      "\n",
      "  | Metric                    | NVIDIA H100 GPU (Self-Hosted)   | Azure OpenAI GPT-4.1-mini (Managed)  |\n",
      "  |---------------------------|---------------------------------|-------------------------------------|\n",
      "  | Latency (1K tokens)       | ~50–150 ms                      | ~1–2 seconds                        |\n",
      "  | Throughput (Tokens/sec)   | 1000s+ (with batching)          | ~500–1000 (typical)                 |\n",
      "  | Deployment                | Custom environment control      | Fully managed service               |\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Qualitative Notes on Maintenance and Scalability\n",
      "\n",
      "- **Maintenance:**\n",
      "\n",
      "  - **NVIDIA H100 GPU-based instances (self-hosted or cloud VMs):**\n",
      "    - Requires ongoing maintenance of infrastructure including OS, drivers, CUDA/cuDNN libraries, and application frameworks.\n",
      "    - Model updates, tuning, and scaling must be managed by engineering teams.\n",
      "    - Troubleshooting and patch management are the customer’s responsibility.\n",
      "  \n",
      "  - **Azure OpenAI Service:**\n",
      "    - Fully managed by Microsoft; maintenance, scaling, security patches, and availability guaranteed by Azure SLA.\n",
      "    - Model upgrades and optimization transparent to the user.\n",
      "    - Simplifies operations considerably; no need for deep infrastructure expertise.\n",
      "\n",
      "- **Scalability:**\n",
      "\n",
      "  - **NVIDIA H100 GPU instances:**\n",
      "    - Scalability depends on infrastructure and budget.\n",
      "    - Scaling horizontally requires orchestration of multiple GPUs/nodes, with considerations for latency and synchronization.\n",
      "    - Expensive upfront and operational effort, but offers full control and flexibility.\n",
      "\n",
      "  - **Azure OpenAI Service:**\n",
      "    - Instantly scalable via managed endpoints.\n",
      "    - Customers can scale request volume without managing hardware.\n",
      "    - Pricing scales with usage; useful for rapid growth or variable workloads.\n",
      "    - Best for users wanting quick deployment and minimal operational overhead.\n",
      "\n",
      "---\n",
      "\n",
      "### Summary Table\n",
      "\n",
      "| Aspect                      | NVIDIA H100 GPU Instance            | Azure OpenAI GPT-4.1-mini             |\n",
      "|-----------------------------|-----------------------------------|--------------------------------------|\n",
      "| **Hourly Cost**              | ~$7–10 per GPU hour                | Pay per 1,000 tokens: ~$0.009–0.012  |\n",
      "| **Latency (1K tokens)**      | ~50–150 ms                       | ~1–2 seconds                        |\n",
      "| **Throughput (tokens/sec)**  | 1,000s+ (batched)                | ~500–1,000                         |\n",
      "| **Maintenance**              | Requires full user maintenance   | Fully managed by Azure               |\n",
      "| **Scalability**              | Infrastructure and budget limited| Instant auto scaling, pay-as-you-go  |\n",
      "| **Deployment Control**       | Full control                     | Managed service                      |\n",
      "\n",
      "---\n",
      "\n",
      "If you need references to specific sources or further details on any aspect, please let me know! (complete)\n",
      "\n",
      "\n",
      "✅ [AGENT: researcher] assistant\n",
      "   Certainly, here is the compiled information based on the most recent available data as of mid-2024:  ---  ### 1. Hourly Cost Estimates for NVIDIA H100 GPU Instances  - **Cloud Providers Offering NVIDI...\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "🎯 [ORCHESTRATOR: instruction]\n",
      "────────────────────────────────────────────────────────────\n",
      "Using the collected hourly GPU cost (~$9/hour), GPT-4.1-mini API cost (~$0.01 per 1K tokens), response time, and throughput data, calculate the 24-hour operating costs and tokens per dollar for both setups assuming continuous 1K-token prompt processing. Create a simple comparison table including performance, cost, maintenance, scalability, and a final recommendation. Save the table as results/cost_summary.csv and plot tokens_per_dollar.png.\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "🤖 [coder]: Let's analyze and compute the 24-hour operating costs and tokens per dollar for two setups:\n",
      "\n",
      "1. **GPU Setup**\n",
      "2. **GPT-4.1-mini API Setup**\n",
      "\n",
      "---\n",
      "\n",
      "### Given:\n",
      "\n",
      "- **GPU Cost:** $9/hour\n",
      "- **GPT-4.1-mini API Cost:** $0.01 per 1,000 tokens\n",
      "- **Prompt Size:** 1,000 tokens per prompt\n",
      "- **Processing Rate:**\n",
      "\n",
      "  We need response time and throughput data for both setups to calculate costs more precisely. Since these are mentioned to be \"collected\" but not explicitly given, I will assume sample realistic values for demonstration:\n",
      "\n",
      "| Metric                     | GPU Setup       | GPT-4.1-mini API Setup |\n",
      "|----------------------------|-----------------|------------------------|\n",
      "| Response Time per 1K tokens | 2 seconds       | 5 seconds              |\n",
      "| Throughput (prompts/hour)   | 1800 /hour      | 720 /hour              |\n",
      "\n",
      "---\n",
      "\n",
      "### Step 1: Calculate 24-hour operating costs\n",
      "\n",
      "- For GPU Setup:\n",
      "\n",
      "  Cost per hour = $9\n",
      "\n",
      "  Cost per 24-hour = 24 * 9 = $216\n",
      "\n",
      "- For GPT-4.1-mini API Setup:\n",
      "\n",
      "  Each 1K tokens costs $0.01, throughput is 720 prompts/hour:\n",
      "\n",
      "  Total tokens in 24 hours = 720 * 1,000 tokens * 24 hours = 17,280,000 tokens\n",
      "\n",
      "  Total cost = (17,280,000 tokens / 1,000) * 0.01 = 17,280 * 0.01 = $172.80\n",
      "\n",
      "---\n",
      "\n",
      "### Step 2: Calculate tokens per dollar\n",
      "\n",
      "- GPU Setup:\n",
      "\n",
      "  Tokens processed in 24 hours = Throughput * 24 * Tokens per prompt\n",
      "\n",
      "  = 1800 * 24 * 1,000 = 43,200,000 tokens\n",
      "\n",
      "  Tokens per dollar = total tokens / cost\n",
      "\n",
      "  = 43,200,000 / 216 ≈ 200,000 tokens/dollar\n",
      "\n",
      "- GPT-4.1-mini API Setup:\n",
      "\n",
      "  Tokens processed in 24 hours = 17,280,000 (from above)\n",
      "\n",
      "  Total cost = $172.80\n",
      "\n",
      "  Tokens per dollar = 17,280,000 / 172.80 ≈ 100,000 tokens/dollar\n",
      "\n",
      "---\n",
      "\n",
      "### Step 3: Additional comparison metrics\n",
      "\n",
      "- **Performance:** GPU generally has better throughput and lower latency.\n",
      "\n",
      "- **Cost:** API is cheaper but slower.\n",
      "\n",
      "- **Maintenance:** GPU requires hardware and software maintenance.\n",
      "\n",
      "- **Scalability:** API can scale based on demand easily; GPU might need additional hardware.\n",
      "\n",
      "---\n",
      "\n",
      "### Final Comparison Table\n",
      "\n",
      "| Metric          | GPU Setup               | GPT-4.1-mini API Setup    |\n",
      "|-----------------|------------------------|---------------------------|\n",
      "| Throughput (1K tokens/hour) | 1800                   | 720                       |\n",
      "| Response Time (sec/1K tokens) | 2                      | 5                         |\n",
      "| 24-hour Cost ($)  | 216                    | 172.80                    |\n",
      "| Tokens per Dollar | 200,000                | 100,000                   |\n",
      "| Maintenance      | High (Hardware, software updates) | Low (Managed by provider) |\n",
      "| Scalability      | Limited (additional GPUs needed) | High (API-based scaling)  |\n",
      "| Performance      | High                   | Moderate                  |\n",
      "| Final Recommendation | Use GPU if continuous high throughput needed, and maintenance acceptable | Use API for ease of use, lower upfront cost, and scaling flexibility |\n",
      "\n",
      "---\n",
      "\n",
      "### Implementation details:\n",
      "\n",
      "- Save this table as `results/cost_summary.csv`\n",
      "- Plot tokens_per_dollar.png showing tokens/dollar for both setups side by side.\n",
      "\n",
      "---\n",
      "\n",
      "### Now I will code to generate the CSV and plot: (complete)\n",
      "\n",
      "\n",
      "✅ [AGENT: coder] assistant\n",
      "   Let's analyze and compute the 24-hour operating costs and tokens per dollar for two setups:  1. **GPU Setup** 2. **GPT-4.1-mini API Setup**  ---  ### Given:  - **GPU Cost:** $9/hour - **GPT-4.1-mini A...\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "🎯 [ORCHESTRATOR: instruction]\n",
      "────────────────────────────────────────────────────────────\n",
      "Please generate the CSV file 'results/cost_summary.csv' containing the comparison table data, and create a bar plot comparing tokens per dollar for both setups, saving it as 'results/tokens_per_dollar.png'. Then provide the comparison table in text.\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "🤖 [coder]: Let's proceed step-by-step:\n",
      "\n",
      "1. Generate the CSV file `'results/cost_summary.csv'` with the detailed comparison table.\n",
      "2. Create a bar plot comparing tokens per dollar for both setups and save it as `'results/tokens_per_dollar.png'`.\n",
      "3. Provide the comparison table in text format.\n",
      "\n",
      "---\n",
      "\n",
      "### Here's the coding implementation:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Ensure directory exists\n",
      "os.makedirs('results', exist_ok=True)\n",
      "\n",
      "# Define the data for the table\n",
      "data = {\n",
      "    \"Metric\": [\n",
      "        \"Throughput (1K tokens/hour)\",\n",
      "        \"Response Time (sec/1K tokens)\",\n",
      "        \"24-hour Cost ($)\",\n",
      "        \"Tokens per Dollar\",\n",
      "        \"Maintenance\",\n",
      "        \"Scalability\",\n",
      "        \"Performance\",\n",
      "        \"Final Recommendation\"\n",
      "    ],\n",
      "    \"GPU Setup\": [\n",
      "        1800,\n",
      "        2,\n",
      "        216,\n",
      "        200000,\n",
      "        \"High (Hardware, software updates)\",\n",
      "        \"Limited (additional GPUs needed)\",\n",
      "        \"High\",\n",
      "        \"Use GPU if continuous high throughput needed, and maintenance acceptable\"\n",
      "    ],\n",
      "    \"GPT-4.1-mini API Setup\": [\n",
      "        720,\n",
      "        5,\n",
      "        172.80,\n",
      "        100000,\n",
      "        \"Low (Managed by provider)\",\n",
      "        \"High (API-based scaling)\",\n",
      "        \"Moderate\",\n",
      "        \"Use API for ease of use, lower upfront cost, and scaling flexibility\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "# Create DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Save to CSV\n",
      "csv_path = 'results/cost_summary.csv'\n",
      "df.to_csv(csv_path, index=False)\n",
      "\n",
      "# Plot tokens per dollar comparison\n",
      "fig, ax = plt.subplots(figsize=(6,4))\n",
      "setups = ['GPU Setup', 'GPT-4.1-mini API Setup']\n",
      "tokens_per_dollar = [200000, 100000]\n",
      "\n",
      "bars = ax.bar(setups, tokens_per_dollar, color=['steelblue', 'orange'])\n",
      "ax.set_ylabel('Tokens per Dollar')\n",
      "ax.set_title('Tokens per Dollar Comparison')\n",
      "ax.set_ylim(0, max(tokens_per_dollar)*1.2)\n",
      "\n",
      "# Add values on top of bars\n",
      "for bar in bars:\n",
      "    height = bar.get_height()\n",
      "    ax.annotate(f'{height:,}',\n",
      "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
      "                xytext=(0,3),\n",
      "                textcoords=\"offset points\",\n",
      "                ha='center', va='bottom')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('results/tokens_per_dollar.png')\n",
      "plt.close()\n",
      "\n",
      "print(f\"CSV saved at {csv_path}\")\n",
      "print(\"Plot saved at 'results/tokens_per_dollar.png'\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Comparison Table:\n",
      "\n",
      "| Metric                        | GPU Setup                                  | GPT-4.1-mini API Setup                         |\n",
      "|-------------------------------|------------------------------------------|------------------------------------------------|\n",
      "| Throughput (1K tokens/hour)    | 1800                                     | 720                                            |\n",
      "| Response Time (sec/1K tokens)  | 2                                        | 5                                              |\n",
      "| 24-hour Cost ($)               | 216                                      | 172.80                                         |\n",
      "| Tokens per Dollar              | 200,000                                  | 100,000                                        |\n",
      "| Maintenance                   | High (Hardware, software updates)          | Low (Managed by provider)                       |\n",
      "| Scalability                   | Limited (additional GPUs needed)           | High (API-based scaling)                        |\n",
      "| Performance                  | High                                      | Moderate                                       |\n",
      "| Final Recommendation         | Use GPU if continuous high throughput needed, and maintenance acceptable | Use API for ease of use, lower upfront cost, and scaling flexibility |\n",
      "\n",
      "---\n",
      "\n",
      "Files are saved and ready to use. Let me know if you want me to display or analyze any other aspect! (complete)\n",
      "\n",
      "\n",
      "✅ [AGENT: coder] assistant\n",
      "   Let's proceed step-by-step:  1. Generate the CSV file `'results/cost_summary.csv'` with the detailed comparison table. 2. Create a bar plot comparing tokens per dollar for both setups and save it as `...\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "================================================================================\n",
      "🎉 MAGENTIC ORCHESTRATION - FINAL RESULT\n",
      "================================================================================\n",
      "\n",
      "Here is the comprehensive comparison between deploying DeepSeek-7B on an NVIDIA H100 GPU instance (self-hosted) versus using GPT-4.1-mini through the Azure OpenAI Service for continuous 24-hour workloads:\n",
      "\n",
      "| Metric                        | DeepSeek-7B on NVIDIA H100 GPU | GPT-4.1-mini via Azure OpenAI Service |\n",
      "|------------------------------|-------------------------------|--------------------------------------|\n",
      "| **Throughput (1,000 tokens/hour)**      | 1,800                         | 720                                  |\n",
      "| **Average Response Time (seconds per 1K tokens)** | 2                             | 5                                    |\n",
      "| **Total 24-hour Cost (USD)**              | $216                          | $172.80                             |\n",
      "| **Tokens per Dollar**                     | 200,000                      | 100,000                             |\n",
      "| **Maintenance Effort**                    | High — requires hardware and software management | Low — fully managed service         |\n",
      "| **Scalability**                          | Limited — requires adding GPUs and infrastructure | High — instant auto-scaling via API |\n",
      "| **Performance**                         | High                          | Moderate                            |\n",
      "| **Recommendation**                       | Best if your workload demands very high throughput and you have the capability to manage infrastructure. Offers better tokens per dollar but at the cost of maintenance and operational complexity. | Ideal if you prefer a low-maintenance, scalable, pay-as-you-go solution. Provides good cost-to-value for moderate throughput and simpler operational overhead. |\n",
      "\n",
      "---\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "- **Cost:** While the Azure OpenAI API offers a slightly lower absolute 24-hour cost, the self-hosted setup delivers roughly double the tokens per dollar due to higher throughput.\n",
      "- **Performance:** DeepSeek-7B on an H100 processes 1K-token prompts significantly faster than the API.\n",
      "- **Operational Considerations:** Managing your own GPU infrastructure requires ongoing maintenance, software updates, and scaling planning. Azure’s managed API offloads all infrastructure responsibility and handles scaling seamlessly.\n",
      "- **Recommendation:**  \n",
      "  - For continuous, heavy workloads where maximizing throughput and minimizing token cost are paramount, and where you have the skills/resources to maintain the setup, **DeepSeek-7B on NVIDIA H100** is the better cost-to-value choice.  \n",
      "  - For flexible, scalable use with minimal maintenance and predictable operational overhead, **GPT-4.1-mini via Azure OpenAI** is preferable.\n",
      "\n",
      "---\n",
      "\n",
      "The detailed comparison table is saved as `results/cost_summary.csv`, and a visualization comparing tokens per dollar for both setups is available at `results/tokens_per_dollar.png`.\n",
      "\n",
      "If you want, I can help interpret the data further or assist with next steps for deployment planning!\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "MAGENTIC WORKFLOW COMPLETED\n",
      "================================================================================\n",
      "\n",
      "📄 Final Output Available (length: 61 characters)\n"
     ]
    }
   ],
   "source": [
    "async def run_magentic_workflow():\n",
    "    \"\"\"Execute Magentic orchestration workflow.\"\"\"\n",
    "    \n",
    "    # Complex task requiring both research and coding\n",
    "    TASK = \"\"\"\n",
    "            Compare the total 24-hour operating cost and efficiency of two language model setups:\n",
    "            DeepSeek-7B deployed on an NVIDIA H100 GPU instance (self-hosted)\n",
    "            GPT-4.1-mini accessed through the Azure OpenAI Service\n",
    "            Include the following points:\n",
    "                24-hour cost estimation – instance hourly rate vs. API token cost\n",
    "                Performance – average response time and throughput for 1K-token prompts\n",
    "                Operational factors – maintenance effort and scalability\n",
    "                Recommendation – which option offers better cost-to-value for continuous workloads\n",
    "                Save Results:\n",
    "                - saved to results/cost_summary.csv \n",
    "                - plot and save results/tokens_per_dollar.png\n",
    "            Present the comparison in a simple table.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"CASE 3: MAGENTIC ORCHESTRATION PATTERN - Intelligent Agent Coordination\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n📋 Complex Task: {TASK[:150]}...\\n\")\n",
    "    \n",
    "    # Reset streaming state\n",
    "    global last_stream_agent_id, stream_line_open\n",
    "    last_stream_agent_id = None\n",
    "    stream_line_open = False\n",
    "    \n",
    "    # Build Magentic workflow\n",
    "    print(\"🔧 Building Magentic workflow...\\n\")\n",
    "    workflow = (\n",
    "        MagenticBuilder()\n",
    "        .participants(researcher=magentic_researcher, coder=magentic_coder)\n",
    "        .on_event(on_magentic_event, mode=MagenticCallbackMode.STREAMING)\n",
    "        .with_standard_manager(\n",
    "            chat_client=AzureOpenAIChatClient(),\n",
    "            max_round_count=10,\n",
    "            max_stall_count=2,\n",
    "            max_reset_count=2\n",
    "        )\n",
    "        .build()\n",
    "    )\n",
    "    \n",
    "    print(\"🚀 Starting Magentic orchestration...\\n\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Execute workflow with streaming\n",
    "    try:\n",
    "        output: str | None = None\n",
    "        async for event in workflow.run_stream(TASK):\n",
    "            if isinstance(event, WorkflowOutputEvent):\n",
    "                output = str(event.data)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"MAGENTIC WORKFLOW COMPLETED\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if output:\n",
    "            print(f\"\\n📄 Final Output Available (length: {len(output)} characters)\")\n",
    "        else:\n",
    "            print(\"\\n⚠️ No final output received\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Workflow execution failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run Magentic workflow\n",
    "await run_magentic_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff61ba",
   "metadata": {},
   "source": [
    "### 📊 Magentic Orchestration Pattern Summary\n",
    "\n",
    "**Advantages:**\n",
    "- ✅ Intelligent planning and coordination\n",
    "- ✅ Dynamic agent selection based on task\n",
    "- ✅ Handles complex multi-step workflows\n",
    "- ✅ Adaptive execution with progress monitoring\n",
    "- ✅ Natural task decomposition\n",
    "\n",
    "**Considerations:**\n",
    "- ⚠️ More complex to set up\n",
    "- ⚠️ Requires capable manager model\n",
    "- ⚠️ Higher token consumption for planning\n",
    "- ⚠️ May have longer execution time for simple tasks\n",
    "\n",
    "**Best Use Cases:**\n",
    "- Complex research projects\n",
    "- Multi-step analysis workflows\n",
    "- Tasks requiring different expertise\n",
    "- Adaptive problem-solving\n",
    "- Production-grade agent systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469cca34",
   "metadata": {},
   "source": [
    "# Comparison of Multi-Agent Patterns\n",
    "---\n",
    "\n",
    "## 📊 Pattern Comparison Matrix\n",
    "\n",
    "| Aspect | Group Chat | Concurrent | Magentic |\n",
    "|--------|-----------|------------|----------|\n",
    "| **Execution** | Sequential | Parallel | Orchestrated |\n",
    "| **Agent Interaction** | Turn-based dialogue | Independent work | Coordinated by manager |\n",
    "| **Speed** | Slowest | Fastest | Medium |\n",
    "| **Complexity** | Low | Low | High |\n",
    "| **Planning** | None | None | Built-in |\n",
    "| **Best For** | Refinement workflows | Independent analysis | Complex multi-step tasks |\n",
    "| **Token Usage** | Medium | Low-Medium | High |\n",
    "| **Observability** | High | Medium | Very High |\n",
    "\n",
    "## 🎯 Decision Guide: When to Use Each Pattern\n",
    "\n",
    "### Use **Group Chat** when:\n",
    "- ✅ You need iterative refinement\n",
    "- ✅ Agents should respond to each other's output\n",
    "- ✅ Sequential review/approval is required\n",
    "- ✅ Conversation flow is important\n",
    "- ❌ Don't use for: Independent parallel work\n",
    "\n",
    "### Use **Concurrent Execution** when:\n",
    "- ✅ You need multiple independent perspectives\n",
    "- ✅ Speed is critical\n",
    "- ✅ Agents don't need to communicate\n",
    "- ✅ You want diverse viewpoints\n",
    "- ❌ Don't use for: Tasks requiring agent coordination\n",
    "\n",
    "### Use **Magentic Orchestration** when:\n",
    "- ✅ Task is complex with multiple steps\n",
    "- ✅ Different agents have different capabilities\n",
    "- ✅ You need intelligent task decomposition\n",
    "- ✅ Adaptive execution is required\n",
    "- ❌ Don't use for: Simple single-agent tasks\n",
    "\n",
    "## 💡 Hybrid Approaches\n",
    "\n",
    "You can also combine patterns:\n",
    "- **Concurrent + Magentic**: Manager orchestrates groups of concurrent agents\n",
    "- **Group Chat + Concurrent**: Run multiple group chats in parallel\n",
    "- **Magentic → Group Chat**: Manager delegates to a group chat for refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56ae4fb",
   "metadata": {},
   "source": [
    "# Key Takeaways\n",
    "---\n",
    "\n",
    "## 🎓 What You Learned\n",
    "\n",
    "1. **Group Chat Pattern**\n",
    "   - Sequential turn-based communication\n",
    "   - Custom termination strategies\n",
    "   - Perfect for collaborative refinement\n",
    "\n",
    "2. **Concurrent Execution Pattern**\n",
    "   - Parallel agent execution\n",
    "   - Fast multi-perspective analysis\n",
    "   - Simple fan-out/fan-in workflow\n",
    "\n",
    "3. **Magentic Orchestration Pattern**\n",
    "   - Intelligent task planning\n",
    "   - Dynamic agent coordination\n",
    "   - Complex multi-step workflows\n",
    "\n",
    "## 🚀 Next Steps\n",
    "\n",
    "- **Experiment with different agent instructions**: Try various persona combinations\n",
    "- **Add more agents**: Scale up to 5-10 specialized agents\n",
    "- **Integrate with real tools**: Connect to databases, APIs, file systems\n",
    "- **Add human-in-the-loop**: Implement approval workflows\n",
    "- **Monitor with observability**: Add Azure AI tracing and logging\n",
    "- **Production deployment**: Use Azure AI Foundry for managed hosting\n",
    "\n",
    "## 📚 Additional Resources\n",
    "\n",
    "- [Microsoft Agent Framework Docs](https://learn.microsoft.com/en-us/agent-framework/)\n",
    "- [Agent Framework Samples](https://github.com/microsoft/agent-framework/tree/main/python/samples)\n",
    "- [Multi-Agent Design Patterns](https://learn.microsoft.com/en-us/agent-framework/patterns)\n",
    "- [Azure AI Foundry](https://ai.azure.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a22f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_agentlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
