{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch evaluation with your custom evaluators\n",
    "The following sample shows the basic way to evaluate a Generative AI application in your development environment with your custom evaluators.\n",
    "\n",
    "> âœ¨ ***Note*** <br>\n",
    "> Please check the reference document before you get started - https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## ðŸ”¨ Current Support and Limitations (as of 2025-01-14) \n",
    "- Check the region support for the Azure AI Evaluation SDK. https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning#region-support\n",
    "\n",
    "### Region support for evaluations\n",
    "| Region              | Hate and Unfairness, Sexual, Violent, Self-Harm, XPIA, ECI (Text) | Groundedness (Text) | Protected Material (Text) | Hate and Unfairness, Sexual, Violent, Self-Harm, Protected Material (Image) |\n",
    "|---------------------|------------------------------------------------------------------|---------------------|----------------------------|----------------------------------------------------------------------------|\n",
    "| North Central US    | no                                                               | no                  | no                         | yes                                                                        |\n",
    "| East US 2           | yes                                                              | yes                 | yes                        | yes                                                                        |\n",
    "| Sweden Central      | yes                                                              | yes                 | yes                        | yes                                                                        |\n",
    "| US North Central    | yes                                                              | no                  | yes                        | yes                                                                        |\n",
    "| France Central      | yes                                                              | yes                 | yes                        | yes                                                                        |\n",
    "| Switzerland West    | yes                                                              | no                  | no                         | yes                                                                        |\n",
    "\n",
    "### Region support for adversarial simulation\n",
    "| Region            | Adversarial Simulation (Text) | Adversarial Simulation (Image) |\n",
    "|-------------------|-------------------------------|---------------------------------|\n",
    "| UK South          | yes                           | no                              |\n",
    "| East US 2         | yes                           | yes                             |\n",
    "| Sweden Central    | yes                           | yes                             |\n",
    "| US North Central  | yes                           | yes                             |\n",
    "| France Central    | yes                           | no                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ”ï¸ Pricing and billing\n",
    "- Effective 1/14/2025, Azure AI Safety Evaluations will no longer be free in public preview. It will be billed based on consumption as following:\n",
    "\n",
    "| Service Name              | Safety Evaluations       | Price Per 1K Tokens (USD) |\n",
    "|---------------------------|--------------------------|---------------------------|\n",
    "| Azure Machine Learning    | Input pricing for 3P     | $0.02                     |\n",
    "| Azure Machine Learning    | Output pricing for 3P    | $0.06                     |\n",
    "| Azure Machine Learning    | Input pricing for 1P     | $0.012                    |\n",
    "| Azure Machine Learning    | Output pricing for 1P    | $0.012                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pprint import pprint\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "from azure.ai.evaluation import GroundednessEvaluator, GroundednessProEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    Dataset,\n",
    "    EvaluatorConfiguration,\n",
    "    ConnectionType,\n",
    "    EvaluationSchedule,\n",
    "    RecurrenceTrigger,\n",
    "    ApplicationInsightsConfiguration,\n",
    ")\n",
    "import pathlib\n",
    "\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ContentSafetyEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    SimilarityEvaluator,\n",
    "    F1ScoreEvaluator,\n",
    "    RetrievalEvaluator,\n",
    ")\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Run Evaluators in Azure Cloud (azure.ai.evaluation.evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "\n",
    "azure_ai_project_conn_str = os.environ.get(\"AZURE_AI_PROJECT_CONN_STR\")\n",
    "subscription_id = azure_ai_project_conn_str.split(\";\")[1]\n",
    "resource_group_name = azure_ai_project_conn_str.split(\";\")[2]\n",
    "project_name = azure_ai_project_conn_str.split(\";\")[3]\n",
    "\n",
    "azure_ai_project_dict = {\n",
    "    \"subscription_id\": subscription_id,\n",
    "    \"resource_group_name\": resource_group_name,\n",
    "    \"project_name\": project_name,\n",
    "}\n",
    "\n",
    "azure_ai_project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(), conn_str=azure_ai_project_conn_str\n",
    ")\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    \"type\": \"azure_openai\",\n",
    "}\n",
    "\n",
    "ml_client = MLClient(credential, subscription_id, resource_group_name, project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set the evaluation dataset\n",
    "- Use your query data set on your storage account. These response records serve as the seed for creating assessments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initialized AzuureOpenAI client ===\n",
      "AZURE_OPENAI_ENDPOINT=https://aoai-services1.openai.azure.com/\n",
      "AZURE_OPENAI_API_VERSION=2024-12-01-preview\n",
      "AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "aoai_api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "aoai_deployment_name = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "\n",
    "try:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=aoai_api_endpoint,\n",
    "        api_key=aoai_api_key,\n",
    "        api_version=aoai_api_version,\n",
    "    )\n",
    "\n",
    "    print(\"=== Initialized AzuureOpenAI client ===\")\n",
    "    print(f\"AZURE_OPENAI_ENDPOINT={aoai_api_endpoint}\")\n",
    "    print(f\"AZURE_OPENAI_API_VERSION={aoai_api_version}\")\n",
    "    print(f\"AZURE_OPENAI_DEPLOYMENT_NAME={aoai_deployment_name}\")\n",
    "\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have all the data (Query + Response + Ground Truth)\n",
    "- If you have all the data, you can evaluate the model / service with the following steps.\n",
    "- Assume that you already upload the data file (csv) to Azure Blob storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'rstrip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m container_name = \u001b[33m\"\u001b[39m\u001b[33meval-container\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m blob_name = \u001b[33m\"\u001b[39m\u001b[33mcustom_data.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m blob_service_client = \u001b[43mBlobServiceClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_connection_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_conn_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m container_client = blob_service_client.get_container_client(container_name)\n\u001b[32m     14\u001b[39m blob_client = container_client.get_blob_client(blob_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/venv_eval/lib/python3.11/site-packages/azure/storage/blob/_blob_service_client.py:184\u001b[39m, in \u001b[36mBlobServiceClient.from_connection_string\u001b[39m\u001b[34m(cls, conn_str, credential, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_connection_string\u001b[39m(\n\u001b[32m    147\u001b[39m     \u001b[38;5;28mcls\u001b[39m, conn_str: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    148\u001b[39m     credential: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mAzureNamedKeyCredential\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAzureSasCredential\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTokenCredential\u001b[39m\u001b[33m\"\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# pylint: disable=line-too-long\u001b[39;00m\n\u001b[32m    149\u001b[39m     **kwargs: Any\n\u001b[32m    150\u001b[39m ) -> Self:\n\u001b[32m    151\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create BlobServiceClient from a Connection String.\u001b[39;00m\n\u001b[32m    152\u001b[39m \n\u001b[32m    153\u001b[39m \u001b[33;03m    :param str conn_str:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m            :caption: Creating the BlobServiceClient from a connection string.\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     account_url, secondary, credential = \u001b[43mparse_connection_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredential\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mblob\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33msecondary_hostname\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m    186\u001b[39m         kwargs[\u001b[33m'\u001b[39m\u001b[33msecondary_hostname\u001b[39m\u001b[33m'\u001b[39m] = secondary\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/venv_eval/lib/python3.11/site-packages/azure/storage/blob/_shared/base_client.py:389\u001b[39m, in \u001b[36mparse_connection_str\u001b[39m\u001b[34m(conn_str, credential, service)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_connection_str\u001b[39m(\n\u001b[32m    385\u001b[39m     conn_str: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    386\u001b[39m     credential: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m], AzureNamedKeyCredential, AzureSasCredential, TokenCredential]],\n\u001b[32m    387\u001b[39m     service: \u001b[38;5;28mstr\u001b[39m\n\u001b[32m    388\u001b[39m ) -> Tuple[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m], Optional[Union[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m], AzureNamedKeyCredential, AzureSasCredential, TokenCredential]]]:  \u001b[38;5;66;03m# pylint: disable=line-too-long\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m     conn_str = \u001b[43mconn_str\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrstrip\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    390\u001b[39m     conn_settings_list = [s.split(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m conn_str.split(\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tup) != \u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tup \u001b[38;5;129;01min\u001b[39;00m conn_settings_list):\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'rstrip'"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Retrieve the storage connection string from environment variables\n",
    "blob_conn_str = os.getenv(\"AZURE_STORAGE_BLOB_CONNECTION_STRING\")\n",
    "\n",
    "# Specify your container and blob name\n",
    "# container_name = \"mycontainer\"\n",
    "# blob_name = \"mydata.csv\"\n",
    "container_name = \"eval-container\"\n",
    "blob_name = \"custom_data.csv\"\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(blob_conn_str)\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "blob_client = container_client.get_blob_client(blob_name)\n",
    "\n",
    "# Download CSV from Azure Blob and save locally\n",
    "with open(\"custom_temp_data.csv\", \"wb\") as f:\n",
    "    blob_data = blob_client.download_blob()\n",
    "    blob_data.readinto(f)\n",
    "\n",
    "df = pd.read_csv(\"custom_temp_data.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     os.mkdir(outdir)\n\u001b[32m      8\u001b[39m input_path = os.path.join(outdir, outname)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mdf\u001b[49m.to_json(input_path, orient=\u001b[33m\"\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m\"\u001b[39m, lines=\u001b[38;5;28;01mTrue\u001b[39;00m, force_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# # Create a jsonl file from the query data\n",
    "outname = \"custom_data.jsonl\"\n",
    "\n",
    "outdir = \"./data\"\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "input_path = os.path.join(outdir, outname)\n",
    "df.to_json(input_path, orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"./data/sythetic_evaluation_data.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Run Evaluators for local and upload to cloud (azure.ai.evaluation.evaluate)\n",
    "- set up your custom ExactMatchEvaluator for Local environment with the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    class ExactMatchEvaluator(EvaluatorBase):\\n    \"\"\"\\n    Evaluates whether the response exactly matches the ground truth.\\n\\n    This evaluator returns a score of 1.0 if the response is identical to the ground truth,\\n    and 0.0 otherwise. It is useful for tasks that require strict correctness such as factual QA.\\n\\n    Example:\\n        evaluator = ExactMatchEvaluator()\\n        result = evaluator(ground_truth=\"Hello, world!\", response=\"Hello, world!\")\\n        print(result)  # {\\'exact_match_score\\': 1.0}\\n    \"\"\"\\n\\n    id = \"update with your azure ml asset id\"\\n    \"\"\"Evaluator identifier, experimental and to be used only with evaluation in cloud.\"\"\"\\n\\n    @override\\n    def __init__(self):\\n        super().__init__()\\n\\n    @override\\n    async def _do_eval(self, eval_input: Dict) -> Dict[str, float]:\\n        \"\"\"Evaluate whether the response matches the ground truth exactly.\"\"\"\\n        ground_truth = eval_input[\"ground_truth\"].strip()\\n        response = eval_input[\"response\"].strip()\\n        \\n        score = 1.0 if ground_truth == response else 0.0\\n\\n        return {\\n            \"exact_match_score\": score,\\n        }\\n\\n    @overload\\n    def __call__(self, *, ground_truth: str, response: str):\\n        \"\"\"\\n        Evaluate whether the response matches the ground truth exactly.\\n\\n        :keyword response: The response to be evaluated.\\n        :paramtype response: str\\n        :keyword ground_truth: The ground truth to be compared against.\\n        :paramtype ground_truth: str\\n        :return: The exact match score.\\n        :rtype: Dict[str, float]\\n        \"\"\"\\n\\n    @override\\n    def __call__(self, *args, **kwargs):\\n        \"\"\"Evaluate whether the response matches the ground truth exactly.\"\"\"\\n        return super().__call__(*args, **kwargs)\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ExactMatchEvaluator._exact_match import ExactMatchEvaluator\n",
    "\n",
    "# check the source code of the ExactMatchEvaluator\n",
    "# This is a custom evaluator that relates to the task of evaluating in local environment\n",
    "'''\n",
    "    class ExactMatchEvaluator(EvaluatorBase):\n",
    "    \"\"\"\n",
    "    Evaluates whether the response exactly matches the ground truth.\n",
    "\n",
    "    This evaluator returns a score of 1.0 if the response is identical to the ground truth,\n",
    "    and 0.0 otherwise. It is useful for tasks that require strict correctness such as factual QA.\n",
    "\n",
    "    Example:\n",
    "        evaluator = ExactMatchEvaluator()\n",
    "        result = evaluator(ground_truth=\"Hello, world!\", response=\"Hello, world!\")\n",
    "        print(result)  # {'exact_match_score': 1.0}\n",
    "    \"\"\"\n",
    "\n",
    "    id = \"update with your azure ml asset id\"\n",
    "    \"\"\"Evaluator identifier, experimental and to be used only with evaluation in cloud.\"\"\"\n",
    "\n",
    "    @override\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @override\n",
    "    async def _do_eval(self, eval_input: Dict) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate whether the response matches the ground truth exactly.\"\"\"\n",
    "        ground_truth = eval_input[\"ground_truth\"].strip()\n",
    "        response = eval_input[\"response\"].strip()\n",
    "        \n",
    "        score = 1.0 if ground_truth == response else 0.0\n",
    "\n",
    "        return {\n",
    "            \"exact_match_score\": score,\n",
    "        }\n",
    "\n",
    "    @overload\n",
    "    def __call__(self, *, ground_truth: str, response: str):\n",
    "        \"\"\"\n",
    "        Evaluate whether the response matches the ground truth exactly.\n",
    "\n",
    "        :keyword response: The response to be evaluated.\n",
    "        :paramtype response: str\n",
    "        :keyword ground_truth: The ground truth to be compared against.\n",
    "        :paramtype ground_truth: str\n",
    "        :return: The exact match score.\n",
    "        :rtype: Dict[str, float]\n",
    "        \"\"\"\n",
    "\n",
    "    @override\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"Evaluate whether the response matches the ground truth exactly.\"\"\"\n",
    "        return super().__call__(*args, **kwargs)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match_score': 1.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exactMatchEvaluator = ExactMatchEvaluator()\n",
    "\n",
    "exactMatch = exactMatchEvaluator(\n",
    "    ground_truth=\"What is the speed of light?\", response=\"What is the speed of light?\"\n",
    ")\n",
    "\n",
    "exactMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = input_path  # be sure which case you took\n",
    "output_path = \"./data/local_upload_cloud_evaluation_output.json\"\n",
    "\n",
    "\n",
    "# https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/flow-evaluate-sdk\n",
    "\n",
    "retrieval_evaluator = RetrievalEvaluator(model_config)\n",
    "groundedness_evaluator = GroundednessEvaluator(model_config)\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "similarity_evaluator = SimilarityEvaluator(model_config)\n",
    "exactMatchEvaluator = ExactMatchEvaluator()\n",
    "\n",
    "column_mapping = {\n",
    "    \"query\": \"${data.query}\",\n",
    "    \"ground_truth\": \"${data.ground_truth}\",\n",
    "    \"response\": \"${data.response}\",\n",
    "    \"context\": \"${data.context}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-11 09:15:10 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-03-11 09:15:10 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-03-11 09:15:10 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-03-11 09:15:10 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-03-11 09:15:10 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_5cqzqpyq_20250311_091510_697486, log path: /root/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_5cqzqpyq_20250311_091510_697486/logs.txt\n",
      "[2025-03-11 09:15:10 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_7jdk8f5t_20250311_091510_703419, log path: /root/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_7jdk8f5t_20250311_091510_703419/logs.txt\n",
      "[2025-03-11 09:15:10 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_4nxofccw_20250311_091510_701884, log path: /root/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_4nxofccw_20250311_091510_701884/logs.txt\n",
      "[2025-03-11 09:15:10 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_4nr0bm66_20250311_091510_694780, log path: /root/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_4nr0bm66_20250311_091510_694780/logs.txt\n",
      "[2025-03-11 09:15:10 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-03-11 09:15:10 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_jjv8rxuv_20250311_091510_705977, log path: /root/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_jjv8rxuv_20250311_091510_705977/logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-11 09:15:11 +0000  204867 execution.bulk     INFO     Finished 10 / 20 lines.\n",
      "2025-03-11 09:15:11 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.11 seconds. Estimated time for incomplete lines: 1.1 seconds.\n",
      "2025-03-11 09:15:12 +0000  204867 execution.bulk     INFO     Finished 12 / 20 lines.\n",
      "2025-03-11 09:15:12 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.11 seconds. Estimated time for incomplete lines: 0.88 seconds.\n",
      "2025-03-11 09:15:12 +0000  204867 execution.bulk     INFO     Finished 14 / 20 lines.\n",
      "2025-03-11 09:15:12 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.09 seconds. Estimated time for incomplete lines: 0.54 seconds.\n",
      "2025-03-11 09:15:12 +0000  204867 execution.bulk     INFO     Finished 16 / 20 lines.\n",
      "2025-03-11 09:15:12 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.08 seconds. Estimated time for incomplete lines: 0.32 seconds.\n",
      "2025-03-11 09:15:12 +0000  204867 execution.bulk     INFO     Finished 18 / 20 lines.\n",
      "2025-03-11 09:15:12 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.07 seconds. Estimated time for incomplete lines: 0.14 seconds.\n",
      "2025-03-11 09:15:12 +0000  204867 execution.bulk     INFO     Finished 20 / 20 lines.\n",
      "2025-03-11 09:15:12 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.07 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Finished 2 / 20 lines.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 1.1 seconds. Estimated time for incomplete lines: 19.8 seconds.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Finished 2 / 20 lines.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 1.12 seconds. Estimated time for incomplete lines: 20.16 seconds.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Finished 4 / 20 lines.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.59 seconds. Estimated time for incomplete lines: 9.44 seconds.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Finished 2 / 20 lines.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 1.21 seconds. Estimated time for incomplete lines: 21.78 seconds.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Finished 6 / 20 lines.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.42 seconds. Estimated time for incomplete lines: 5.88 seconds.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Finished 4 / 20 lines.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.65 seconds. Estimated time for incomplete lines: 10.4 seconds.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Finished 6 / 20 lines.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.44 seconds. Estimated time for incomplete lines: 6.16 seconds.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Finished 4 / 20 lines.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.7 seconds. Estimated time for incomplete lines: 11.2 seconds.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Finished 8 / 20 lines.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.36 seconds. Estimated time for incomplete lines: 4.32 seconds.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Finished 6 / 20 lines.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.51 seconds. Estimated time for incomplete lines: 7.14 seconds.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Finished 10 / 20 lines.\n",
      "2025-03-11 09:15:13 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.31 seconds. Estimated time for incomplete lines: 3.1 seconds.\n",
      "2025-03-11 09:15:14 +0000  204867 execution.bulk     INFO     Finished 8 / 20 lines.\n",
      "2025-03-11 09:15:14 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.45 seconds. Estimated time for incomplete lines: 5.4 seconds.\n",
      "2025-03-11 09:15:14 +0000  204867 execution.bulk     INFO     Finished 8 / 20 lines.\n",
      "2025-03-11 09:15:14 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.5 seconds. Estimated time for incomplete lines: 6.0 seconds.\n",
      "2025-03-11 09:15:15 +0000  204867 execution.bulk     INFO     Finished 12 / 20 lines.\n",
      "2025-03-11 09:15:15 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.35 seconds. Estimated time for incomplete lines: 2.8 seconds.\n",
      "2025-03-11 09:15:15 +0000  204867 execution.bulk     INFO     Finished 10 / 20 lines.\n",
      "2025-03-11 09:15:15 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.42 seconds. Estimated time for incomplete lines: 4.2 seconds.\n",
      "2025-03-11 09:15:15 +0000  204867 execution.bulk     INFO     Finished 10 / 20 lines.\n",
      "2025-03-11 09:15:15 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.44 seconds. Estimated time for incomplete lines: 4.4 seconds.\n",
      "2025-03-11 09:15:15 +0000  204867 execution.bulk     INFO     Finished 14 / 20 lines.\n",
      "2025-03-11 09:15:15 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.32 seconds. Estimated time for incomplete lines: 1.92 seconds.\n",
      "2025-03-11 09:15:15 +0000  204867 execution.bulk     INFO     Finished 12 / 20 lines.\n",
      "2025-03-11 09:15:15 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.4 seconds. Estimated time for incomplete lines: 3.2 seconds.\n",
      "2025-03-11 09:15:15 +0000  204867 execution.bulk     INFO     Finished 12 / 20 lines.\n",
      "2025-03-11 09:15:15 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.4 seconds. Estimated time for incomplete lines: 3.2 seconds.\n",
      "2025-03-11 09:15:15 +0000  204867 execution.bulk     INFO     Finished 16 / 20 lines.\n",
      "2025-03-11 09:15:15 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.31 seconds. Estimated time for incomplete lines: 1.24 seconds.\n",
      "2025-03-11 09:15:15 +0000  204867 execution.bulk     INFO     Finished 14 / 20 lines.\n",
      "2025-03-11 09:15:15 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.36 seconds. Estimated time for incomplete lines: 2.16 seconds.\n",
      "2025-03-11 09:15:16 +0000  204867 execution.bulk     INFO     Finished 18 / 20 lines.\n",
      "2025-03-11 09:15:16 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.3 seconds. Estimated time for incomplete lines: 0.6 seconds.\n",
      "2025-03-11 09:15:16 +0000  204867 execution.bulk     INFO     Finished 14 / 20 lines.\n",
      "2025-03-11 09:15:16 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.38 seconds. Estimated time for incomplete lines: 2.28 seconds.\n",
      "2025-03-11 09:15:16 +0000  204867 execution.bulk     INFO     Finished 16 / 20 lines.\n",
      "2025-03-11 09:15:16 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.35 seconds. Estimated time for incomplete lines: 1.4 seconds.\n",
      "2025-03-11 09:15:16 +0000  204867 execution.bulk     INFO     Finished 20 / 20 lines.\n",
      "2025-03-11 09:15:16 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.28 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-03-11 09:15:16 +0000  204867 execution.bulk     INFO     Finished 16 / 20 lines.\n",
      "2025-03-11 09:15:16 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.37 seconds. Estimated time for incomplete lines: 1.48 seconds.\n",
      "2025-03-11 09:15:17 +0000  204867 execution.bulk     INFO     Finished 18 / 20 lines.\n",
      "2025-03-11 09:15:17 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.34 seconds. Estimated time for incomplete lines: 0.68 seconds.\n",
      "2025-03-11 09:15:17 +0000  204867 execution.bulk     INFO     Finished 18 / 20 lines.\n",
      "2025-03-11 09:15:17 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.37 seconds. Estimated time for incomplete lines: 0.74 seconds.\n",
      "2025-03-11 09:15:18 +0000  204867 execution.bulk     INFO     Finished 20 / 20 lines.\n",
      "2025-03-11 09:15:18 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.4 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-03-11 09:15:23 +0000  204867 execution.bulk     INFO     Finished 20 / 20 lines.\n",
      "2025-03-11 09:15:23 +0000  204867 execution.bulk     INFO     Average execution time for completed lines: 0.64 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "result = evaluate(\n",
    "    evaluation_name=f\"custom_evaluation_local_upload_cloud_{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "    data=input_path,\n",
    "    evaluators={\n",
    "        \"retrieval\": retrieval_evaluator,\n",
    "        \"groundedness\": groundedness_evaluator,\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"similarity\": similarity_evaluator,\n",
    "        \"exact_match\": exactMatchEvaluator,\n",
    "    },\n",
    "    evaluator_config={\n",
    "        \"retrieval\": {\"column_mapping\": column_mapping},\n",
    "        \"groundedness\": {\"column_mapping\": column_mapping},\n",
    "        \"relevance\": {\"column_mapping\": column_mapping},\n",
    "        \"similarity\": {\"column_mapping\": column_mapping},\n",
    "        \"exact_match\": {\"column_mapping\": column_mapping},\n",
    "    },\n",
    "    azure_ai_project=azure_ai_project_dict,\n",
    "    output_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize evaluation Results as html\n",
    "- You can visualize the evaluation results as HTML with the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def generate_evaluation_report(data_file):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Load JSON file\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    rows = data.get('rows', [])\n",
    "\n",
    "    # Define score ranges\n",
    "    score_range = [0, 1, 2, 3, 4, 5]\n",
    "    freq_retrieval = {score: 0 for score in score_range}\n",
    "    freq_relevance = {score: 0 for score in score_range}\n",
    "    freq_similarity = {score: 0 for score in score_range}\n",
    "    freq_groundedness = {score: 0 for score in score_range}\n",
    "    freq_exact_match = {score: 0 for score in [0, 1]}\n",
    "\n",
    "    # Count occurrences of each score\n",
    "    for row in rows:\n",
    "        retrieval_score = row.get('outputs.retrieval.retrieval', 0)\n",
    "        relevance_score = row.get('outputs.relevance.relevance', 0)\n",
    "        similarity_score = row.get('outputs.similarity.similarity', 0)\n",
    "        groundedness_score = row.get('outputs.groundedness.groundedness', 0)\n",
    "        exact_match_score = row.get('outputs.exact_match.exact_match_score', 0)\n",
    "        \n",
    "        if retrieval_score in freq_retrieval:\n",
    "            freq_retrieval[retrieval_score] += 1\n",
    "        if relevance_score in freq_relevance:\n",
    "            freq_relevance[relevance_score] += 1\n",
    "        if similarity_score in freq_similarity:\n",
    "            freq_similarity[similarity_score] += 1\n",
    "        if groundedness_score in freq_groundedness:\n",
    "            freq_groundedness[groundedness_score] += 1\n",
    "        if exact_match_score in freq_exact_match:\n",
    "            freq_exact_match[exact_match_score] += 1\n",
    "\n",
    "    # Function to generate bar chart\n",
    "    def generate_chart(freq_dict, title):\n",
    "        fig, ax = plt.subplots()\n",
    "        x = np.arange(len(freq_dict))\n",
    "        ax.bar(x, [freq_dict.get(score, 0) for score in freq_dict], width=0.5, label=title)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(list(freq_dict.keys()))\n",
    "        ax.set_xlabel('Score')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title(title)\n",
    "        ax.legend()\n",
    "        buf = BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        chart_data = base64.b64encode(buf.read()).decode('utf-8')\n",
    "        plt.close(fig)\n",
    "        return chart_data\n",
    "\n",
    "    # Generate charts\n",
    "    retrieval_chart = generate_chart(freq_retrieval, 'Retrieval Score Distribution')\n",
    "    exact_match_chart = generate_chart(freq_exact_match, 'Exact Match Score Distribution')\n",
    "\n",
    "    # Generate combined response chart\n",
    "    def generate_response_chart():\n",
    "        fig, ax = plt.subplots()\n",
    "        x = np.arange(len(score_range))\n",
    "        width = 0.3\n",
    "        ax.bar(x - width, [freq_relevance[score] for score in score_range], width, label='Relevance')\n",
    "        ax.bar(x, [freq_similarity[score] for score in score_range], width, label='Similarity')\n",
    "        ax.bar(x + width, [freq_groundedness[score] for score in score_range], width, label='Groundedness')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(score_range)\n",
    "        ax.set_xlabel('Score')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title('Response Score Distribution')\n",
    "        ax.legend()\n",
    "        buf = BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        chart_data = base64.b64encode(buf.read()).decode('utf-8')\n",
    "        plt.close(fig)\n",
    "        return chart_data\n",
    "\n",
    "    response_chart = generate_response_chart()\n",
    "\n",
    "    # Generate HTML table\n",
    "    table_html = '<table border=\"1\" style=\"border-collapse: collapse;\"><tr><th>Query</th><th>Response</th><th>Relevance</th><th>Similarity</th><th>Groundedness</th><th>Exact Match</th></tr>'\n",
    "    for row in rows:\n",
    "        table_html += f\"<tr><td>{row.get('inputs.query', '')}</td><td>{row.get('inputs.response', '')}</td><td>{row.get('outputs.relevance.relevance', '')}</td><td>{row.get('outputs.similarity.similarity', '')}</td><td>{row.get('outputs.groundedness.groundedness', '')}</td><td>{row.get('outputs.exact_match.exact_match_score', '')}</td></tr>\"\n",
    "    table_html += '</table>'\n",
    "\n",
    "    # Generate HTML content\n",
    "    html_content = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Evaluation Results</title>\n",
    "        <style>\n",
    "            .image-container {{ display: flex; flex-wrap: wrap; justify-content: space-around; }}\n",
    "            .image-container div {{ margin: 10px; text-align: center; }}\n",
    "            img {{ max-width: 100%; height: auto; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Evaluation Results</h1>\n",
    "        <div class=\"image-container\">\n",
    "            <div><h2>Retrieval Score Distribution</h2><img src=\"data:image/png;base64,{retrieval_chart}\"/></div>\n",
    "            <div><h2>Response Score Distribution</h2><img src=\"data:image/png;base64,{response_chart}\"/></div>\n",
    "            <div><h2>Exact Match Score Distribution</h2><img src=\"data:image/png;base64,{exact_match_chart}\"/></div>\n",
    "        </div>\n",
    "        <h2>Results Table</h2>\n",
    "        {table_html}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Save HTML file with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'{timestamp}_evaluation_results.html'\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "    print(f\"HTML file '{filename}' generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluation_report('data/local_upload_cloud_evaluation_output.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"evaluation_result\"](images/evaluation_result_local_upload_cloud.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Run Evaluators in Azure Cloud (azure.ai.projects.models.Evaluation)\n",
    "- set up your custom evaluator for cloud environment with the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
    "from azure.ai.ml.entities import Model\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_or_create_model_asset(\n",
    "    ml_client,\n",
    "    model_name,\n",
    "    model_dir=\"ExactMatchEvaluator\",\n",
    "    model_type=\"custom_model\",\n",
    "    update=True,\n",
    "):\n",
    "\n",
    "    try:\n",
    "        latest_model_version = max(\n",
    "            [int(m.version) for m in ml_client.models.list(name=model_name)]\n",
    "        )\n",
    "        if update:\n",
    "            raise ResourceExistsError(\"Found Model asset, but will update the Model.\")\n",
    "        else:\n",
    "            model_asset = ml_client.models.get(\n",
    "                name=model_name, version=latest_model_version\n",
    "            )\n",
    "            logger.info(f\"Found Model asset: {model_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "\n",
    "        logger.info(f\"Exception: {e}\")\n",
    "        run_model = Model(\n",
    "            name=model_name,\n",
    "            path=model_dir,\n",
    "            description=\"Model created from run.\",\n",
    "            properties={\n",
    "                \"_default-display-file\": \"./ExactMatchEvaluator/_exact_match_cloud.py\",\n",
    "                \"is-evaluator\": True,\n",
    "                \"is-promptflow\": True,\n",
    "                \"show-artifact\": True,\n",
    "            },\n",
    "            type=model_type,\n",
    "        )\n",
    "        model_asset = ml_client.models.create_or_update(run_model)\n",
    "        logger.info(f\"Created Model asset: {model_name}\")\n",
    "\n",
    "    return model_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match_model_asset = get_or_create_model_asset(\n",
    "    ml_client,\n",
    "    model_name=\"ExactMatchEvaluator\",\n",
    "    model_dir=\"ExactMatchEvaluator\",\n",
    "    model_type=\"custom_model\",\n",
    "    update=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the azure ml asset id from Azure Machine Learning\n",
    "- You can get the asset id from Azure Machine Learning workspace.\n",
    "![azureml asset id](images/copy_azure_ml_asset_id.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id for each evaluator can be found in your AI Studio registry - please see documentation for more information\n",
    "# init_params is the configuration for the model to use to perform the evaluation\n",
    "# data_mapping is used to map the output columns of your query to the names required by the evaluator\n",
    "# Evaluator parameter format - https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk#evaluator-parameter-format\n",
    "evaluators_cloud = {\n",
    "    \"exact_match\": EvaluatorConfiguration(\n",
    "        # need to check the azure machine learning service to find the appropriate id\n",
    "        # id=\"azureml://locations/swedencentral/workspaces/c68cb823-8b8f-4f88-bcc0-2c9f49675905/models/ExactMatchEvaluator/versions/5\",\n",
    "        id=\"your azureml asset id for ExactMatchEvaluator\",\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.predicted}\",\n",
    "            \"ground_truth\": \"${data.actual}\",\n",
    "        },\n",
    "    ),\n",
    "    \"similarity\": EvaluatorConfiguration(\n",
    "        # currently bug in the SDK, please use the id below\n",
    "        # id=SimilarityEvaluator.id,\n",
    "        id=\"azureml://registries/azureml/models/Similarity-Evaluator/versions/3\",\n",
    "        init_params={\"model_config\": model_config},\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.predicted}\",\n",
    "            \"ground_truth\": \"${data.actual}\",\n",
    "        },\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "- The following code demonstrates how to upload the data for evaluation to your Azure AI project. Below we use evaluate_test_data.jsonl which exemplifies LLM-generated data in the query-response format expected by the Azure AI Evaluation SDK. For your use case, you should upload data in the same format, which can be generated using the Simulator from Azure AI Evaluation SDK.\n",
    "\n",
    "- Alternatively, if you already have an existing dataset for evaluation, you can use that by finding the link to your dataset in your registry or find the dataset ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Upload data for evaluation\n",
    "data_id, _ = azure_ai_project_client.upload_file(\"data/custom_data.jsonl\")\n",
    "# data_id = \"azureml://registries/<registry>/data/<dataset>/versions/<version>\"\n",
    "# To use an existing dataset, replace the above line with the following line\n",
    "# data_id = \"<dataset_id>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Evaluators to Run\n",
    "- The code below demonstrates how to configure the evaluators you want to run. In this example, we use the F1ScoreEvaluator, RelevanceEvaluator and the ViolenceEvaluator, but all evaluators supported by Azure AI Evaluation are supported by cloud evaluation and can be configured here. You can either import the classes from the SDK and reference them with the .id property, or you can find the fully formed id of the evaluator in the AI Studio registry of evaluators, and use it here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = Evaluation(\n",
    "    display_name=f\"custom_evaluation_cloud_{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "    description=\"Cloud Evaluation of dataset\",\n",
    "    data=Dataset(id=data_id),\n",
    "    evaluators=evaluators_cloud,\n",
    ")\n",
    "\n",
    "# Create evaluation\n",
    "evaluation_response = azure_ai_project_client.evaluations.create(\n",
    "    evaluation=evaluation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import notebook\n",
    "import time\n",
    "\n",
    "\n",
    "# Monitor the status of the run_result\n",
    "def monitor_status(project_client: AIProjectClient, evaluation_response_id: str):\n",
    "    with notebook.tqdm(total=3, desc=\"Running Status\", unit=\"step\") as pbar:\n",
    "        status = project_client.evaluations.get(evaluation_response_id).status\n",
    "        if status == \"Queued\":\n",
    "            pbar.update(1)\n",
    "        while status != \"Completed\" and status != \"Failed\":\n",
    "            if status == \"Running\" and pbar.n < 2:\n",
    "                pbar.update(1)\n",
    "            notebook.tqdm.write(f\"Current Status: {status}\")\n",
    "            time.sleep(10)\n",
    "            status = project_client.evaluations.get(evaluation_response_id).status\n",
    "        while pbar.n < 3:\n",
    "            pbar.update(1)\n",
    "        notebook.tqdm.write(\"Operation Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_status(azure_ai_project_client, evaluation_response.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add custom chart \n",
    "- After running the evaluation, you can add custom chart to visualize the evaluation results in Azure AI Foundry. \n",
    "![custom chart](images/exact_match_chart.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
