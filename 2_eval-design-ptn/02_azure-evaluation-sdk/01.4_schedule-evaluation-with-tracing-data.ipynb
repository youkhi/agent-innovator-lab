{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedule an Online Evaluation with Tracing and AI Inference SDK (deprecated in Azure AI Foundry project)\n",
    "- You can configure the RecurrenceTrigger based on the class definition here. The code below demonstrates how to configure the RecurrenceTrigger to run the evaluation every 24 hours. You can also configure the trigger to run at a different interval, or at a specific time of day. Check the Trace menu in the Azure AI Foundry to see the results of the evaluation.\n",
    "- reference: https://learn.microsoft.com/en-us/azure/ai-studio/how-to/online-evaluation\n",
    "\n",
    "> âœ¨ ***Note*** <br>\n",
    "> You application insight need to be created in Azure AI Foundry to trace your generative AI application. <br>\n",
    "> Prior to setting up online evaluation, ensure you have first set up [tracing for your generative AI application using Azure AI Inference SDK](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/trace-local-sdk?tabs=python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pprint import pprint\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "from azure.ai.evaluation import GroundednessEvaluator, GroundednessProEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    Dataset,\n",
    "    EvaluatorConfiguration,\n",
    "    ConnectionType,\n",
    "    EvaluationSchedule,\n",
    "    RecurrenceTrigger,\n",
    "    ApplicationInsightsConfiguration\n",
    ")\n",
    "import pathlib\n",
    "\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ContentSafetyEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    SimilarityEvaluator,\n",
    "    F1ScoreEvaluator,\n",
    "    RetrievalEvaluator\n",
    ")\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "\n",
    "azure_ai_project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ.get(\"AZURE_AI_PROJECT_CONN_STR\"),  # At the moment, it should be in the format \"<Region>.api.azureml.ms;<AzureSubscriptionId>;<ResourceGroup>;<HubName>\" Ex: eastus2.api.azureml.ms;xxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxxxx;rg-sample;sample-project-eastus2\n",
    ")\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    \"type\": \"azure_openai\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_insights_connection_string = azure_ai_project_client.telemetry.get_connection_string()\n",
    "if not application_insights_connection_string:\n",
    "    print(\"Application Insights was not enabled for this project.\")\n",
    "    print(\"Enable it via the 'Tracing' tab in your Azure AI Foundry project page.\")\n",
    "    exit()\n",
    "\n",
    "from azure.core.settings import settings \n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "\n",
    "# https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/trace-local-sdk?tabs=python\n",
    "os.environ['AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED'] = 'true'\n",
    "settings.tracing_implementation = \"opentelemetry\" \n",
    "\n",
    "configure_azure_monitor(connection_string=application_insights_connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.tracing import AIInferenceInstrumentor \n",
    "\n",
    "# Instrument AI Inference API to enable trace instrumentation for AI Inference\n",
    "AIInferenceInstrumentor().instrument() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage\n",
    "from azure.ai.inference.models import UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "aoai_inference_endpoint = os.getenv(\"AZURE_AI_INFERENCE_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_deployment_name = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "\n",
    "try:\n",
    "    azure_ai_inference_client = ChatCompletionsClient(\n",
    "        endpoint = aoai_inference_endpoint,\n",
    "        credential = AzureKeyCredential(aoai_api_key),\n",
    "    )\n",
    "\n",
    "    print(\"=== Initialized azure_ai_inference_client ===\")\n",
    "    print(f\"AZURE_AI_INFERENCE_ENDPOINT={aoai_inference_endpoint}\")\n",
    "    print(f\"AZURE_OPENAI_DEPLOYMENT_NAME={aoai_deployment_name}\")\n",
    "        \n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference.models import SystemMessage\n",
    "from azure.ai.inference.models import UserMessage\n",
    "\n",
    "topic = f\"\"\"\n",
    "Tell me about the history of Microsoft. \n",
    "\"\"\"\n",
    "\n",
    "system_message = \"\"\"\n",
    "Generate plain text sentences of #topic# related text to improve the recognition of domain-specific words and phrases. using json format. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_message = f\"\"\"\n",
    "#topic#: {topic}\n",
    "\"\"\"\n",
    "\n",
    "# Simple API Call\n",
    "response = azure_ai_inference_client.complete(\n",
    "    model=aoai_deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "    temperature=0.8,\n",
    "    top_p=0.1\n",
    ")\n",
    "\n",
    "content = response.choices[0].message.content\n",
    "print(content)\n",
    "print(\"Usage Information:\")\n",
    "#print(f\"Cached Tokens: {response.usage.prompt_tokens_details.cached_tokens}\") #only o1 models support this\n",
    "print(f\"Completion Tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Prompt Tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Total Tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up online evaluation schedule \n",
    "\n",
    "Evaluations are only supported in the same regions as AI-assisted risk and safety metrics. Please find the reference document here. (https://learn.microsoft.com/en-us/azure/ai-studio/how-to/online-evaluation?tabs=linux) \n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- A new User-assigned Managed Identity in the same resource group and region. Make a note of the clientId; you'll need it later.\n",
    "- An Azure AI Hub in the same resource group and region.\n",
    "- An Azure AI project in this hub, see Create a project in Azure AI Foundry portal.\n",
    "- An Azure Monitor Application Insights resource created in Azure AI Foundry portal.\n",
    "- Navigate to the hub page in Azure portal and add Application Insights resource, see Update Azure Application Insights\n",
    "- Azure OpenAI Deployment with GPT model supporting chat completion, for example gpt-4.\n",
    "- Navigate to your Application Insights resource in the Azure portal and use the Access control (IAM) tab to add the Log Analytics Contributor role to the User-assigned Managed Identity you created previously.\n",
    "- [Attach the User-assigned Managed Identity to your project.](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-identity-based-service-authentication?view=azureml-api-2&tabs=cli#add-a-user-assigned-managed-identity-to-a-workspace-in-addition-to-a-system-assigned-identity)\n",
    "- modify managed_identity_sample.yaml to attach the managed identity to your workspace\n",
    "- az ml workspace update --resource-group <RESOURCE_GROUP> --name <WORKSPACE_NAME> --file <YAML_FILE_NAME>.yaml\n",
    "- Before attaching the User-assigned Managed Identity to your project, you also need to set the permission on the user assigned managed identity including keyvault access policies. - https://learn.microsoft.com/en-us/azure/machine-learning/how-to-identity-based-service-authentication?view=azureml-api-2&tabs=cli#user-assigned-managed-identity\n",
    "- If you run into issues attaching the User-assigned Managed Identity to your project, you need to assign Key Vault Contributor role to the workspace (project) resource. \n",
    "- Navigate to your Azure AI Services in the Azure portal and use the Access control (IAM) tab to add the Cognitive Services OpenAI Contributor role to the User-assigned Managed Identity you created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_evaluator_config = EvaluatorConfiguration(\n",
    "    id=RelevanceEvaluator.id,\n",
    "    init_params={\"model_config\": model_config},\n",
    "    data_mapping={\"query\": \"${data.Input}\", \"response\": \"${data.Output}\"}\n",
    ")\n",
    "\n",
    "\n",
    "# CoherenceEvaluator\n",
    "coherence_evaluator_config = EvaluatorConfiguration(\n",
    "    id=CoherenceEvaluator.id,\n",
    "    init_params={\"model_config\": model_config},\n",
    "    data_mapping={\"query\": \"${data.Input}\", \"response\": \"${data.Output}\"}\n",
    ")\n",
    "\n",
    "evaluators_cloud = {\n",
    "    \"relevance\": relevance_evaluator_config,\n",
    "    \"coherence\" : coherence_evaluator_config\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kusto_query = 'let gen_ai_spans=(dependencies | where isnotnull(customDimensions[\"gen_ai.system\"]) | extend response_id = tostring(customDimensions[\"gen_ai.response.id\"]) | project id, operation_Id, operation_ParentId, timestamp, response_id); let gen_ai_events=(traces | where message in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") or tostring(customDimensions[\"event.name\"]) in (\"gen_ai.choice\", \"gen_ai.user.message\", \"gen_ai.system.message\") | project id= operation_ParentId, operation_Id, operation_ParentId, user_input = iff(message == \"gen_ai.user.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.user.message\", parse_json(iff(message == \"gen_ai.user.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), system = iff(message == \"gen_ai.system.message\" or tostring(customDimensions[\"event.name\"]) == \"gen_ai.system.message\", parse_json(iff(message == \"gen_ai.system.message\", tostring(customDimensions[\"gen_ai.event.content\"]), message)).content, \"\"), llm_response = iff(message == \"gen_ai.choice\", parse_json(tostring(parse_json(tostring(customDimensions[\"gen_ai.event.content\"])).message)).content, iff(tostring(customDimensions[\"event.name\"]) == \"gen_ai.choice\", parse_json(parse_json(message).message).content, \"\")) | summarize operation_ParentId = any(operation_ParentId), Input = maxif(user_input, user_input != \"\"), System = maxif(system, system != \"\"), Output = maxif(llm_response, llm_response != \"\") by operation_Id, id); gen_ai_spans | join kind=inner (gen_ai_events) on id, operation_Id | project Input, System, Output, operation_Id, operation_ParentId, gen_ai_response_id = response_id'\n",
    "\n",
    "# AzureMSIClientId is the clientID of the User-assigned managed identity created during set-up - see documentation for how to find it\n",
    "properties = {\"AzureMSIClientId\": os.environ.get(\"AZURE_MSI_CLIENT_ID\")}\n",
    "\n",
    "service_name = \"evaluation_sdk_schedule\"\n",
    "\n",
    "# Your Application Insights resource ID\n",
    "# At the moment, it should be something in the format \"/subscriptions/<AzureSubscriptionId>/resourceGroups/<ResourceGroup>/providers/Microsoft.Insights/components/<ApplicationInsights>\"\"\n",
    "app_insights_resource_id = os.environ.get(\"APP_INSIGHTS_RESOURCE_ID\")\n",
    "\n",
    "# Connect to your Application Insights resource\n",
    "app_insights_config = ApplicationInsightsConfiguration(\n",
    "    resource_id=app_insights_resource_id, query=kusto_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency to run the schedule\n",
    "recurrence_trigger = RecurrenceTrigger(frequency=\"Hour\", interval=1)\n",
    "\n",
    "# Configure the online evaluation schedule\n",
    "evaluation_schedule = EvaluationSchedule(\n",
    "    data=app_insights_config,\n",
    "    evaluators=evaluators_cloud,\n",
    "    trigger=recurrence_trigger,\n",
    "    description=f\"scheduled evaluation\",\n",
    "    properties=properties\n",
    ")\n",
    "\n",
    "# Create the online evaluation schedule\n",
    "created_evaluation_schedule = azure_ai_project_client.evaluations.create_or_replace_schedule(service_name, evaluation_schedule)\n",
    "print(\n",
    "    f\"Successfully submitted the online evaluation schedule creation request - {created_evaluation_schedule.name}, currently in {created_evaluation_schedule.provisioning_state} state.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "evaluation_schedule = azure_ai_project_client.evaluations.get_schedule(service_name)\n",
    "print(evaluation_schedule)\n",
    "\n",
    "# Sample for list evaluation schedules\n",
    "for evaluation_schedule in azure_ai_project_client.evaluations.list_schedule():\n",
    "    print(evaluation_schedule)\n",
    "\n",
    "# Sample for disable an evaluation schedule with name\n",
    "# project_client.evaluations.disable_schedule(service_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for evaluation_schedule in azure_ai_project_client.evaluations.list_schedule():\n",
    "    count += 1\n",
    "    print(f\"{count}.{evaluation_schedule.name} \"\n",
    "    f\"[IsEnabled: {evaluation_schedule.is_enabled}]\")\n",
    "    print(f\"Total evaluation schedules: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### go to the workspace to check the instance_results.jsonl files on the Outputs + log tab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![instance_results](./images/instance_results_jsonl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disable (soft-delete) online evaluation schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_service_name = \"evaluation_sdk_schedule\"\n",
    "# azure_ai_project_client.evaluations.disable_schedule(target_service_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
