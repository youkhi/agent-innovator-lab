{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26ab996",
   "metadata": {},
   "source": [
    "# Understand the kernel with code samples\n",
    "\n",
    "----\n",
    "The kernel is the central component of Semantic Kernel. At its simplest, the kernel is a Dependency Injection container that manages all of the services and plugins necessary to run your AI application. If you provide all of your services and plugins to the kernel, they will then be seamlessly used by the AI as needed. \n",
    "\n",
    "| Components | Description |\n",
    "|------------|-------------|\n",
    "| **1. Services** | These consist of both AI services (e.g., chat completion) and other services (e.g., logging and HTTP clients) that are necessary to run your application. This was modeled after the Service Provider pattern in .NET to support dependency injection across all languages. |\n",
    "| **2. Plugins** | These are components used by your AI services and prompt templates to perform work. For example, AI services can use plugins to retrieve data from a database or call an external API to perform actions. |\n",
    "\n",
    "![kernel](images/the-kernel-is-at-the-center-of-everything.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e456fe",
   "metadata": {},
   "source": [
    "## Hierarchical Architecture Diagram\n",
    "---\n",
    "\n",
    "Architecture of a Retrieval-Augmented Generation (RAG) based chatbot using the latest Semantic Kernel. The diagram highlights how the Agent Framework (for chat interactions) and Process Framework (for workflow orchestration) sit above the core Kernel, leveraging it to perform AI tasks. The Semantic Kernel core orchestrates between user requests and the underlying AI services, memory stores, and plugins. It integrates with external LLM services for generation, uses a vector DB for semantic memory (retrieval of relevant data), and can invoke plugins which may call other external APIs or services as needed.\n",
    "\n",
    "![Hierarchical](images/semantic_kernel_component.png)\n",
    "\n",
    "***AI Agent (Agent Framework)***: The Agent Framework in Semantic Kernel is an optional layer that helps create conversational AI agents (like chatbots) using the core kernelâ€™s capabilitiesâ€‹. It is not a replacement for the kernel but builds on top of it â€“ your application still includes the Semantic Kernel library, and the agent uses the kernelâ€™s functions internally. In a chatbot scenario, the Agent Framework manages the dialogue (turn-taking, system prompts, etc.) while delegating AI tasks to the kernel.\n",
    "\n",
    "***Semantic Kernel (Core)***: The Semantic Kernel core is the heart of the system. It orchestrates calls to AI models, retrieves memories, and executes plugin functions. The kernel provides abstractions to connect to AI services (LLM APIs for text generation, embedding models, etc.) and to various memory stores (for example, vector databases)â€‹. It also manages context (prompts) and can use Planners to chain or select functions dynamically to fulfill a user requestâ€‹. The kernel itself is part of your appâ€™s runtime, coordinating all other components.\n",
    "\n",
    "***Plugins (Skills/Functions)***: Plugins (also called skills or functions) are units of functionality that the kernel can invoke. They might be defined with natural language prompts (semantic functions) or as native code functions. Plugins can perform calculations, transform data, or call external services/APIs. They are registered with and executed via the kernel, meaning they depend on the kernel to be invoked as part of an AI workflowâ€‹. However, the plugin implementations (e.g. an HTTP call to a web service, a database query) run outside the kernel â€“ the kernel just orchestrates their usage. In essence, plugins extend the kernelâ€™s abilities, and the kernel can automatically chain plugins to accomplish complex tasks for the userâ€‹.\n",
    "\n",
    "***AI Services***: These are external AI model endpoints that the kernel calls through its connectors. For example, the OpenAI or Azure OpenAI service provides the GPT-4 model for text generation, and there are embedding model services for vector generation. The kernel has integrations for many AI services (text completion, chat, image generation, speech recognition, etc.) and it uses them by calling out to the respective APIsâ€‹. These services are not â€œinsideâ€ the kernel â€“ instead, the kernel depends on them to provide the intelligence. In the architecture, they appear as external components that the kernel invokes (e.g. sending a prompt to the GPT model and getting a completion).\n",
    "\n",
    "***Semantic Memory (Vector DB)***: The Semantic Kernel includes a memory abstraction that allows storing and retrieving contextual information. Under the hood, this is often backed by a vector database or search index. In a RAG-based chatbot, this is critical: documents or knowledge are embedded into vector representations and stored, so that relevant pieces can be retrieved to ground the AIâ€™s answers. Semantic Kernelâ€™s memory can integrate with many vector stores (e.g. Azure Cognitive Search, ChromaDB, Qdrant, Redis, Pinecone, etc.)â€‹. This means â€œMemoryâ€ is essentially an AI Search over a vector DB, enabling the bot to find relevant information by semantic similarity. The memory component is used by the kernel but the database itself is external â€“ the kernel just sends queries and gets results.\n",
    "\n",
    "***Process Framework (Workflow Orchestration)***: The Process Framework is another optional layer in the latest Semantic Kernel, aimed at long-running or multi-step workflows. It lets developers define structured processes composed of multiple steps, where each step can call kernel functions (AI or non-AI tasks) in an event-driven sequenceâ€‹. Like the Agent Framework, the Process Framework builds on the kernel rather than enclosing it. It uses the kernel to execute AI functions at each step of a business workflow. This is especially useful if your chatbot or assistant needs to carry out complex transactions or procedures (for example, an order processing workflow or a support ticket resolution that involves several back-and-forth steps) beyond a single conversational turn. The Process Framework uses technologies like Orleans or Dapr under the hood for reliability and can reuse any existing kernel plugins within those processesâ€‹."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b243d6",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae1d038",
   "metadata": {},
   "source": [
    "Let's define our kernel for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "651e17cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import Annotated\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "from enum import Enum\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "from semantic_kernel.agents import AssistantAgentThread, AzureAssistantAgent\n",
    "from semantic_kernel.kernel_pydantic import KernelBaseSettings\n",
    "from semantic_kernel.connectors.search.bing import BingSearch\n",
    "from semantic_kernel.functions import KernelArguments, KernelParameterMetadata, KernelPlugin\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.filters import FilterTypes, FunctionInvocationContext\n",
    "from collections.abc import Awaitable, Callable\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.agents import AzureAIAgent, AzureAIAgentSettings, AzureAIAgentThread\n",
    "from semantic_kernel.agents import AzureResponsesAgent, ResponsesAgentThread\n",
    "from semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.agents import AzureResponsesAgent\n",
    "from azure.identity.aio import DefaultAzureCredential\n",
    "from semantic_kernel.agents.strategies.termination.termination_strategy import TerminationStrategy\n",
    "from semantic_kernel.agents import AgentGroupChat\n",
    "from semantic_kernel.contents.utils.author_role import AuthorRole\n",
    "from semantic_kernel.contents import AuthorRole\n",
    "from semantic_kernel.contents.streaming_chat_message_content import StreamingChatMessageContent\n",
    "from azure.ai.agents.models import OpenApiAnonymousAuthDetails, OpenApiTool, CodeInterpreterTool\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "\n",
    "\n",
    "class Service(Enum):\n",
    "    \"\"\"Attributes:\n",
    "    OpenAI (str): Represents the OpenAI service.\n",
    "    AzureOpenAI (str): Represents the Azure OpenAI service.\n",
    "    HuggingFace (str): Represents the HuggingFace service.\n",
    "    \"\"\"\n",
    "\n",
    "    OpenAI = \"openai\"\n",
    "    AzureOpenAI = \"azureopenai\"\n",
    "    HuggingFace = \"huggingface\"\n",
    "\n",
    "class ServiceSettings(KernelBaseSettings):\n",
    "    \"\"\"The Learn Resources Service Settings.\n",
    "\n",
    "    The settings are first loaded from environment variables. If the\n",
    "    environment variables are not found, the settings can be loaded from a .env file with the\n",
    "    encoding 'utf-8' as default or the specific encoding. If the settings are not found in the\n",
    "    .env file, the settings are ignored; however, validation will fail alerting that the settings\n",
    "    are missing.\n",
    "\n",
    "    Args:\n",
    "        global_llm_service (str | None): The LLM service to use for the samples, either \"OpenAI\" or \"AzureOpenAI\"\n",
    "            If not provided, defaults to \"AzureOpenAI\".\n",
    "    \"\"\"\n",
    "\n",
    "    global_llm_service: str | None = None\n",
    "    \n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1b01f0",
   "metadata": {},
   "source": [
    "We will load our settings and get the LLM service to use for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b0c8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The create method is deprecated. Use the __new__ method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using service type: Service.AzureOpenAI\n"
     ]
    }
   ],
   "source": [
    "service_settings = ServiceSettings.create()\n",
    "\n",
    "# Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
    "selectedService = (\n",
    "    Service.AzureOpenAI\n",
    "    if service_settings.global_llm_service is None\n",
    "    else Service(service_settings.global_llm_service.lower())\n",
    ")\n",
    "print(f\"Using service type: {selectedService}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793ed61f",
   "metadata": {},
   "source": [
    "We now configure our Chat Completion service on the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed56686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all services so that this cell can be re-run without restarting the kernel\n",
    "kernel.remove_all_services()\n",
    "\n",
    "service_id = None\n",
    "if selectedService == Service.OpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(\n",
    "            service_id=service_id,\n",
    "        ),\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(\n",
    "            service_id=service_id,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c79873",
   "metadata": {},
   "source": [
    "# Run a Semantic Function (No Agent, No Process)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd6ea0",
   "metadata": {},
   "source": [
    "## ðŸ§ª Case 1.1 running a prompt without plugin\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a84f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft was founded by Bill Gates and Paul Allen in 1975,  \n",
      "Initially created a version of the BASIC programming language for Altair 8800,  \n",
      "In 1980, they signed a contract with IBM to provide an operating system,  \n",
      "This led to the creation of MS-DOS, which became a dominant OS for PCs,  \n",
      "In 1985, Microsoft launched Windows 1.0, a graphical extension for MS-DOS,  \n",
      "Windows 3.0 was released in 1990, gaining significant market share,  \n",
      "The 1995 launch of Windows 95 marked a major success, featuring a new user interface,  \n",
      "Microsoft Office suite was introduced, becoming essential for businesses,  \n",
      "The late 1990s saw antitrust lawsuits against Microsoft for monopolistic practices,  \n",
      "In 2001, Windows XP was released, praised for its stability and user-friendly design,  \n",
      "The rise of the internet led to the launch of Internet Explorer, competing with Netscape,  \n",
      "In 2007, Microsoft introduced Windows Vista, followed by Windows 7 in 2009,  \n",
      "The company expanded into gaming with the Xbox, launched in 2001,  \n",
      "Microsoft acquired LinkedIn in 2016, enhancing its enterprise services,  \n",
      "In 2020, they focused on cloud computing with Azure, becoming a leader in the field,  \n",
      "Microsoft remains a key player in software, hardware, and cloud solutions today."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the request settings\n",
    "req_settings = kernel.get_prompt_execution_settings_from_service_id(service_id)\n",
    "req_settings.max_tokens = 2000\n",
    "req_settings.temperature = 0.7\n",
    "req_settings.top_p = 0.8\n",
    "\n",
    "prompt=\"tell me about the history of {{$company_name}} as TLDR in exactly 200 words. make line seperated by commas. \\n\\nTLDR:\"\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"tldr\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    execution_settings=req_settings,\n",
    ")\n",
    "\n",
    "function = kernel.add_function(\n",
    "    function_name=\"tldr_function\",\n",
    "    plugin_name=\"tldr_plugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")\n",
    "\n",
    "\n",
    "########################################\n",
    "# invoke mode \n",
    "########################################\n",
    "\n",
    "# result = await kernel.invoke(function)\n",
    "# print(result) # => Robots must not harm humans.\n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "\n",
    "response = kernel.invoke_stream(plugin_name=\"tldr_plugin\", function_name=\"tldr_function\", company_name=\"Microsoft\")\n",
    "all_chunks_str = \"\"\n",
    "print(\"Assistant:> \", end=\"\")\n",
    "async for chunk in response:\n",
    "    if isinstance(chunk[0], StreamingChatMessageContent) and chunk[0].role == AuthorRole.ASSISTANT:\n",
    "        all_chunks_str += str(chunk[0])\n",
    "        print(str(chunk[0]), end=\"\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef0c277",
   "metadata": {},
   "source": [
    "## ðŸ§ª Case 1.2 running a prompt with plugin\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b34af3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plugin = kernel.add_plugin(parent_directory=\"prompt_template_samples/\", plugin_name=\"FunPlugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d008b3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the time traveler bring a ladder to the dinosaur age?\n",
      "\n",
      "Because he heard the T-Rex was a little \"short\" on friends and wanted to help him reach new heights in socializing! ðŸ¦–ðŸ˜‚\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "joke_function = plugin[\"Joke\"]\n",
    "\n",
    "joke = await kernel.invoke(\n",
    "    joke_function,\n",
    "    KernelArguments(input=\"time travel to dinosaur age\", style=\"super silly\"),\n",
    ")\n",
    "print(joke)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece006e",
   "metadata": {},
   "source": [
    "## ðŸ§ª Case 1.3 running a prompt with Bing Search v7 plugin\n",
    "---\n",
    "- This case shows how to use the Bing Search v7 plugin to search the web and return results.\n",
    "- You must have a valid Bing Search v7 API key to run this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eee4e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the chat bot!    \n",
      "  Type 'exit' to exit.    \n",
      "  Try to search weather, news and more using bing search tool.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Bing search with arguments:\n",
      "  Query: \"ê°€ìž¥ ì¢‹ì€ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ 2023\"\n",
      "Bing search completed.\n",
      "SearchChatbot:> 2023ë…„ ê°€ìž¥ ì¢‹ì€ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ëª¨ë¸ë“¤ì´ ìžˆìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **Falcon LLM**: ì´ ëª¨ë¸ì€ 1,800ì–µ ê°œì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§€ê³  ìžˆìœ¼ë©°, 3ì¡° 5,000ì–µ ê°œì˜ í† í°ìœ¼ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤. í—ˆê¹…íŽ˜ì´ìŠ¤ì˜ ì˜¤í”ˆ LLM ë¦¬ë”ë³´ë“œì—ì„œ 1ìœ„ë¥¼ ì°¨ì§€í–ˆìœ¼ë©°, ìƒì—…ìš© ë° ì—°êµ¬ìš©ìœ¼ë¡œ ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "\n",
      "2. **Gemini**: êµ¬ê¸€ê³¼ ë”¥ë§ˆì¸ë“œê°€ ê°œë°œí•œ ë©€í‹°ëª¨ë‹¬ ìƒì„±í˜• ì¸ê³µì§€ëŠ¥ ëª¨ë¸ë¡œ, 2023ë…„ 5ì›”ì— ì²˜ìŒ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•˜ë©°, ì—¬ëŸ¬ ê°€ì§€ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **Adobe Firefly**: Adobeì˜ FireflyëŠ” ì¼ëŸ¬ìŠ¤íŠ¸ë ˆì´ì…˜, ì•„íŠ¸ ì»¨ì…‰, ì‚¬ì§„ ì¡°ìž‘ ë“± ë‹¤ì–‘í•œ ì‹œê° ì½˜í…ì¸  ìƒì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, Photoshopì— í†µí•©ë˜ì–´ ì‚¬ìš©ìžì—ê²Œ AIì˜ íž˜ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ ì™¸ì—ë„ ì—¬ëŸ¬ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ë“¤ì´ ì¡´ìž¬í•˜ì§€ë§Œ, ìœ„ì˜ ëª¨ë¸ë“¤ì´ íŠ¹ížˆ ì£¼ëª©ë°›ê³  ìžˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "\n",
      "Exiting chat...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import semantic_kernel.connectors.search.bing\n",
    "\n",
    "# BING_API_KEY needs to be set in your environment variables\n",
    "if not os.getenv(\"BING_API_KEY\"):\n",
    "    raise ValueError(\"BING_API_KEY environment variable is not set. Please set it to your Bing Search API key.\")\n",
    "\n",
    "\n",
    "kernel = Kernel()\n",
    "kernel.add_service(AzureChatCompletion(service_id=\"chat\"))\n",
    "bing_search_plugin = kernel.add_plugin(KernelPlugin.from_text_search_with_search(\n",
    "        semantic_kernel.connectors.search.bing.BingSearch(),\n",
    "        plugin_name=\"bing\",\n",
    "        description=\"Search the web information using bing search.\",\n",
    "        parameters=[\n",
    "            KernelParameterMetadata(\n",
    "                name=\"query\",\n",
    "                description=\"The search query.\",\n",
    "                type=\"str\",\n",
    "                is_required=True,\n",
    "                type_object=str,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"top\",\n",
    "                description=\"The number of results to return.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=2,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"skip\",\n",
    "                description=\"The number of results to skip.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=0,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"site\",\n",
    "                description=\"The site to search.\",\n",
    "                default_value=\"\",\n",
    "                type=\"str\",\n",
    "                is_required=False,\n",
    "                type_object=str,\n",
    "            ),\n",
    "        ],\n",
    "    ))\n",
    "\n",
    "chat_function = kernel.add_function(\n",
    "    prompt=\"{{$chat_history}}{{$user_input}}\",\n",
    "    plugin_name=\"ChatBot\",\n",
    "    function_name=\"Chat\",\n",
    ")\n",
    "execution_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=\"chat\",\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    function_choice_behavior=FunctionChoiceBehavior.Auto(auto_invoke=True),\n",
    ")\n",
    "\n",
    "history = ChatHistory()\n",
    "system_message = \"\"\"\n",
    "You are a chat bot, use bing plugin to find answers.\n",
    "\"\"\"\n",
    "history.add_system_message(system_message)\n",
    "history.add_user_message(\"Hi there, who are you?\")\n",
    "history.add_assistant_message(\"I am Information Finder, a chat bot. use bing plugin to find answers.\")\n",
    "\n",
    "arguments = KernelArguments(settings=execution_settings)\n",
    "\n",
    "@kernel.filter(filter_type=FilterTypes.FUNCTION_INVOCATION)\n",
    "async def log_bing_filter(\n",
    "    context: FunctionInvocationContext, next: Callable[[FunctionInvocationContext], Awaitable[None]]\n",
    "):\n",
    "    if context.function.plugin_name == \"bing\":\n",
    "        print(\"Calling Bing search with arguments:\")\n",
    "        if \"query\" in context.arguments:\n",
    "            print(f'  Query: \"{context.arguments[\"query\"]}\"')\n",
    "        if \"count\" in context.arguments:\n",
    "            print(f'  Count: \"{context.arguments[\"count\"]}\"')\n",
    "        if \"skip\" in context.arguments:\n",
    "            print(f'  Skip: \"{context.arguments[\"skip\"]}\"')\n",
    "        await next(context)\n",
    "        print(\"Bing search completed.\")\n",
    "    else:\n",
    "        await next(context)\n",
    "\n",
    "\n",
    "async def chat() -> bool:\n",
    "    try:\n",
    "        user_input = input(\"User:> \")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    except EOFError:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    if user_input == \"exit\":\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    arguments[\"user_input\"] = user_input\n",
    "    arguments[\"chat_history\"] = history\n",
    "    result = await kernel.invoke(chat_function, arguments=arguments)\n",
    "    print(f\"SearchChatbot:> {result}\")\n",
    "    history.add_user_message(user_input)\n",
    "    history.add_assistant_message(str(result))\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "chatting = True\n",
    "print(\n",
    "    \"Welcome to the chat bot!\\\n",
    "    \\n  Type 'exit' to exit.\\\n",
    "    \\n  Try to search weather, news and more using bing search tool.\"\n",
    ")\n",
    "while chatting:\n",
    "    chatting = await chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbc0df44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Bing search with arguments:\n",
      "  Query: \"what is new opening hotels in NYC 2025?\"\n",
      "  Skip: \"0\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bing search completed.\n",
      "We try to keep our pages about new hotels in NYC as up-to-date as possible and will be updating this page when any new hotels open or when any upcoming hotels in NYC open for reservation.,New York is poised to welcome new hotels in New York City and beyond in 2025. Add these five to your must-stay list.,Now Now NoHo â€“ A Unique Sleeper Cabin Hotel in Manhattan Now Now NoHo is a distinctive sleeper cabin hotel set to open on April 1, 2025, at 338 Bowery in Manhattanâ€™s NoHo neighborhood.,Opened Hotels in New York We have created a list with the best newly opened hotels in New York, USA. Hope you enjoy it. We think it might be the best of its kind around. Tip: Set your dates to see room prices!,A list of my most anticipated New Hotel Openings or Renovations for 2025. Most, but not all, are Resorts, many are Luxury or Boutique properties. Hopefully they will be bringing something new and unique to the world of hospitality.,The top luxury hotel openings in 2025, bringing world-class event venues, corporate meeting spaces, and private party spaces to New York City.,Keep scrolling to see which properties caught our eye, including the first-ever Graduate Hotel in Texas and the reopening of a swanky South Florida resort. A historic, transformed New York City property is expected to open its doors in spring 2025.,Luxury travel in 2025 just got even more exciting! Uncover the hottest new hotel openingsâ€”from dreamy Costa Rican beaches to lavish European escapes. Donâ€™t miss these five-star destinations redefining indulgenceâ€”start planning now.,Here are five new and exciting luxury hotels in the Big Apple, some already in full swing, and others gearing up to welcome guests, from storied Grand Dames to the playfully high-end and seductive.,Donâ€™t miss 25 more 2025 new hotels beyond those detailed below that deserve to be on your radar. The iconic Waldorf Astoria New York is set to welcome guests to its Midtown East address for the first time since 2017 following a meticulous multi-million dollar restoration.\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "bing_search_fuction = kernel.get_function(\"bing\", \"search\")\n",
    "\n",
    "search_result = await kernel.invoke(\n",
    "    bing_search_fuction,\n",
    "    KernelArguments(\n",
    "        query=\"what is new opening hotels in NYC 2025?\",\n",
    "        top=10,\n",
    "        skip=0,\n",
    "    ),\n",
    ")\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8505e28c",
   "metadata": {},
   "source": [
    "# Use single agent to chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# The following sample demonstrates how to create a simple,       #\n",
    "# Azure AI agent, ChatCompletionAgent, AzureResponsesAgent        #\n",
    "# that answers questions about a sample menu                      #\n",
    "# using a Semantic Kernel Plugin.                                 #\n",
    "###################################################################\n",
    "\n",
    "\n",
    "# Define a sample plugin for the sample\n",
    "class MenuPlugin:\n",
    "    \"\"\"A sample Menu Plugin used for the concept sample.\"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Provides a list of specials from the menu.\")\n",
    "    def get_specials(self) -> Annotated[str, \"Returns the specials from the menu.\"]:\n",
    "        return \"\"\"\n",
    "        Special Soup: Clam Chowder\n",
    "        Special Salad: Cobb Salad\n",
    "        Special Drink: Chai Tea\n",
    "        \"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Provides the price of the requested menu item.\")\n",
    "    def get_item_price(\n",
    "        self, menu_item: Annotated[str, \"The name of the menu item.\"]\n",
    "    ) -> Annotated[str, \"Returns the price of the menu item.\"]:\n",
    "        return \"$9.99\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39602eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_INPUTS = [\n",
    "    \"Hello\",\n",
    "    \"What is the special soup?\",\n",
    "    \"How much does that cost?\",\n",
    "    \"Thank you\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f952185",
   "metadata": {},
   "source": [
    "## ðŸ§ª Case 2.1 chat with Azure AI Agent \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65844e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'What is the name and population of the country that uses currency with abbreviation KRW'\n",
      "# Host: The country that uses the currency with the abbreviation **KRW** (Korean Won) is **South Korea**. As of 2023, the population of South Korea is approximately **52 million** people.\n",
      "# User: 'What is the current weather in the capital city of the country?'\n",
      "# Host: I cannot provide real-time weather updates. To find the current weather in **Seoul**, the capital city of South Korea, you can check a weather service like [Weather.com](https://www.weather.com) or use a weather app for the most accurate and up-to-date information.\n"
     ]
    }
   ],
   "source": [
    "# Azure AI Agent Endpoint and model deployment name need to be set in your environment variables\n",
    "if not os.getenv(\"AZURE_AI_AGENT_ENDPOINT\"):\n",
    "    raise ValueError(\"AZURE_AI_AGENT_ENDPOINT environment variable is not set. Please set it to your Azure AI Agent endpoint.\")\n",
    "if not os.getenv(\"AZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME\"):\n",
    "    raise ValueError(\"AZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME environment variable is not set. Please set it to your Azure AI Agent model deployment name.\")\n",
    "\n",
    "# Make sure to set the environment variables for the Azure AI Agent\n",
    "ai_agent_settings = AzureAIAgentSettings()\n",
    "\n",
    "\n",
    "async with (\n",
    "    DefaultAzureCredential() as creds,\n",
    "    AzureAIAgent.create_client(credential=creds) as client,\n",
    "):\n",
    "    # 1. Create an agent on the Azure AI agent service\n",
    "    agent_definition = await client.agents.create_agent(\n",
    "        model=ai_agent_settings.model_deployment_name,\n",
    "        name=\"Host\",\n",
    "        instructions=\"Answer questions about the menu.\",\n",
    "    )\n",
    "\n",
    "    # 2. Create a Semantic Kernel agent for the Azure AI agent\n",
    "    agent = AzureAIAgent(\n",
    "        client=client,\n",
    "        definition=agent_definition,\n",
    "        plugins=[MenuPlugin()],  # Add the plugin to the agent\n",
    "    )\n",
    "\n",
    "    # 3. Create a thread for the agent\n",
    "    # If no thread is provided, a new thread will be\n",
    "    # created and returned with the initial response\n",
    "    thread: AzureAIAgentThread = None\n",
    "\n",
    "########################################\n",
    "# invoke mode \n",
    "########################################\n",
    "\n",
    "    # for user_input in USER_INPUTS:\n",
    "    #     print(f\"# User: {user_input}\")\n",
    "    #     # 4. Invoke the agent for the specified thread for response\n",
    "    #     async for response in agent.invoke(\n",
    "    #         messages=user_input,\n",
    "    #         thread=thread,\n",
    "    #     ):\n",
    "    #         print(f\"# {response.name}: {response}\")\n",
    "    #         thread = response.thread\n",
    "    \n",
    "    # await thread.delete() if thread else None    \n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "# Simulate a conversation with the agent\n",
    "\n",
    "    try:\n",
    "        for user_input in USER_INPUTS:\n",
    "            print(f\"# User: '{user_input}'\")\n",
    "            first_chunk = True\n",
    "            # 4. Invoke the agent for the current message and print the response\n",
    "            async for response in agent.invoke_stream(messages=user_input, thread=thread):\n",
    "                thread = response.thread\n",
    "                if first_chunk:\n",
    "                    print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "                    first_chunk = False\n",
    "                print(response.content, end=\"\", flush=True)\n",
    "            print()\n",
    "    finally:\n",
    "            # Cleanup: Delete the thread and agent\n",
    "            await thread.delete() if thread else None\n",
    "            await client.agents.delete_agent(agent.id)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024625db",
   "metadata": {},
   "source": [
    "## ðŸ§ª Case 2.2 chat with ChatCompletionAgent\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68221ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Hello'\n",
      "# Host: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "# User: 'What is the special soup?'\n",
      "# Host: The special soup is Clam Chowder. Would you like to know more about it or anything else?\n",
      "# User: 'How much does that cost?'\n",
      "# Host: The Clam Chowder costs $9.99. Would you like to know anything else?\n",
      "# User: 'Thank you'\n",
      "# Host: You're welcome! If you have any more questions or need assistance, feel free to ask. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "#  need to be set in your environment variables - from semantic_kernel.connectors.ai.open_ai.settings.azure_open_ai_settings import AzureOpenAISettings\n",
    "if not os.getenv(\"AZURE_OPENAI_ENDPOINT\"):\n",
    "    raise ValueError(\"AZURE_OPENAI_ENDPOINT environment variable is not set. Please set it to your Azure OpenAI endpoint.\")\n",
    "if not os.getenv(\"AZURE_OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"AZURE_OPENAI_API_KEY environment variable is not set. Please set it to your Azure OpenAI API key.\")\n",
    "if not os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"):\n",
    "    raise ValueError(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME environment variable is not set. Please set it to your Azure OpenAI chat deployment name.\")\n",
    "\n",
    "\n",
    "\n",
    "agent = ChatCompletionAgent(\n",
    "        service=AzureChatCompletion(),\n",
    "        name=\"Host\",\n",
    "        instructions=\"Answer questions based on web search\",\n",
    "        plugins=[MenuPlugin()],\n",
    "    )\n",
    "\n",
    "# 2. Create a thread to hold the conversation\n",
    "# If no thread is provided, a new thread will be\n",
    "# created and returned with the initial response\n",
    "thread: ChatHistoryAgentThread = None\n",
    "\n",
    "########################################\n",
    "# invoke mode \n",
    "########################################\n",
    "\n",
    "# for user_input in USER_INPUTS:\n",
    "#     print(f\"# User: {user_input}\")\n",
    "#     # 4. Invoke the agent for a response\n",
    "#     response = await agent.get_response(messages=user_input, thread=thread)\n",
    "#     print(f\"# {response.name}: {response} \")\n",
    "#     thread = response.thread\n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "\n",
    "for user_input in USER_INPUTS:\n",
    "        print(f\"# User: '{user_input}'\")\n",
    "        first_chunk = True\n",
    "        # 4. Invoke the agent for the current message and print the response\n",
    "        async for response in agent.invoke_stream(messages=user_input, thread=thread):\n",
    "            thread = response.thread\n",
    "            if first_chunk:\n",
    "                print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "                first_chunk = False\n",
    "            print(response.content, end=\"\", flush=True)\n",
    "        print()\n",
    "\n",
    "\n",
    "await thread.delete() if thread else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732bbee4",
   "metadata": {},
   "source": [
    "## ðŸ§ª Case 2.3 chat with ResponseAgent (OpenAI Responses API)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f82cb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Hello'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Host: Hi there! How can I assist you today?\n",
      "# User: 'What is the special soup?'\n",
      "# Host: The special soup is Clam Chowder. Is there anything else you'd like to know?\n",
      "# User: 'How much does that cost?'\n",
      "# Host: The Clam Chowder costs $9.99. Would you like to know anything else?\n",
      "# User: 'Thank you'\n",
      "# Host: You're welcome! If you have any more questions, feel free to ask. Enjoy your day!\n"
     ]
    }
   ],
   "source": [
    "#  need to be set in your environment variables - from semantic_kernel.connectors.ai.open_ai.settings.azure_open_ai_settings import AzureOpenAISettings\n",
    "if not os.getenv(\"AZURE_OPENAI_ENDPOINT\"):\n",
    "    raise ValueError(\"AZURE_OPENAI_ENDPOINT environment variable is not set. Please set it to your Azure OpenAI endpoint.\")\n",
    "if not os.getenv(\"AZURE_OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"AZURE_OPENAI_API_KEY environment variable is not set. Please set it to your Azure OpenAI API key.\")\n",
    "if not os.getenv(\"AZURE_OPENAI_RESPONSES_DEPLOYMENT_NAME\"):\n",
    "    raise ValueError(\"AZURE_OPENAI_RESPONSES_DEPLOYMENT_NAME environment variable is not set. Please set it to your Azure OpenAI responses deployment name.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "client = AzureResponsesAgent.create_client()\n",
    "\n",
    "# 2. Create a Semantic Kernel agent for the OpenAI Responses API\n",
    "agent = AzureResponsesAgent(\n",
    "    ai_model_id=\"gpt-4o-mini\",\n",
    "    client=client,\n",
    "    instructions=\"Answer questions about the menu.\",\n",
    "    name=\"Host\",\n",
    "    plugins=[MenuPlugin()],\n",
    ")\n",
    "\n",
    "# 3. Create a thread for the agent\n",
    "# If no thread is provided, a new thread will be\n",
    "# created and returned with the initial response\n",
    "thread: ResponsesAgentThread = None\n",
    "\n",
    "########################################\n",
    "# invoke mode \n",
    "########################################\n",
    "# for user_input in USER_INPUTS:\n",
    "#     print(f\"# User: '{user_input}'\")\n",
    "#     # 4. Invoke the agent for the current message and print the response\n",
    "#     response = await agent.get_response(messages=user_input, thread=thread)\n",
    "#     print(f\"# {response.name}: {response.content}\")\n",
    "#     thread = response.thread\n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "for user_input in USER_INPUTS:\n",
    "        print(f\"# User: '{user_input}'\")\n",
    "        first_chunk = True\n",
    "        # 4. Invoke the agent for the current message and print the response\n",
    "        async for response in agent.invoke_stream(messages=user_input, thread=thread):\n",
    "            thread = response.thread\n",
    "            if first_chunk:\n",
    "                print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "                first_chunk = False\n",
    "            print(response.content, end=\"\", flush=True)\n",
    "        print()\n",
    "\n",
    "await thread.delete() if thread else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c01be",
   "metadata": {},
   "source": [
    "# Use tools to answer specific questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f8d674",
   "metadata": {},
   "source": [
    "## ðŸ§ª Case 3.1 code interpreter with Azure AI Agent \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c502bca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The create method is deprecated. Use the __new__ method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Use code to determine the values in the Fibonacci sequence that that are less then the value of 101.'\n",
      "# Agent: # Create a Fibonacci sequence with numbers less than 101\n",
      "fibonacci_sequence = []\n",
      "a, b = 0, 1  # Starting values of Fibonacci numbers\n",
      "\n",
      "while a < 101:\n",
      "    fibonacci_sequence.append(a)\n",
      "    a, b = b, a + b  # Move to the next Fibonacci number\n",
      "\n",
      "fibonacci_sequence\n",
      "# Agent: The values in the Fibonacci sequence that are less than 101 are:\n",
      "\n",
      "\\[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89\\]\n"
     ]
    }
   ],
   "source": [
    "TASK = \"Use code to determine the values in the Fibonacci sequence that that are less then the value of 101.\"\n",
    "\n",
    "ai_agent_settings = AzureAIAgentSettings()\n",
    "\n",
    "async with (\n",
    "    DefaultAzureCredential() as creds,\n",
    "    AzureAIAgent.create_client(credential=creds) as client,\n",
    "):\n",
    "    # 1. Create an agent with a code interpreter on the Azure AI agent service\n",
    "    code_interpreter = CodeInterpreterTool()\n",
    "    agent_definition = await client.agents.create_agent(\n",
    "        model=ai_agent_settings.model_deployment_name,\n",
    "        tools=code_interpreter.definitions,\n",
    "        tool_resources=code_interpreter.resources,\n",
    "    )\n",
    "\n",
    "    # 2. Create a Semantic Kernel agent for the Azure AI agent\n",
    "    agent = AzureAIAgent(\n",
    "        client=client,\n",
    "        definition=agent_definition,\n",
    "    )\n",
    "\n",
    "    # 3. Create a thread for the agent\n",
    "    # If no thread is provided, a new thread will be\n",
    "    # created and returned with the initial response\n",
    "    thread: AzureAIAgentThread = None\n",
    "\n",
    "    try:\n",
    "        print(f\"# User: '{TASK}'\")\n",
    "        # 4. Invoke the agent for the specified thread for response\n",
    "        async for response in agent.invoke(messages=TASK, thread=thread):\n",
    "            if response.role != AuthorRole.TOOL:\n",
    "                print(f\"# Agent: {response}\")\n",
    "            thread = response.thread\n",
    "    finally:\n",
    "        # 6. Cleanup: Delete the thread and agent\n",
    "        await thread.delete() if thread else None\n",
    "        await client.agents.delete_agent(agent.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e22764b",
   "metadata": {},
   "source": [
    "## ðŸ§ª Case 3.2 open API with Azure AI Agent \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f81c4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'What is the name and population of the country that uses currency with abbreviation KRW'\n",
      "# Agent: I encountered an issue retrieving the information for the currency \"KRW.\" However, I can provide the details directly:\n",
      "\n",
      "The currency abbreviation \"KRW\" stands for the South Korean Won, which is used in **South Korea**. The population of South Korea is approximately **51.8 million** as of 2023.\n",
      "# User: 'What is the current weather in the capital city of the country?'\n",
      "# Agent: The current weather in **Seoul**, South Korea (capital city), is:\n",
      "\n",
      "- **Temperature**: 28Â°C (83Â°F)  \n",
      "- **Feels Like**: 29Â°C (85Â°F)  \n",
      "- **Condition**: Partly cloudy  \n",
      "- **Humidity**: 58%  \n",
      "- **Wind Speed**: 9 km/h (6 mph) from the WSW  \n",
      "- **Visibility**: 10 km  \n",
      "\n",
      "Do you need a detailed forecast or any specific weather details?\n"
     ]
    }
   ],
   "source": [
    "USER_INPUTS = [\n",
    "    \"What is the name and population of the country that uses currency with abbreviation KRW\",\n",
    "    \"What is the current weather in the capital city of the country?\",\n",
    "]\n",
    "\n",
    "ai_agent_settings = AzureAIAgentSettings()\n",
    "\n",
    "async with (\n",
    "    DefaultAzureCredential() as creds,\n",
    "    AzureAIAgent.create_client(credential=creds) as client,\n",
    "):\n",
    "    # 1. Read in the OpenAPI spec files\n",
    "    openapi_spec_file_path = \"resources\"\n",
    "    with open(os.path.join(openapi_spec_file_path, \"weather.json\")) as weather_file:\n",
    "        weather_openapi_spec = json.loads(weather_file.read())\n",
    "    with open(os.path.join(openapi_spec_file_path, \"countries.json\")) as countries_file:\n",
    "        countries_openapi_spec = json.loads(countries_file.read())\n",
    "\n",
    "    # 2. Create OpenAPI tools\n",
    "    # Note that connection or managed identity auth setup requires additional setup in Azure\n",
    "    auth = OpenApiAnonymousAuthDetails()\n",
    "    openapi_weather = OpenApiTool(\n",
    "        name=\"get_weather\",\n",
    "        spec=weather_openapi_spec,\n",
    "        description=\"Retrieve weather information for a location\",\n",
    "        auth=auth,\n",
    "    )\n",
    "    openapi_countries = OpenApiTool(\n",
    "        name=\"get_country\",\n",
    "        spec=countries_openapi_spec,\n",
    "        description=\"Retrieve country information\",\n",
    "        auth=auth,\n",
    "    )\n",
    "\n",
    "    # 3. Create an agent on the Azure AI agent service with the OpenAPI tools\n",
    "    agent_definition = await client.agents.create_agent(\n",
    "        model=ai_agent_settings.model_deployment_name,\n",
    "        tools=openapi_weather.definitions + openapi_countries.definitions,\n",
    "    )\n",
    "\n",
    "    # 4. Create a Semantic Kernel agent for the Azure AI agent\n",
    "    agent = AzureAIAgent(\n",
    "        client=client,\n",
    "        definition=agent_definition,\n",
    "    )\n",
    "\n",
    "    # 5. Create a thread for the agent\n",
    "    # If no thread is provided, a new thread will be\n",
    "    # created and returned with the initial response\n",
    "    thread: AzureAIAgentThread = None\n",
    "\n",
    "    try:\n",
    "        for user_input in USER_INPUTS:\n",
    "            print(f\"# User: '{user_input}'\")\n",
    "            # 7. Invoke the agent for the specified thread for response\n",
    "            async for response in agent.invoke(messages=user_input, thread=thread):\n",
    "                if response.role != AuthorRole.TOOL:\n",
    "                    print(f\"# Agent: {response}\")\n",
    "                thread = response.thread\n",
    "    finally:\n",
    "        # 8. Cleanup: Delete the thread and agent\n",
    "        await client.agents.delete_agent(agent.id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be56f4a",
   "metadata": {},
   "source": [
    "## ðŸ§ª Case 3.3 bing search API as plugin with ChatCompletionAgent (optional, deprecated) \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16c636c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: What is today's weather in South Korea?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Host: As of today, October 26, 2023, the weather in South Korea, particularly in Seoul, is generally clear and sunny. Current temperatures are around 19Â°C (66Â°F) with light winds. The conditions are favorable for outdoor activities, but it might get cooler in the evenings.\n",
      "\n",
      "For more details, you can check the full weather report here: [Weather Report for Seoul](https://www.weather.com/news/weather/news) (Please note that this URL is a placeholder, as specific URLs were not returned in the search). \n"
     ]
    }
   ],
   "source": [
    "# Simulate a conversation with the agent\n",
    "USER_INPUTS = [\n",
    "    \"What is today's weather in South Korea?\"\n",
    "]\n",
    "from semantic_kernel.connectors.search.bing import BingSearch\n",
    "from semantic_kernel.functions import KernelArguments, KernelParameterMetadata, KernelPlugin\n",
    "\n",
    "webplugin = KernelPlugin.from_text_search_with_search(\n",
    "        BingSearch(api_key=os.getenv(\"BING_API_KEY\")),\n",
    "        plugin_name=\"bing\",\n",
    "        description=\"Search the web for information.\",\n",
    "        parameters=[\n",
    "            KernelParameterMetadata(\n",
    "                name=\"query\",\n",
    "                description=\"The search query.\",\n",
    "                type=\"str\",\n",
    "                is_required=True,\n",
    "                type_object=str,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"top\",\n",
    "                description=\"The number of results to return.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=1,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"skip\",\n",
    "                description=\"The number of results to skip.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=0,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            # KernelParameterMetadata(\n",
    "            #     name=\"site\",\n",
    "            #     description=\"The site to search.\",\n",
    "            #     default_value=\"https://github.com/microsoft/semantic-kernel/tree/main/python\",\n",
    "            #     type=\"str\",\n",
    "            #     is_required=False,\n",
    "            #     type_object=str,\n",
    "            # ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "agent = ChatCompletionAgent(\n",
    "        service=AzureChatCompletion(),\n",
    "        name=\"Host\",\n",
    "        instructions=\"Answer questions from web search results. Add the web search reference url to the answer.\",\n",
    "        plugins=[webplugin],\n",
    "    )\n",
    "\n",
    "# 2. Create a thread to hold the conversation\n",
    "# If no thread is provided, a new thread will be\n",
    "# created and returned with the initial response\n",
    "thread: ChatHistoryAgentThread = None\n",
    "\n",
    "for user_input in USER_INPUTS:\n",
    "    print(f\"# User: {user_input}\")\n",
    "    # 4. Invoke the agent for a response\n",
    "    response = await agent.get_response(messages=user_input, thread=thread)\n",
    "    print(f\"# {response.name}: {response} \")\n",
    "    thread = response.thread\n",
    "\n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "# for user_input in USER_INPUTS:\n",
    "#         print(f\"# User: '{user_input}'\")\n",
    "#         first_chunk = True\n",
    "#         # 4. Invoke the agent for the current message and print the response\n",
    "#         async for response in agent.invoke_stream(messages=user_input, thread=thread):\n",
    "#             thread = response.thread\n",
    "#             if first_chunk:\n",
    "#                 print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "#                 first_chunk = False\n",
    "#             print(response.content, end=\"\", flush=True)\n",
    "#         print()\n",
    "\n",
    "await thread.delete() if thread else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb7922",
   "metadata": {},
   "source": [
    "## ðŸ§ª Case 3.4 file search tool with AzureAssistantAgent (Open AI Asisstant)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88bb7af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Who works in sales?'\n",
      "# Agent: Based on the provided file, the individuals working in sales are:\n",
      "\n",
      "1. Mariam Jaslyn - Sales Representativeã€4:0â€ employees.pdfã€‘.\n",
      "2. Hicran Bea - Sales Managerã€4:0â€ employees.pdfã€‘.\n",
      "3. Angelino Embla - Sales Representativeã€4:0â€ employees.pdfã€‘.\n",
      "# User: 'I have a customer request, who can help me?'\n",
      "# Agent: To assist with customer requests, you may consider reaching out to the sales team:\n",
      "\n",
      "1. **Mariam Jaslyn** - Sales Representativeã€4:0â€ employees.pdfã€‘.\n",
      "2. **Hicran Bea** - Sales Managerã€4:0â€ employees.pdfã€‘.\n",
      "3. **Angelino Embla** - Sales Representativeã€4:0â€ employees.pdfã€‘.\n",
      "\n",
      "Additionally, the **Marketing Specialist** or **Administrative Assistant** roles may help depending on the nature of the request.\n",
      "# User: 'Who is the youngest employee?'\n",
      "# Agent: The youngest employee is **Teodor Britton**, born on January 9, 1997ã€12:0â€ employees.pdfã€‘.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simulate a conversation with the agent\n",
    "USER_INPUTS = {\n",
    "    \"Who is the youngest employee?\",\n",
    "    \"Who works in sales?\",\n",
    "    \"I have a customer request, who can help me?\",\n",
    "}\n",
    "\n",
    "ai_agent_settings = AzureAIAgentSettings()\n",
    "\n",
    "# 1. Create the client using Azure OpenAI resources and configuration\n",
    "client = AzureAssistantAgent.create_client()\n",
    "\n",
    "# 2. Read and upload the file to the Azure OpenAI assistant service\n",
    "pdf_file_path = \"resources/employees.pdf\"\n",
    "\n",
    "\n",
    "with open(pdf_file_path, \"rb\") as file:\n",
    "    file = await client.files.create(file=file, purpose=\"assistants\")\n",
    "\n",
    "vector_store = await client.vector_stores.create(\n",
    "    name=\"step4_assistant_file_search\",\n",
    "    file_ids=[file.id],\n",
    ")\n",
    "\n",
    "# 3. Create file search tool with uploaded resources\n",
    "file_search_tool, file_search_tool_resources = AzureAssistantAgent.configure_file_search_tool(vector_store.id)\n",
    "\n",
    "# 4. Create the assistant on the Azure OpenAI service with the file search tool\n",
    "definition = await client.beta.assistants.create(\n",
    "    model=ai_agent_settings.model_deployment_name,\n",
    "    instructions=\"Find answers to the user's questions in the provided file.\",\n",
    "    name=\"FileSearch\",\n",
    "    tools=file_search_tool,\n",
    "    tool_resources=file_search_tool_resources,\n",
    ")\n",
    "\n",
    "# 5. Create a Semantic Kernel agent for the Azure OpenAI assistant\n",
    "agent = AzureAssistantAgent(\n",
    "    client=client,\n",
    "    definition=definition,\n",
    ")\n",
    "\n",
    "# 6. Create a new thread for use with the assistant\n",
    "# If no thread is provided, a new thread will be\n",
    "# created and returned with the initial response\n",
    "thread: AssistantAgentThread = None\n",
    "\n",
    "try:\n",
    "    for user_input in USER_INPUTS:\n",
    "        print(f\"# User: '{user_input}'\")\n",
    "        # 7. Invoke the agent for the current thread and print the response\n",
    "        async for response in agent.invoke(messages=user_input, thread=thread):\n",
    "            print(f\"# Agent: {response}\")\n",
    "            thread = response.thread\n",
    "finally:\n",
    "    # 9. Clean up the resources\n",
    "    await client.files.delete(file.id)\n",
    "    await client.vector_stores.delete(vector_store.id)\n",
    "    await client.beta.threads.delete(thread.id)\n",
    "    await client.beta.assistants.delete(agent.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d8159",
   "metadata": {},
   "source": [
    "## ðŸ§ª Case 3.5 BingGrounding with Azure AI Agent (SK)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d58fbb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'ê°€ìž¥ ìµœê·¼ì— ì„œìš¸ ë„ì‹¬ì— ìƒˆë¡œ ì˜¤í”ˆí•œ í•«í”Œë ˆì´ìŠ¤ëŠ”?'\n",
      "Function Call:> bing_grounding with arguments: {'requesturl': 'https://api.bing.microsoft.com/v7.0/search?q=ê°€ìž¥ ìµœê·¼ì— ì„œìš¸ ë„ì‹¬ì— ìƒˆë¡œ ì˜¤í”ˆí•œ í•«í”Œë ˆì´ìŠ¤ 2025ë…„ 6ì›”', 'response_metadata': \"{'market': 'ko-KR', 'num_docs_retrieved': 3, 'num_docs_actually_used': 3}\"}\n",
      "# SKBingGroundingAgent:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "2025ë…„ 6ì›” í˜„ìž¬ ì„œìš¸ ë„ì‹¬ì—ì„œ ì£¼ëª©ë°›ëŠ” ìƒˆë¡œìš´ í•«í”Œë ˆì´ìŠ¤ ëª‡ ê³³ì„ ì†Œê°œí•©ë‹ˆë‹¤:\n",
       "\n",
       "1. **ë…¸ëŸ‰ì§„ìˆ˜ì‚°ì‹œìž¥ í˜„ëŒ€í™”ë‹¨ì§€**: ê¸°ì¡´ ìˆ˜ì‚°ì‹œìž¥ì„ ì™„ì „ížˆ í˜„ëŒ€í™”í•˜ì—¬ ê¹”ë”í•˜ê³  ì¾Œì í•œ í™˜ê²½ì—ì„œ ì‹ ì„ í•œ í•´ì‚°ë¬¼ì„ ì§ì ‘ ê³ ë¥´ê³  ì‹ì‚¬í•  ìˆ˜ ìžˆëŠ” ìž¥ì†Œìž…ë‹ˆë‹¤ã€3:0â€ sourceã€‘.\n",
       "\n",
       "2. **í™ëŒ€ ë¯¸ë””ì–´ ì•„íŠ¸ ê°¤ëŸ¬ë¦¬**: ë¯¸ë””ì–´ ì•„íŠ¸ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ì „ì‹œì™€ ê³µì—° ê³µê°„ì´ ìƒˆë¡­ê²Œ ì˜¤í”ˆí•˜ì—¬ ì˜ˆìˆ ì  ì˜ê°ì„ ì£¼ëŠ” ìž¥ì†Œë¡œ ì£¼ëª©ë°›ê³  ìžˆìŠµë‹ˆë‹¤ã€3:0â€ sourceã€‘.\n",
       "\n",
       "3. **ì—¬ì˜ë„ í•œê°•ê³µì› ë£¨í”„íƒ‘ ì¹´íŽ˜**: í•œê°•ì˜ ë©‹ì§„ ì•¼ê²½ì„ ì¦ê¸°ë©° ìŒë£Œì™€ ë””ì €íŠ¸ë¥¼ ë§›ë³¼ ìˆ˜ ìžˆëŠ” ë£¨í”„íƒ‘ ê³µê°„ì´ ìƒˆë¡œìš´ ëª…ì†Œë¡œ ë– ì˜¬ëžìŠµë‹ˆë‹¤ã€3:0â€ sourceã€‘.\n",
       "\n",
       "ì´ ì™¸ì—ë„ ì„±ìˆ˜ë™ì˜ ë‹¤ì–‘í•œ ê°¤ëŸ¬ë¦¬í˜• ì¹´íŽ˜, ìž ì‹¤ ì„ì´Œí˜¸ìˆ˜ ê·¼ì²˜ì˜ ë·° ì¹´íŽ˜ ë“±ì´ ê³„ì†í•´ì„œ íŠ¸ë Œë“œì˜ ì¤‘ì‹¬ì— ìžˆìœ¼ë©°, í•œë‚¨ë™ì´ë‚˜ ìµì„ ë™ê³¼ ê°™ì€ ë¬¸í™”ì  ê°œì„±ì˜ ë™ë„¤ë„ ê¾¸ì¤€ížˆ ì¸ê¸°ë¥¼ ëŒê³  ìžˆìŠµë‹ˆë‹¤ã€3:1â€ sourceã€‘."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation :> https://peppercorntc.com/2025%EB%85%84-%EC%84%9C%EC%9A%B8%ED%95%AB%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-10%EA%B3%B3-%EB%86%93%EC%B9%98%EB%A9%B4-%ED%9B%84%ED%9A%8C/, source=ã€3:0â€ sourceã€‘, with start_index=133 and end_index=145\n",
      "Annotation :> https://peppercorntc.com/2025%EB%85%84-%EC%84%9C%EC%9A%B8%ED%95%AB%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-10%EA%B3%B3-%EB%86%93%EC%B9%98%EB%A9%B4-%ED%9B%84%ED%9A%8C/, source=ã€3:0â€ sourceã€‘, with start_index=229 and end_index=241\n",
      "Annotation :> https://peppercorntc.com/2025%EB%85%84-%EC%84%9C%EC%9A%B8%ED%95%AB%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-10%EA%B3%B3-%EB%86%93%EC%B9%98%EB%A9%B4-%ED%9B%84%ED%9A%8C/, source=ã€3:0â€ sourceã€‘, with start_index=322 and end_index=334\n",
      "Annotation :> https://presentlive.tistory.com/entry/%F0%9F%93%8C-2025%EB%85%84-%EC%84%9C%EC%9A%B8-%ED%95%AB%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-TOP-5-%E2%80%93-%EC%9A%94%EC%A6%98-%EA%B0%80%EC%9E%A5-%ED%95%AB%ED%95%9C-%EA%B3%B3, source=ã€3:1â€ sourceã€‘, with start_index=441 and end_index=453\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.agents.models import BingGroundingTool, MessageRole\n",
    "from azure.identity.aio import DefaultAzureCredential\n",
    "\n",
    "from semantic_kernel.agents import AzureAIAgent, AzureAIAgentSettings, AzureAIAgentThread\n",
    "from semantic_kernel.contents import (\n",
    "    AnnotationContent,\n",
    "    ChatMessageContent,\n",
    "    FunctionCallContent,\n",
    "    FunctionResultContent,\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "\n",
    "BING_GROUNDING_PROJECT_ENDPOINT = os.getenv(\"BING_GROUNDING_PROJECT_ENDPOINT\")\n",
    "# BING_GROUNDING_CONNECTION_ID = os.getenv(\"BING_GROUNDING_CONNECTION_ID\")\n",
    "BING_GROUNDING_CONNECTION_NAME = os.getenv(\"BING_GROUNDING_CONNECTION_NAME\")\n",
    "BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME = os.getenv(\"BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME\")\n",
    "BING_GROUNDING_MAX_RESULTS = int(os.getenv(\"BING_GROUNDING_MAX_RESULTS\", 3))\n",
    "BING_GROUNDING_MARKET = os.getenv(\"BING_GROUNDING_MARKET\", \"ko-KR\")\n",
    "BING_GROUNDING_SET_LANG = os.getenv(\"BING_GROUNDING_SET_LANG\", \"ko-KR\")\n",
    "    \n",
    "ai_agent_settings = AzureAIAgentSettings()\n",
    "\n",
    "async def handle_intermediate_steps(message: ChatMessageContent) -> None:\n",
    "    for item in message.items or []:\n",
    "        if isinstance(item, FunctionResultContent):\n",
    "            print(f\"Function Result:> {item.result} for function: {item.name}\")\n",
    "        elif isinstance(item, FunctionCallContent):\n",
    "            print(f\"Function Call:> {item.name} with arguments: {item.arguments}\")\n",
    "        else:\n",
    "            print(f\"{item}\")\n",
    "\n",
    "async with (\n",
    "        DefaultAzureCredential() as creds,\n",
    "        AzureAIAgent.create_client(credential=creds) as client,\n",
    "    ):\n",
    "        # 1. Enter your Bing Grounding Connection Name\n",
    "        bing_connection = await client.connections.get(name=BING_GROUNDING_CONNECTION_NAME)\n",
    "        conn_id = bing_connection.id\n",
    "\n",
    "        # 2. Initialize agent bing tool and add the connection id\n",
    "        bing = BingGroundingTool(connection_id=conn_id, market=BING_GROUNDING_MARKET, set_lang=BING_GROUNDING_SET_LANG, count=int(BING_GROUNDING_MAX_RESULTS))\n",
    "\n",
    "        # 3. Create an agent with Bing grounding on the Azure AI agent service\n",
    "        agent_definition = await client.agents.create_agent(\n",
    "            name=\"SKBingGroundingAgent\",\n",
    "            instructions=\"Use the Bing grounding tool to answer the user's question.\",\n",
    "            model=AzureAIAgentSettings().model_deployment_name,\n",
    "            tools=bing.definitions,\n",
    "        )\n",
    "\n",
    "        # 4. Create a Semantic Kernel agent for the Azure AI agent\n",
    "        agent = AzureAIAgent(\n",
    "            client=client,\n",
    "            definition=agent_definition,\n",
    "        )\n",
    "\n",
    "        # 5. Create a thread for the agent\n",
    "        # If no thread is provided, a new thread will be\n",
    "        # created and returned with the initial response\n",
    "        thread: AzureAIAgentThread | None = None\n",
    "        \n",
    "        SEARCH_GEN_USER_PROMPT_TEMPLATE = \"\"\"\n",
    "            please provide as rich and specific an answer and reference links as possible for `{query}`.\n",
    "            Today is {current_date}. Results should be based on the recent information available.\n",
    "        \"\"\"\n",
    "    \n",
    "        SEARCH_GEN_USER_PROMPT = PromptTemplate(\n",
    "            template=SEARCH_GEN_USER_PROMPT_TEMPLATE,\n",
    "            input_variables=[\"query\",\"current_date\"]\n",
    "        )\n",
    "        \n",
    "        current_date = datetime.now(tz=pytz.timezone(\"Asia/Seoul\")).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        try:\n",
    "            USER_INPUT = \"ê°€ìž¥ ìµœê·¼ì— ì„œìš¸ ë„ì‹¬ì— ìƒˆë¡œ ì˜¤í”ˆí•œ í•«í”Œë ˆì´ìŠ¤ëŠ”?\"\n",
    "            print(f\"# User: '{USER_INPUT}'\")\n",
    "            # 6. Invoke the agent for the specified thread for response\n",
    "            async for response in agent.invoke(\n",
    "                messages=SEARCH_GEN_USER_PROMPT.format(\n",
    "                    query=USER_INPUT,\n",
    "                    current_date=current_date\n",
    "                ), thread=thread, on_intermediate_message=handle_intermediate_steps\n",
    "            ):\n",
    "                print(f\"# {response.name}:\")\n",
    "                display(Markdown(response.message.content))\n",
    "                thread = response.thread\n",
    "\n",
    "                # 7. Show annotations\n",
    "                if any(isinstance(item, AnnotationContent) for item in response.items):\n",
    "                    for annotation in response.items:\n",
    "                        if isinstance(annotation, AnnotationContent):\n",
    "                            print(\n",
    "                                f\"Annotation :> {annotation.url}, source={annotation.quote}, with \"\n",
    "                                f\"start_index={annotation.start_index} and end_index={annotation.end_index}\"\n",
    "                            )\n",
    "        finally:\n",
    "            # 8. Cleanup: Delete the thread and agent\n",
    "            await thread.delete() if thread else None\n",
    "            await client.agents.delete_agent(agent.id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6697a76b",
   "metadata": {},
   "source": [
    "## ðŸ§ª Case 3.6 BingGrounding with Azure AI Agent (Vanila)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725c19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent, ID: asst_Ig5WOsYOCGGu5iHH8dCRRm8Y\n",
      "Created thread, ID: thread_EiknvevrOdXAv0qmDmXmxA0l\n",
      "# User: 'ê°€ìž¥ ìµœê·¼ì— ì„œìš¸ ë„ì‹¬ì— ìƒˆë¡œ ì˜¤í”ˆí•œ í•«í”Œë ˆì´ìŠ¤ëŠ”?'\n",
      "Created message, ID: msg_BBtZhrsSeBpunqhdhRDU49ZA\n",
      "Run finished with status: RunStatus.COMPLETED\n",
      "Role: MessageRole.AGENT\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ê°€ìž¥ ìµœê·¼ì— ì„œìš¸ ë„ì‹¬ì— ìƒˆë¡œ ì˜¤í”ˆí•œ í•«í”Œë ˆì´ìŠ¤ì™€ ê´€ë ¨í•´ 2025ë…„ í˜„ìž¬ ì„œìš¸ì—ì„œ ì£¼ëª©ë°›ëŠ” ì—¬ëŸ¬ í•«í”Œë ˆì´ìŠ¤ë¥¼ ì†Œê°œí•˜ê² ìŠµë‹ˆë‹¤.\n",
       "\n",
       "1. ì„±ìˆ˜ë™:  \n",
       "- 'ì˜¤ë¥´ì—ë¥´'ë¼ëŠ” ë² ì´ê¸€ ì „ë¬¸ ì¹´íŽ˜ê°€ ìµœê·¼ ì¸ê¸°ë¥¼ ëŒê³  ìžˆìŠµë‹ˆë‹¤. ì„±ìˆ˜ë™ì€ ê³µìž¥ì§€ëŒ€ì—ì„œ ê°ê°ì ì¸ ì¹´íŽ˜ì™€ ê°¤ëŸ¬ë¦¬ê°€ ë°€ì§‘í•œ íŠ¸ë Œë””í•œ ì§€ì—­ìœ¼ë¡œ ë³€ëª¨í–ˆìœ¼ë©°, 'ì˜¤ë¥´ì—ë¥´'ëŠ” ì•„ì¹¨ ì¼ì° ì˜¤í”ˆí•´ ì«€ë“í•œ ë² ì´ê¸€ê³¼ í¬ë¦¼ì¹˜ì¦ˆ ë“±ì´ ì¸ê¸° ë©”ë‰´ìž…ë‹ˆë‹¤.\n",
       "\n",
       "2. í•œë‚¨ë™:  \n",
       "- 'ì–´ë‹ˆì–¸'ì´ë¼ëŠ” ì¹´íŽ˜ ê²¸ ë¸ŒëŸ°ì¹˜ ë§›ì§‘ì´ ëª…ì„±ì„ ì–»ê³  ìžˆìŠµë‹ˆë‹¤. ë¹ˆí‹°ì§€í•œ ë¶„ìœ„ê¸°ì˜ ì˜¤ëž˜ëœ ì£¼íƒì„ ê°œì¡°í•´ ì•„ë³´ì¹´ë„ í† ìŠ¤íŠ¸ì™€ ë°”ì§ˆ íŽ˜ìŠ¤í†  íŒŒìŠ¤íƒ€ ë“±ì´ ì£¼ë ¥ ë©”ë‰´ìž…ë‹ˆë‹¤.\n",
       "\n",
       "3. ì—°ë‚¨ë™:  \n",
       "- ì¼ì‹ê³¼ í“¨ì „ì„ ì ‘ëª©í•œ 'ì—°ë‚¨í† ë§ˆ'ê°€ ìµœê·¼ ìž…ì†Œë¬¸ì„ íƒ€ê³  ìžˆìŠµë‹ˆë‹¤. ëª…ëž€ í¬ë¦¼ ìš°ë™ê³¼ ì—°ì–´ ë®ë°¥ ë“±ì´ ì¸ê¸°ì´ë©°, ì¼ë³¸ì‹ ì„ ìˆ ì§‘ ë¶„ìœ„ê¸°ê°€ íŠ¹ì§•ìž…ë‹ˆë‹¤.\n",
       "\n",
       "4. ì´íƒœì›:  \n",
       "- ë¹„ê±´ ë””ì €íŠ¸ ì „ë¬¸ ì¹´íŽ˜ 'í”ŒëžœíŠ¸'ê°€ ê±´ê°•ê³¼ í™˜ê²½ì„ ìƒê°í•˜ëŠ” íŠ¸ë Œë“œì— ë§žì¶° ê°ê´‘ë°›ê³  ìžˆìŠµë‹ˆë‹¤. ì½”ì½”ë„› ì´ˆì½œë¦¿ ì¼€ì´í¬ì™€ ë‹¹ê·¼ ì¼€ì´í¬ ë“±ì´ ëŒ€í‘œ ë©”ë‰´ìž…ë‹ˆë‹¤.\n",
       "\n",
       "5. ì—¬ì˜ë„:  \n",
       "- ë…ì¼ì‹ ë¹µê³¼ ë¸ŒëŸ°ì¹˜ ì¹´íŽ˜ 'ë”ë² ì´ì»¤ìŠ¤í…Œì´ë¸”'ì´ ìµœê·¼ ì£¼ëª©ë°›ê³  ìžˆìŠµë‹ˆë‹¤. í˜¸ë°€ë¹µ ìƒŒë“œìœ„ì¹˜ì™€ í”„ë ˆì²¼ ë“±ì´ ì¸ê¸° ë©”ë‰´ì´ë©°, í•œê°•ê³µì› ê·¼ì²˜ì—¬ì„œ ì‹ì‚¬ í›„ ì‚°ì±…í•˜ê¸° ì¢‹ìŠµë‹ˆë‹¤.\n",
       "\n",
       "ì„œìš¸ ë„ì‹¬ì˜ ìƒˆë¡œìš´ í•«í”Œë ˆì´ìŠ¤ë“¤ì€ ê° ì§€ì—­ íŠ¹ì„±ì„ ìž˜ ë°˜ì˜í•˜ì—¬ ì§€ì† ê°€ëŠ¥ì„±, ê°œì„±, ê±´ê°• íŠ¸ë Œë“œ ë“±ì„ ë°˜ì˜í•œ ê³µê°„ë“¤ì´ ë§ŽìŠµë‹ˆë‹¤. íŠ¹ížˆ ì„±ìˆ˜ë™ê³¼ í•œë‚¨ë™, ì—°ë‚¨ë™, ì´íƒœì›, ì—¬ì˜ë„ ì§€ì—­ì´ 2025ë…„ì—ë„ í™œë°œížˆ ìƒˆ ëª…ì†Œë“¤ì´ ìƒê²¨ë‚˜ê³  ìžˆìœ¼ë©°, ê°ê° ë…íŠ¹í•œ ë¶„ìœ„ê¸°ì™€ ë§›ì§‘ì„ ê°–ì¶”ê³  ìžˆìŠµë‹ˆë‹¤.\n",
       "\n",
       "ë” ìžì„¸í•œ ë‚´ìš©ê³¼ ìœ„ì¹˜, ë©”ë‰´, ìš´ì˜ì‹œê°„ ë“±ì€ ì•„ëž˜ ë§í¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.  \n",
       "- 2025ë…„ ì„œìš¸ í•«í”Œë ˆì´ìŠ¤ ì¶”ì²œ ë° ìƒì„¸ ê°€ì´ë“œ: https://ifif2y.tistory.com/entry/2025%EB%85%84-%EC%84%9C%EC%9A%B8-%ED%95%AB%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-%EC%B6%94%EC%B2%9C-%ED%8A%B8%EB%A0%8C%EB%94%94%ED%95%9C-%EB%AA%85%EC%86%8C%EC%99%80-%EC%88%A8%EC%9D%80-%EB%B3%B4%EC%84%9D  \n",
       "- ì„œìš¸ í•«í”Œë ˆì´ìŠ¤ TOP 5 ë° ê°€ë³¼ ë§Œí•œ ê³³: https://blog.naver.com/PostView.naver?blogId=trendreviewer&logNo=223719780932  \n",
       "\n",
       "ìš”ì•½í•˜ìžë©´, 2025ë…„ í˜„ìž¬ ì„±ìˆ˜ë™ì˜ 'ì˜¤ë¥´ì—ë¥´', í•œë‚¨ë™ 'ì–´ë‹ˆì–¸', ì—°ë‚¨ë™ 'ì—°ë‚¨í† ë§ˆ', ì´íƒœì›ì˜ 'í”ŒëžœíŠ¸', ì—¬ì˜ë„ì˜ 'ë”ë² ì´ì»¤ìŠ¤í…Œì´ë¸”' ë“±ì´ ì„œìš¸ ë„ì‹¬ì—ì„œ ìƒˆë¡œ ì˜¤í”ˆí•´ ì¸ê¸° ìžˆëŠ” í•«í”Œë ˆì´ìŠ¤ë¡œ ê¼½íž™ë‹ˆë‹¤."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.agents.models import BingGroundingTool, MessageRole\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Create an Azure AI Client from an endpoint, copied from your Azure AI Foundry project.\n",
    "# You need to login to Azure subscription via Azure CLI and set the environment variables\n",
    "BING_GROUNDING_PROJECT_ENDPOINT = os.getenv(\"BING_GROUNDING_PROJECT_ENDPOINT\")\n",
    "\n",
    "BING_GROUNDING_CONNECTION_ID = os.getenv(\"BING_GROUNDING_CONNECTION_ID\")\n",
    "BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME = os.getenv(\"BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME\")\n",
    "BING_GROUNDING_MAX_RESULTS = int(os.getenv(\"BING_GROUNDING_MAX_RESULTS\", 3))\n",
    "BING_GROUNDING_MARKET = os.getenv(\"BING_GROUNDING_MARKET\", \"ko-KR\")\n",
    "BING_GROUNDING_SET_LANG = os.getenv(\"BING_GROUNDING_SET_LANG\", \"ko-KR\")\n",
    "\n",
    "\n",
    "# Create an AIProjectClient instance\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=BING_GROUNDING_PROJECT_ENDPOINT,\n",
    "    credential=DefaultAzureCredential(),  # Use Azure Default Credential for authentication\n",
    ")\n",
    "\n",
    "# Initialize the Bing Grounding tool\n",
    "bing = BingGroundingTool(connection_id=BING_GROUNDING_CONNECTION_ID)\n",
    "\n",
    "with project_client:\n",
    "    # Create an agent with the Bing Grounding tool\n",
    "    agent = project_client.agents.create_agent(\n",
    "        model=BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME,  # Model deployment name\n",
    "        name=\"VanilaBingGroundingAgent\",  # Name of the agent\n",
    "        instructions=\"You are a helpful agent\",  # Instructions for the agent\n",
    "        tools=bing.definitions,  # Attach the Bing Grounding tool\n",
    "    )\n",
    "    print(f\"Created agent, ID: {agent.id}\")\n",
    "    \n",
    "    # Create a thread for communication\n",
    "    thread = project_client.agents.threads.create()\n",
    "    print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "\n",
    "    SEARCH_GEN_USER_PROMPT_TEMPLATE = \"\"\"\n",
    "        please provide as rich and specific an answer and reference links as possible for `{query}`.\n",
    "        Today is {current_date}. Results should be based on the recent information available.\n",
    "    \"\"\"\n",
    "\n",
    "    SEARCH_GEN_USER_PROMPT = PromptTemplate(\n",
    "        template=SEARCH_GEN_USER_PROMPT_TEMPLATE,\n",
    "        input_variables=[\"query\",\"current_date\"]\n",
    "    )\n",
    "\n",
    "    current_date = datetime.now(tz=pytz.timezone(\"Asia/Seoul\")).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    USER_INPUT = \"ê°€ìž¥ ìµœê·¼ì— ì„œìš¸ ë„ì‹¬ì— ìƒˆë¡œ ì˜¤í”ˆí•œ í•«í”Œë ˆì´ìŠ¤ëŠ”?\"\n",
    "    print(f\"# User: '{USER_INPUT}'\")\n",
    "\n",
    "    message = project_client.agents.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=MessageRole.USER,  # Role of the message sender\n",
    "        content=SEARCH_GEN_USER_PROMPT.format(\n",
    "            query=USER_INPUT,\n",
    "            current_date=current_date\n",
    "        )\n",
    "    )\n",
    "    print(f\"Created message, ID: {message['id']}\")\n",
    "\n",
    "    # Create and process an agent run\n",
    "    run = project_client.agents.runs.create_and_process(thread_id=thread.id, agent_id=agent.id)\n",
    "    print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "    # Check if the run failed\n",
    "    if run.status == \"failed\":\n",
    "        print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "    # Fetch and log all messages\n",
    "    message = project_client.agents.messages.get_last_message_by_role(thread_id=thread.id, role=MessageRole.AGENT)\n",
    "    \n",
    "    print(f\"Role: {message.role}\")\n",
    "    # Extract the text content from the message\n",
    "    if isinstance(message.content, list):\n",
    "        for content_item in message.content:\n",
    "            if content_item.get('type') == 'text':\n",
    "                display(Markdown(content_item['text']['value']))\n",
    "    else:\n",
    "        display(Markdown(message.content))\n",
    "\n",
    "    # Delete the agent when done\n",
    "    project_client.agents.delete_agent(agent.id)\n",
    "    print(\"Deleted agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a94755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d7ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_agentlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
