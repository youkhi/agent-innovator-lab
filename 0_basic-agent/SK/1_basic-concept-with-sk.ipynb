{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26ab996",
   "metadata": {},
   "source": [
    "# Understand the kernel with code samples\n",
    "\n",
    "----\n",
    "The kernel is the central component of Semantic Kernel. At its simplest, the kernel is a Dependency Injection container that manages all of the services and plugins necessary to run your AI application. If you provide all of your services and plugins to the kernel, they will then be seamlessly used by the AI as needed. \n",
    "\n",
    "| Components | Description |\n",
    "|------------|-------------|\n",
    "| **1. Services** | These consist of both AI services (e.g., chat completion) and other services (e.g., logging and HTTP clients) that are necessary to run your application. This was modeled after the Service Provider pattern in .NET to support dependency injection across all languages. |\n",
    "| **2. Plugins** | These are components used by your AI services and prompt templates to perform work. For example, AI services can use plugins to retrieve data from a database or call an external API to perform actions. |\n",
    "\n",
    "![kernel](images/the-kernel-is-at-the-center-of-everything.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e456fe",
   "metadata": {},
   "source": [
    "## Hierarchical Architecture Diagram\n",
    "---\n",
    "\n",
    "Architecture of a Retrieval-Augmented Generation (RAG) based chatbot using the latest Semantic Kernel. The diagram highlights how the Agent Framework (for chat interactions) and Process Framework (for workflow orchestration) sit above the core Kernel, leveraging it to perform AI tasks. The Semantic Kernel core orchestrates between user requests and the underlying AI services, memory stores, and plugins. It integrates with external LLM services for generation, uses a vector DB for semantic memory (retrieval of relevant data), and can invoke plugins which may call other external APIs or services as needed.\n",
    "\n",
    "![Hierarchical](images/semantic_kernel_component.png)\n",
    "\n",
    "***AI Agent (Agent Framework)***: The Agent Framework in Semantic Kernel is an optional layer that helps create conversational AI agents (like chatbots) using the core kernel’s capabilities​. It is not a replacement for the kernel but builds on top of it – your application still includes the Semantic Kernel library, and the agent uses the kernel’s functions internally. In a chatbot scenario, the Agent Framework manages the dialogue (turn-taking, system prompts, etc.) while delegating AI tasks to the kernel.\n",
    "\n",
    "***Semantic Kernel (Core)***: The Semantic Kernel core is the heart of the system. It orchestrates calls to AI models, retrieves memories, and executes plugin functions. The kernel provides abstractions to connect to AI services (LLM APIs for text generation, embedding models, etc.) and to various memory stores (for example, vector databases)​. It also manages context (prompts) and can use Planners to chain or select functions dynamically to fulfill a user request​. The kernel itself is part of your app’s runtime, coordinating all other components.\n",
    "\n",
    "***Plugins (Skills/Functions)***: Plugins (also called skills or functions) are units of functionality that the kernel can invoke. They might be defined with natural language prompts (semantic functions) or as native code functions. Plugins can perform calculations, transform data, or call external services/APIs. They are registered with and executed via the kernel, meaning they depend on the kernel to be invoked as part of an AI workflow​. However, the plugin implementations (e.g. an HTTP call to a web service, a database query) run outside the kernel – the kernel just orchestrates their usage. In essence, plugins extend the kernel’s abilities, and the kernel can automatically chain plugins to accomplish complex tasks for the user​.\n",
    "\n",
    "***AI Services***: These are external AI model endpoints that the kernel calls through its connectors. For example, the OpenAI or Azure OpenAI service provides the GPT-4 model for text generation, and there are embedding model services for vector generation. The kernel has integrations for many AI services (text completion, chat, image generation, speech recognition, etc.) and it uses them by calling out to the respective APIs​. These services are not “inside” the kernel – instead, the kernel depends on them to provide the intelligence. In the architecture, they appear as external components that the kernel invokes (e.g. sending a prompt to the GPT model and getting a completion).\n",
    "\n",
    "***Semantic Memory (Vector DB)***: The Semantic Kernel includes a memory abstraction that allows storing and retrieving contextual information. Under the hood, this is often backed by a vector database or search index. In a RAG-based chatbot, this is critical: documents or knowledge are embedded into vector representations and stored, so that relevant pieces can be retrieved to ground the AI’s answers. Semantic Kernel’s memory can integrate with many vector stores (e.g. Azure Cognitive Search, ChromaDB, Qdrant, Redis, Pinecone, etc.)​. This means “Memory” is essentially an AI Search over a vector DB, enabling the bot to find relevant information by semantic similarity. The memory component is used by the kernel but the database itself is external – the kernel just sends queries and gets results.\n",
    "\n",
    "***Process Framework (Workflow Orchestration)***: The Process Framework is another optional layer in the latest Semantic Kernel, aimed at long-running or multi-step workflows. It lets developers define structured processes composed of multiple steps, where each step can call kernel functions (AI or non-AI tasks) in an event-driven sequence​. Like the Agent Framework, the Process Framework builds on the kernel rather than enclosing it. It uses the kernel to execute AI functions at each step of a business workflow. This is especially useful if your chatbot or assistant needs to carry out complex transactions or procedures (for example, an order processing workflow or a support ticket resolution that involves several back-and-forth steps) beyond a single conversational turn. The Process Framework uses technologies like Orleans or Dapr under the hood for reliability and can reuse any existing kernel plugins within those processes​."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b243d6",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae1d038",
   "metadata": {},
   "source": [
    "Let's define our kernel for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "651e17cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import Annotated\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "from enum import Enum\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "from semantic_kernel.agents import AssistantAgentThread, AzureAssistantAgent\n",
    "from semantic_kernel.kernel_pydantic import KernelBaseSettings\n",
    "from semantic_kernel.connectors.search.bing import BingSearch\n",
    "from semantic_kernel.functions import KernelArguments, KernelParameterMetadata, KernelPlugin\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.filters import FilterTypes, FunctionInvocationContext\n",
    "from collections.abc import Awaitable, Callable\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.agents import AzureAIAgent, AzureAIAgentSettings, AzureAIAgentThread\n",
    "from semantic_kernel.agents import AzureResponsesAgent, ResponsesAgentThread\n",
    "from semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.agents import AzureResponsesAgent\n",
    "from azure.identity.aio import DefaultAzureCredential\n",
    "from semantic_kernel.agents.strategies.termination.termination_strategy import TerminationStrategy\n",
    "from semantic_kernel.agents import AgentGroupChat\n",
    "from semantic_kernel.contents.utils.author_role import AuthorRole\n",
    "from semantic_kernel.contents import AuthorRole\n",
    "from semantic_kernel.contents.streaming_chat_message_content import StreamingChatMessageContent\n",
    "from azure.ai.agents.models import OpenApiAnonymousAuthDetails, OpenApiTool, CodeInterpreterTool\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "\n",
    "\n",
    "class Service(Enum):\n",
    "    \"\"\"Attributes:\n",
    "    OpenAI (str): Represents the OpenAI service.\n",
    "    AzureOpenAI (str): Represents the Azure OpenAI service.\n",
    "    HuggingFace (str): Represents the HuggingFace service.\n",
    "    \"\"\"\n",
    "\n",
    "    OpenAI = \"openai\"\n",
    "    AzureOpenAI = \"azureopenai\"\n",
    "    HuggingFace = \"huggingface\"\n",
    "\n",
    "class ServiceSettings(KernelBaseSettings):\n",
    "    \"\"\"The Learn Resources Service Settings.\n",
    "\n",
    "    The settings are first loaded from environment variables. If the\n",
    "    environment variables are not found, the settings can be loaded from a .env file with the\n",
    "    encoding 'utf-8' as default or the specific encoding. If the settings are not found in the\n",
    "    .env file, the settings are ignored; however, validation will fail alerting that the settings\n",
    "    are missing.\n",
    "\n",
    "    Args:\n",
    "        global_llm_service (str | None): The LLM service to use for the samples, either \"OpenAI\" or \"AzureOpenAI\"\n",
    "            If not provided, defaults to \"AzureOpenAI\".\n",
    "    \"\"\"\n",
    "\n",
    "    global_llm_service: str | None = None\n",
    "    \n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1b01f0",
   "metadata": {},
   "source": [
    "We will load our settings and get the LLM service to use for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b0c8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The create method is deprecated. Use the __new__ method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using service type: Service.AzureOpenAI\n"
     ]
    }
   ],
   "source": [
    "service_settings = ServiceSettings.create()\n",
    "\n",
    "# Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
    "selectedService = (\n",
    "    Service.AzureOpenAI\n",
    "    if service_settings.global_llm_service is None\n",
    "    else Service(service_settings.global_llm_service.lower())\n",
    ")\n",
    "print(f\"Using service type: {selectedService}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793ed61f",
   "metadata": {},
   "source": [
    "We now configure our Chat Completion service on the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed56686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all services so that this cell can be re-run without restarting the kernel\n",
    "kernel.remove_all_services()\n",
    "\n",
    "service_id = None\n",
    "if selectedService == Service.OpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(\n",
    "            service_id=service_id,\n",
    "        ),\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(\n",
    "            service_id=service_id,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c79873",
   "metadata": {},
   "source": [
    "# Run a Semantic Function (No Agent, No Process)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd6ea0",
   "metadata": {},
   "source": [
    "## 🧪 Case 1.1 running a prompt without plugin\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a84f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft was founded by Bill Gates and Paul Allen in 1975,  \n",
      "Initially created a version of the BASIC programming language for Altair 8800,  \n",
      "In 1980, they signed a contract with IBM to provide an operating system,  \n",
      "This led to the creation of MS-DOS, which became a dominant OS for PCs,  \n",
      "In 1985, Microsoft launched Windows 1.0, a graphical extension for MS-DOS,  \n",
      "Windows 3.0 was released in 1990, gaining significant market share,  \n",
      "The 1995 launch of Windows 95 marked a major success, featuring a new user interface,  \n",
      "Microsoft Office suite was introduced, becoming essential for businesses,  \n",
      "The late 1990s saw antitrust lawsuits against Microsoft for monopolistic practices,  \n",
      "In 2001, Windows XP was released, praised for its stability and user-friendly design,  \n",
      "The rise of the internet led to the launch of Internet Explorer, competing with Netscape,  \n",
      "In 2007, Microsoft introduced Windows Vista, followed by Windows 7 in 2009,  \n",
      "The company expanded into gaming with the Xbox, launched in 2001,  \n",
      "Microsoft acquired LinkedIn in 2016, enhancing its enterprise services,  \n",
      "In 2020, they focused on cloud computing with Azure, becoming a leader in the field,  \n",
      "Microsoft remains a key player in software, hardware, and cloud solutions today."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the request settings\n",
    "req_settings = kernel.get_prompt_execution_settings_from_service_id(service_id)\n",
    "req_settings.max_tokens = 2000\n",
    "req_settings.temperature = 0.7\n",
    "req_settings.top_p = 0.8\n",
    "\n",
    "prompt=\"tell me about the history of {{$company_name}} as TLDR in exactly 200 words. make line seperated by commas. \\n\\nTLDR:\"\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"tldr\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    execution_settings=req_settings,\n",
    ")\n",
    "\n",
    "function = kernel.add_function(\n",
    "    function_name=\"tldr_function\",\n",
    "    plugin_name=\"tldr_plugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")\n",
    "\n",
    "\n",
    "########################################\n",
    "# invoke mode \n",
    "########################################\n",
    "\n",
    "# result = await kernel.invoke(function)\n",
    "# print(result) # => Robots must not harm humans.\n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "\n",
    "response = kernel.invoke_stream(plugin_name=\"tldr_plugin\", function_name=\"tldr_function\", company_name=\"Microsoft\")\n",
    "all_chunks_str = \"\"\n",
    "print(\"Assistant:> \", end=\"\")\n",
    "async for chunk in response:\n",
    "    if isinstance(chunk[0], StreamingChatMessageContent) and chunk[0].role == AuthorRole.ASSISTANT:\n",
    "        all_chunks_str += str(chunk[0])\n",
    "        print(str(chunk[0]), end=\"\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef0c277",
   "metadata": {},
   "source": [
    "## 🧪 Case 1.2 running a prompt with plugin\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b34af3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plugin = kernel.add_plugin(parent_directory=\"prompt_template_samples/\", plugin_name=\"FunPlugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d008b3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the time traveler bring a ladder to the dinosaur age?\n",
      "\n",
      "Because he heard the T-Rex was a little \"short\" on friends and wanted to help him reach new heights in socializing! 🦖😂\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "joke_function = plugin[\"Joke\"]\n",
    "\n",
    "joke = await kernel.invoke(\n",
    "    joke_function,\n",
    "    KernelArguments(input=\"time travel to dinosaur age\", style=\"super silly\"),\n",
    ")\n",
    "print(joke)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece006e",
   "metadata": {},
   "source": [
    "## 🧪 Case 1.3 running a prompt with Bing Search v7 plugin\n",
    "---\n",
    "- This case shows how to use the Bing Search v7 plugin to search the web and return results.\n",
    "- You must have a valid Bing Search v7 API key to run this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eee4e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the chat bot!    \n",
      "  Type 'exit' to exit.    \n",
      "  Try to search weather, news and more using bing search tool.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Bing search with arguments:\n",
      "  Query: \"가장 좋은 인공지능 모델 2023\"\n",
      "Bing search completed.\n",
      "SearchChatbot:> 2023년 가장 좋은 인공지능 모델로는 다음과 같은 모델들이 있습니다:\n",
      "\n",
      "1. **Falcon LLM**: 이 모델은 1,800억 개의 매개변수를 가지고 있으며, 3조 5,000억 개의 토큰으로 학습되었습니다. 허깅페이스의 오픈 LLM 리더보드에서 1위를 차지했으며, 상업용 및 연구용으로 모두 사용 가능합니다.\n",
      "\n",
      "2. **Gemini**: 구글과 딥마인드가 개발한 멀티모달 생성형 인공지능 모델로, 2023년 5월에 처음 공개되었습니다. 다양한 기능을 제공하며, 여러 가지 형태의 데이터를 처리할 수 있습니다.\n",
      "\n",
      "3. **Adobe Firefly**: Adobe의 Firefly는 일러스트레이션, 아트 컨셉, 사진 조작 등 다양한 시각 콘텐츠 생성을 가능하게 하며, Photoshop에 통합되어 사용자에게 AI의 힘을 제공합니다.\n",
      "\n",
      "이 외에도 여러 인공지능 모델들이 존재하지만, 위의 모델들이 특히 주목받고 있습니다.\n",
      "\n",
      "\n",
      "Exiting chat...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import semantic_kernel.connectors.search.bing\n",
    "\n",
    "# BING_API_KEY needs to be set in your environment variables\n",
    "if not os.getenv(\"BING_API_KEY\"):\n",
    "    raise ValueError(\"BING_API_KEY environment variable is not set. Please set it to your Bing Search API key.\")\n",
    "\n",
    "\n",
    "kernel = Kernel()\n",
    "kernel.add_service(AzureChatCompletion(service_id=\"chat\"))\n",
    "bing_search_plugin = kernel.add_plugin(KernelPlugin.from_text_search_with_search(\n",
    "        semantic_kernel.connectors.search.bing.BingSearch(),\n",
    "        plugin_name=\"bing\",\n",
    "        description=\"Search the web information using bing search.\",\n",
    "        parameters=[\n",
    "            KernelParameterMetadata(\n",
    "                name=\"query\",\n",
    "                description=\"The search query.\",\n",
    "                type=\"str\",\n",
    "                is_required=True,\n",
    "                type_object=str,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"top\",\n",
    "                description=\"The number of results to return.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=2,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"skip\",\n",
    "                description=\"The number of results to skip.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=0,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"site\",\n",
    "                description=\"The site to search.\",\n",
    "                default_value=\"\",\n",
    "                type=\"str\",\n",
    "                is_required=False,\n",
    "                type_object=str,\n",
    "            ),\n",
    "        ],\n",
    "    ))\n",
    "\n",
    "chat_function = kernel.add_function(\n",
    "    prompt=\"{{$chat_history}}{{$user_input}}\",\n",
    "    plugin_name=\"ChatBot\",\n",
    "    function_name=\"Chat\",\n",
    ")\n",
    "execution_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=\"chat\",\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    function_choice_behavior=FunctionChoiceBehavior.Auto(auto_invoke=True),\n",
    ")\n",
    "\n",
    "history = ChatHistory()\n",
    "system_message = \"\"\"\n",
    "You are a chat bot, use bing plugin to find answers.\n",
    "\"\"\"\n",
    "history.add_system_message(system_message)\n",
    "history.add_user_message(\"Hi there, who are you?\")\n",
    "history.add_assistant_message(\"I am Information Finder, a chat bot. use bing plugin to find answers.\")\n",
    "\n",
    "arguments = KernelArguments(settings=execution_settings)\n",
    "\n",
    "@kernel.filter(filter_type=FilterTypes.FUNCTION_INVOCATION)\n",
    "async def log_bing_filter(\n",
    "    context: FunctionInvocationContext, next: Callable[[FunctionInvocationContext], Awaitable[None]]\n",
    "):\n",
    "    if context.function.plugin_name == \"bing\":\n",
    "        print(\"Calling Bing search with arguments:\")\n",
    "        if \"query\" in context.arguments:\n",
    "            print(f'  Query: \"{context.arguments[\"query\"]}\"')\n",
    "        if \"count\" in context.arguments:\n",
    "            print(f'  Count: \"{context.arguments[\"count\"]}\"')\n",
    "        if \"skip\" in context.arguments:\n",
    "            print(f'  Skip: \"{context.arguments[\"skip\"]}\"')\n",
    "        await next(context)\n",
    "        print(\"Bing search completed.\")\n",
    "    else:\n",
    "        await next(context)\n",
    "\n",
    "\n",
    "async def chat() -> bool:\n",
    "    try:\n",
    "        user_input = input(\"User:> \")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    except EOFError:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    if user_input == \"exit\":\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    arguments[\"user_input\"] = user_input\n",
    "    arguments[\"chat_history\"] = history\n",
    "    result = await kernel.invoke(chat_function, arguments=arguments)\n",
    "    print(f\"SearchChatbot:> {result}\")\n",
    "    history.add_user_message(user_input)\n",
    "    history.add_assistant_message(str(result))\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "chatting = True\n",
    "print(\n",
    "    \"Welcome to the chat bot!\\\n",
    "    \\n  Type 'exit' to exit.\\\n",
    "    \\n  Try to search weather, news and more using bing search tool.\"\n",
    ")\n",
    "while chatting:\n",
    "    chatting = await chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbc0df44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Bing search with arguments:\n",
      "  Query: \"what is new opening hotels in NYC 2025?\"\n",
      "  Skip: \"0\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bing search completed.\n",
      "We try to keep our pages about new hotels in NYC as up-to-date as possible and will be updating this page when any new hotels open or when any upcoming hotels in NYC open for reservation.,New York is poised to welcome new hotels in New York City and beyond in 2025. Add these five to your must-stay list.,Now Now NoHo – A Unique Sleeper Cabin Hotel in Manhattan Now Now NoHo is a distinctive sleeper cabin hotel set to open on April 1, 2025, at 338 Bowery in Manhattan’s NoHo neighborhood.,Opened Hotels in New York We have created a list with the best newly opened hotels in New York, USA. Hope you enjoy it. We think it might be the best of its kind around. Tip: Set your dates to see room prices!,A list of my most anticipated New Hotel Openings or Renovations for 2025. Most, but not all, are Resorts, many are Luxury or Boutique properties. Hopefully they will be bringing something new and unique to the world of hospitality.,The top luxury hotel openings in 2025, bringing world-class event venues, corporate meeting spaces, and private party spaces to New York City.,Keep scrolling to see which properties caught our eye, including the first-ever Graduate Hotel in Texas and the reopening of a swanky South Florida resort. A historic, transformed New York City property is expected to open its doors in spring 2025.,Luxury travel in 2025 just got even more exciting! Uncover the hottest new hotel openings—from dreamy Costa Rican beaches to lavish European escapes. Don’t miss these five-star destinations redefining indulgence—start planning now.,Here are five new and exciting luxury hotels in the Big Apple, some already in full swing, and others gearing up to welcome guests, from storied Grand Dames to the playfully high-end and seductive.,Don’t miss 25 more 2025 new hotels beyond those detailed below that deserve to be on your radar. The iconic Waldorf Astoria New York is set to welcome guests to its Midtown East address for the first time since 2017 following a meticulous multi-million dollar restoration.\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "bing_search_fuction = kernel.get_function(\"bing\", \"search\")\n",
    "\n",
    "search_result = await kernel.invoke(\n",
    "    bing_search_fuction,\n",
    "    KernelArguments(\n",
    "        query=\"what is new opening hotels in NYC 2025?\",\n",
    "        top=10,\n",
    "        skip=0,\n",
    "    ),\n",
    ")\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8505e28c",
   "metadata": {},
   "source": [
    "# Use single agent to chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# The following sample demonstrates how to create a simple,       #\n",
    "# Azure AI agent, ChatCompletionAgent, AzureResponsesAgent        #\n",
    "# that answers questions about a sample menu                      #\n",
    "# using a Semantic Kernel Plugin.                                 #\n",
    "###################################################################\n",
    "\n",
    "\n",
    "# Define a sample plugin for the sample\n",
    "class MenuPlugin:\n",
    "    \"\"\"A sample Menu Plugin used for the concept sample.\"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Provides a list of specials from the menu.\")\n",
    "    def get_specials(self) -> Annotated[str, \"Returns the specials from the menu.\"]:\n",
    "        return \"\"\"\n",
    "        Special Soup: Clam Chowder\n",
    "        Special Salad: Cobb Salad\n",
    "        Special Drink: Chai Tea\n",
    "        \"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Provides the price of the requested menu item.\")\n",
    "    def get_item_price(\n",
    "        self, menu_item: Annotated[str, \"The name of the menu item.\"]\n",
    "    ) -> Annotated[str, \"Returns the price of the menu item.\"]:\n",
    "        return \"$9.99\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39602eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_INPUTS = [\n",
    "    \"Hello\",\n",
    "    \"What is the special soup?\",\n",
    "    \"How much does that cost?\",\n",
    "    \"Thank you\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f952185",
   "metadata": {},
   "source": [
    "## 🧪 Case 2.1 chat with Azure AI Agent \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65844e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'What is the name and population of the country that uses currency with abbreviation KRW'\n",
      "# Host: The country that uses the currency with the abbreviation **KRW** (Korean Won) is **South Korea**. As of 2023, the population of South Korea is approximately **52 million** people.\n",
      "# User: 'What is the current weather in the capital city of the country?'\n",
      "# Host: I cannot provide real-time weather updates. To find the current weather in **Seoul**, the capital city of South Korea, you can check a weather service like [Weather.com](https://www.weather.com) or use a weather app for the most accurate and up-to-date information.\n"
     ]
    }
   ],
   "source": [
    "# Azure AI Agent Endpoint and model deployment name need to be set in your environment variables\n",
    "if not os.getenv(\"AZURE_AI_AGENT_ENDPOINT\"):\n",
    "    raise ValueError(\"AZURE_AI_AGENT_ENDPOINT environment variable is not set. Please set it to your Azure AI Agent endpoint.\")\n",
    "if not os.getenv(\"AZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME\"):\n",
    "    raise ValueError(\"AZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME environment variable is not set. Please set it to your Azure AI Agent model deployment name.\")\n",
    "\n",
    "# Make sure to set the environment variables for the Azure AI Agent\n",
    "ai_agent_settings = AzureAIAgentSettings()\n",
    "\n",
    "\n",
    "async with (\n",
    "    DefaultAzureCredential() as creds,\n",
    "    AzureAIAgent.create_client(credential=creds) as client,\n",
    "):\n",
    "    # 1. Create an agent on the Azure AI agent service\n",
    "    agent_definition = await client.agents.create_agent(\n",
    "        model=ai_agent_settings.model_deployment_name,\n",
    "        name=\"Host\",\n",
    "        instructions=\"Answer questions about the menu.\",\n",
    "    )\n",
    "\n",
    "    # 2. Create a Semantic Kernel agent for the Azure AI agent\n",
    "    agent = AzureAIAgent(\n",
    "        client=client,\n",
    "        definition=agent_definition,\n",
    "        plugins=[MenuPlugin()],  # Add the plugin to the agent\n",
    "    )\n",
    "\n",
    "    # 3. Create a thread for the agent\n",
    "    # If no thread is provided, a new thread will be\n",
    "    # created and returned with the initial response\n",
    "    thread: AzureAIAgentThread = None\n",
    "\n",
    "########################################\n",
    "# invoke mode \n",
    "########################################\n",
    "\n",
    "    # for user_input in USER_INPUTS:\n",
    "    #     print(f\"# User: {user_input}\")\n",
    "    #     # 4. Invoke the agent for the specified thread for response\n",
    "    #     async for response in agent.invoke(\n",
    "    #         messages=user_input,\n",
    "    #         thread=thread,\n",
    "    #     ):\n",
    "    #         print(f\"# {response.name}: {response}\")\n",
    "    #         thread = response.thread\n",
    "    \n",
    "    # await thread.delete() if thread else None    \n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "# Simulate a conversation with the agent\n",
    "\n",
    "    try:\n",
    "        for user_input in USER_INPUTS:\n",
    "            print(f\"# User: '{user_input}'\")\n",
    "            first_chunk = True\n",
    "            # 4. Invoke the agent for the current message and print the response\n",
    "            async for response in agent.invoke_stream(messages=user_input, thread=thread):\n",
    "                thread = response.thread\n",
    "                if first_chunk:\n",
    "                    print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "                    first_chunk = False\n",
    "                print(response.content, end=\"\", flush=True)\n",
    "            print()\n",
    "    finally:\n",
    "            # Cleanup: Delete the thread and agent\n",
    "            await thread.delete() if thread else None\n",
    "            await client.agents.delete_agent(agent.id)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024625db",
   "metadata": {},
   "source": [
    "## 🧪 Case 2.2 chat with ChatCompletionAgent\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68221ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Hello'\n",
      "# Host: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "# User: 'What is the special soup?'\n",
      "# Host: The special soup is Clam Chowder. Would you like to know more about it or anything else?\n",
      "# User: 'How much does that cost?'\n",
      "# Host: The Clam Chowder costs $9.99. Would you like to know anything else?\n",
      "# User: 'Thank you'\n",
      "# Host: You're welcome! If you have any more questions or need assistance, feel free to ask. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "#  need to be set in your environment variables - from semantic_kernel.connectors.ai.open_ai.settings.azure_open_ai_settings import AzureOpenAISettings\n",
    "if not os.getenv(\"AZURE_OPENAI_ENDPOINT\"):\n",
    "    raise ValueError(\"AZURE_OPENAI_ENDPOINT environment variable is not set. Please set it to your Azure OpenAI endpoint.\")\n",
    "if not os.getenv(\"AZURE_OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"AZURE_OPENAI_API_KEY environment variable is not set. Please set it to your Azure OpenAI API key.\")\n",
    "if not os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"):\n",
    "    raise ValueError(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME environment variable is not set. Please set it to your Azure OpenAI chat deployment name.\")\n",
    "\n",
    "\n",
    "\n",
    "agent = ChatCompletionAgent(\n",
    "        service=AzureChatCompletion(),\n",
    "        name=\"Host\",\n",
    "        instructions=\"Answer questions based on web search\",\n",
    "        plugins=[MenuPlugin()],\n",
    "    )\n",
    "\n",
    "# 2. Create a thread to hold the conversation\n",
    "# If no thread is provided, a new thread will be\n",
    "# created and returned with the initial response\n",
    "thread: ChatHistoryAgentThread = None\n",
    "\n",
    "########################################\n",
    "# invoke mode \n",
    "########################################\n",
    "\n",
    "# for user_input in USER_INPUTS:\n",
    "#     print(f\"# User: {user_input}\")\n",
    "#     # 4. Invoke the agent for a response\n",
    "#     response = await agent.get_response(messages=user_input, thread=thread)\n",
    "#     print(f\"# {response.name}: {response} \")\n",
    "#     thread = response.thread\n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "\n",
    "for user_input in USER_INPUTS:\n",
    "        print(f\"# User: '{user_input}'\")\n",
    "        first_chunk = True\n",
    "        # 4. Invoke the agent for the current message and print the response\n",
    "        async for response in agent.invoke_stream(messages=user_input, thread=thread):\n",
    "            thread = response.thread\n",
    "            if first_chunk:\n",
    "                print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "                first_chunk = False\n",
    "            print(response.content, end=\"\", flush=True)\n",
    "        print()\n",
    "\n",
    "\n",
    "await thread.delete() if thread else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732bbee4",
   "metadata": {},
   "source": [
    "## 🧪 Case 2.3 chat with ResponseAgent (OpenAI Responses API)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f82cb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Hello'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Host: Hi there! How can I assist you today?\n",
      "# User: 'What is the special soup?'\n",
      "# Host: The special soup is Clam Chowder. Is there anything else you'd like to know?\n",
      "# User: 'How much does that cost?'\n",
      "# Host: The Clam Chowder costs $9.99. Would you like to know anything else?\n",
      "# User: 'Thank you'\n",
      "# Host: You're welcome! If you have any more questions, feel free to ask. Enjoy your day!\n"
     ]
    }
   ],
   "source": [
    "#  need to be set in your environment variables - from semantic_kernel.connectors.ai.open_ai.settings.azure_open_ai_settings import AzureOpenAISettings\n",
    "if not os.getenv(\"AZURE_OPENAI_ENDPOINT\"):\n",
    "    raise ValueError(\"AZURE_OPENAI_ENDPOINT environment variable is not set. Please set it to your Azure OpenAI endpoint.\")\n",
    "if not os.getenv(\"AZURE_OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"AZURE_OPENAI_API_KEY environment variable is not set. Please set it to your Azure OpenAI API key.\")\n",
    "if not os.getenv(\"AZURE_OPENAI_RESPONSES_DEPLOYMENT_NAME\"):\n",
    "    raise ValueError(\"AZURE_OPENAI_RESPONSES_DEPLOYMENT_NAME environment variable is not set. Please set it to your Azure OpenAI responses deployment name.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "client = AzureResponsesAgent.create_client()\n",
    "\n",
    "# 2. Create a Semantic Kernel agent for the OpenAI Responses API\n",
    "agent = AzureResponsesAgent(\n",
    "    ai_model_id=\"gpt-4o-mini\",\n",
    "    client=client,\n",
    "    instructions=\"Answer questions about the menu.\",\n",
    "    name=\"Host\",\n",
    "    plugins=[MenuPlugin()],\n",
    ")\n",
    "\n",
    "# 3. Create a thread for the agent\n",
    "# If no thread is provided, a new thread will be\n",
    "# created and returned with the initial response\n",
    "thread: ResponsesAgentThread = None\n",
    "\n",
    "########################################\n",
    "# invoke mode \n",
    "########################################\n",
    "# for user_input in USER_INPUTS:\n",
    "#     print(f\"# User: '{user_input}'\")\n",
    "#     # 4. Invoke the agent for the current message and print the response\n",
    "#     response = await agent.get_response(messages=user_input, thread=thread)\n",
    "#     print(f\"# {response.name}: {response.content}\")\n",
    "#     thread = response.thread\n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "for user_input in USER_INPUTS:\n",
    "        print(f\"# User: '{user_input}'\")\n",
    "        first_chunk = True\n",
    "        # 4. Invoke the agent for the current message and print the response\n",
    "        async for response in agent.invoke_stream(messages=user_input, thread=thread):\n",
    "            thread = response.thread\n",
    "            if first_chunk:\n",
    "                print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "                first_chunk = False\n",
    "            print(response.content, end=\"\", flush=True)\n",
    "        print()\n",
    "\n",
    "await thread.delete() if thread else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c01be",
   "metadata": {},
   "source": [
    "# Use tools to answer specific questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f8d674",
   "metadata": {},
   "source": [
    "## 🧪 Case 3.1 code interpreter with Azure AI Agent \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c502bca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The create method is deprecated. Use the __new__ method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Use code to determine the values in the Fibonacci sequence that that are less then the value of 101.'\n",
      "# Agent: # Create a Fibonacci sequence with numbers less than 101\n",
      "fibonacci_sequence = []\n",
      "a, b = 0, 1  # Starting values of Fibonacci numbers\n",
      "\n",
      "while a < 101:\n",
      "    fibonacci_sequence.append(a)\n",
      "    a, b = b, a + b  # Move to the next Fibonacci number\n",
      "\n",
      "fibonacci_sequence\n",
      "# Agent: The values in the Fibonacci sequence that are less than 101 are:\n",
      "\n",
      "\\[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89\\]\n"
     ]
    }
   ],
   "source": [
    "TASK = \"Use code to determine the values in the Fibonacci sequence that that are less then the value of 101.\"\n",
    "\n",
    "ai_agent_settings = AzureAIAgentSettings()\n",
    "\n",
    "async with (\n",
    "    DefaultAzureCredential() as creds,\n",
    "    AzureAIAgent.create_client(credential=creds) as client,\n",
    "):\n",
    "    # 1. Create an agent with a code interpreter on the Azure AI agent service\n",
    "    code_interpreter = CodeInterpreterTool()\n",
    "    agent_definition = await client.agents.create_agent(\n",
    "        model=ai_agent_settings.model_deployment_name,\n",
    "        tools=code_interpreter.definitions,\n",
    "        tool_resources=code_interpreter.resources,\n",
    "    )\n",
    "\n",
    "    # 2. Create a Semantic Kernel agent for the Azure AI agent\n",
    "    agent = AzureAIAgent(\n",
    "        client=client,\n",
    "        definition=agent_definition,\n",
    "    )\n",
    "\n",
    "    # 3. Create a thread for the agent\n",
    "    # If no thread is provided, a new thread will be\n",
    "    # created and returned with the initial response\n",
    "    thread: AzureAIAgentThread = None\n",
    "\n",
    "    try:\n",
    "        print(f\"# User: '{TASK}'\")\n",
    "        # 4. Invoke the agent for the specified thread for response\n",
    "        async for response in agent.invoke(messages=TASK, thread=thread):\n",
    "            if response.role != AuthorRole.TOOL:\n",
    "                print(f\"# Agent: {response}\")\n",
    "            thread = response.thread\n",
    "    finally:\n",
    "        # 6. Cleanup: Delete the thread and agent\n",
    "        await thread.delete() if thread else None\n",
    "        await client.agents.delete_agent(agent.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e22764b",
   "metadata": {},
   "source": [
    "## 🧪 Case 3.2 open API with Azure AI Agent \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f81c4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'What is the name and population of the country that uses currency with abbreviation KRW'\n",
      "# Agent: I encountered an issue retrieving the information for the currency \"KRW.\" However, I can provide the details directly:\n",
      "\n",
      "The currency abbreviation \"KRW\" stands for the South Korean Won, which is used in **South Korea**. The population of South Korea is approximately **51.8 million** as of 2023.\n",
      "# User: 'What is the current weather in the capital city of the country?'\n",
      "# Agent: The current weather in **Seoul**, South Korea (capital city), is:\n",
      "\n",
      "- **Temperature**: 28°C (83°F)  \n",
      "- **Feels Like**: 29°C (85°F)  \n",
      "- **Condition**: Partly cloudy  \n",
      "- **Humidity**: 58%  \n",
      "- **Wind Speed**: 9 km/h (6 mph) from the WSW  \n",
      "- **Visibility**: 10 km  \n",
      "\n",
      "Do you need a detailed forecast or any specific weather details?\n"
     ]
    }
   ],
   "source": [
    "USER_INPUTS = [\n",
    "    \"What is the name and population of the country that uses currency with abbreviation KRW\",\n",
    "    \"What is the current weather in the capital city of the country?\",\n",
    "]\n",
    "\n",
    "ai_agent_settings = AzureAIAgentSettings()\n",
    "\n",
    "async with (\n",
    "    DefaultAzureCredential() as creds,\n",
    "    AzureAIAgent.create_client(credential=creds) as client,\n",
    "):\n",
    "    # 1. Read in the OpenAPI spec files\n",
    "    openapi_spec_file_path = \"resources\"\n",
    "    with open(os.path.join(openapi_spec_file_path, \"weather.json\")) as weather_file:\n",
    "        weather_openapi_spec = json.loads(weather_file.read())\n",
    "    with open(os.path.join(openapi_spec_file_path, \"countries.json\")) as countries_file:\n",
    "        countries_openapi_spec = json.loads(countries_file.read())\n",
    "\n",
    "    # 2. Create OpenAPI tools\n",
    "    # Note that connection or managed identity auth setup requires additional setup in Azure\n",
    "    auth = OpenApiAnonymousAuthDetails()\n",
    "    openapi_weather = OpenApiTool(\n",
    "        name=\"get_weather\",\n",
    "        spec=weather_openapi_spec,\n",
    "        description=\"Retrieve weather information for a location\",\n",
    "        auth=auth,\n",
    "    )\n",
    "    openapi_countries = OpenApiTool(\n",
    "        name=\"get_country\",\n",
    "        spec=countries_openapi_spec,\n",
    "        description=\"Retrieve country information\",\n",
    "        auth=auth,\n",
    "    )\n",
    "\n",
    "    # 3. Create an agent on the Azure AI agent service with the OpenAPI tools\n",
    "    agent_definition = await client.agents.create_agent(\n",
    "        model=ai_agent_settings.model_deployment_name,\n",
    "        tools=openapi_weather.definitions + openapi_countries.definitions,\n",
    "    )\n",
    "\n",
    "    # 4. Create a Semantic Kernel agent for the Azure AI agent\n",
    "    agent = AzureAIAgent(\n",
    "        client=client,\n",
    "        definition=agent_definition,\n",
    "    )\n",
    "\n",
    "    # 5. Create a thread for the agent\n",
    "    # If no thread is provided, a new thread will be\n",
    "    # created and returned with the initial response\n",
    "    thread: AzureAIAgentThread = None\n",
    "\n",
    "    try:\n",
    "        for user_input in USER_INPUTS:\n",
    "            print(f\"# User: '{user_input}'\")\n",
    "            # 7. Invoke the agent for the specified thread for response\n",
    "            async for response in agent.invoke(messages=user_input, thread=thread):\n",
    "                if response.role != AuthorRole.TOOL:\n",
    "                    print(f\"# Agent: {response}\")\n",
    "                thread = response.thread\n",
    "    finally:\n",
    "        # 8. Cleanup: Delete the thread and agent\n",
    "        await client.agents.delete_agent(agent.id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be56f4a",
   "metadata": {},
   "source": [
    "## 🧪 Case 3.3 bing search API as plugin with ChatCompletionAgent (optional, deprecated) \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16c636c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: What is today's weather in South Korea?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Host: As of today, October 26, 2023, the weather in South Korea, particularly in Seoul, is generally clear and sunny. Current temperatures are around 19°C (66°F) with light winds. The conditions are favorable for outdoor activities, but it might get cooler in the evenings.\n",
      "\n",
      "For more details, you can check the full weather report here: [Weather Report for Seoul](https://www.weather.com/news/weather/news) (Please note that this URL is a placeholder, as specific URLs were not returned in the search). \n"
     ]
    }
   ],
   "source": [
    "# Simulate a conversation with the agent\n",
    "USER_INPUTS = [\n",
    "    \"What is today's weather in South Korea?\"\n",
    "]\n",
    "from semantic_kernel.connectors.search.bing import BingSearch\n",
    "from semantic_kernel.functions import KernelArguments, KernelParameterMetadata, KernelPlugin\n",
    "\n",
    "webplugin = KernelPlugin.from_text_search_with_search(\n",
    "        BingSearch(api_key=os.getenv(\"BING_API_KEY\")),\n",
    "        plugin_name=\"bing\",\n",
    "        description=\"Search the web for information.\",\n",
    "        parameters=[\n",
    "            KernelParameterMetadata(\n",
    "                name=\"query\",\n",
    "                description=\"The search query.\",\n",
    "                type=\"str\",\n",
    "                is_required=True,\n",
    "                type_object=str,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"top\",\n",
    "                description=\"The number of results to return.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=1,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"skip\",\n",
    "                description=\"The number of results to skip.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=0,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            # KernelParameterMetadata(\n",
    "            #     name=\"site\",\n",
    "            #     description=\"The site to search.\",\n",
    "            #     default_value=\"https://github.com/microsoft/semantic-kernel/tree/main/python\",\n",
    "            #     type=\"str\",\n",
    "            #     is_required=False,\n",
    "            #     type_object=str,\n",
    "            # ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "agent = ChatCompletionAgent(\n",
    "        service=AzureChatCompletion(),\n",
    "        name=\"Host\",\n",
    "        instructions=\"Answer questions from web search results. Add the web search reference url to the answer.\",\n",
    "        plugins=[webplugin],\n",
    "    )\n",
    "\n",
    "# 2. Create a thread to hold the conversation\n",
    "# If no thread is provided, a new thread will be\n",
    "# created and returned with the initial response\n",
    "thread: ChatHistoryAgentThread = None\n",
    "\n",
    "for user_input in USER_INPUTS:\n",
    "    print(f\"# User: {user_input}\")\n",
    "    # 4. Invoke the agent for a response\n",
    "    response = await agent.get_response(messages=user_input, thread=thread)\n",
    "    print(f\"# {response.name}: {response} \")\n",
    "    thread = response.thread\n",
    "\n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "# for user_input in USER_INPUTS:\n",
    "#         print(f\"# User: '{user_input}'\")\n",
    "#         first_chunk = True\n",
    "#         # 4. Invoke the agent for the current message and print the response\n",
    "#         async for response in agent.invoke_stream(messages=user_input, thread=thread):\n",
    "#             thread = response.thread\n",
    "#             if first_chunk:\n",
    "#                 print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "#                 first_chunk = False\n",
    "#             print(response.content, end=\"\", flush=True)\n",
    "#         print()\n",
    "\n",
    "await thread.delete() if thread else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb7922",
   "metadata": {},
   "source": [
    "## 🧪 Case 3.4 file search tool with AzureAssistantAgent (Open AI Asisstant)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88bb7af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Who works in sales?'\n",
      "# Agent: Based on the provided file, the individuals working in sales are:\n",
      "\n",
      "1. Mariam Jaslyn - Sales Representative【4:0†employees.pdf】.\n",
      "2. Hicran Bea - Sales Manager【4:0†employees.pdf】.\n",
      "3. Angelino Embla - Sales Representative【4:0†employees.pdf】.\n",
      "# User: 'I have a customer request, who can help me?'\n",
      "# Agent: To assist with customer requests, you may consider reaching out to the sales team:\n",
      "\n",
      "1. **Mariam Jaslyn** - Sales Representative【4:0†employees.pdf】.\n",
      "2. **Hicran Bea** - Sales Manager【4:0†employees.pdf】.\n",
      "3. **Angelino Embla** - Sales Representative【4:0†employees.pdf】.\n",
      "\n",
      "Additionally, the **Marketing Specialist** or **Administrative Assistant** roles may help depending on the nature of the request.\n",
      "# User: 'Who is the youngest employee?'\n",
      "# Agent: The youngest employee is **Teodor Britton**, born on January 9, 1997【12:0†employees.pdf】.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simulate a conversation with the agent\n",
    "USER_INPUTS = {\n",
    "    \"Who is the youngest employee?\",\n",
    "    \"Who works in sales?\",\n",
    "    \"I have a customer request, who can help me?\",\n",
    "}\n",
    "\n",
    "ai_agent_settings = AzureAIAgentSettings()\n",
    "\n",
    "# 1. Create the client using Azure OpenAI resources and configuration\n",
    "client = AzureAssistantAgent.create_client()\n",
    "\n",
    "# 2. Read and upload the file to the Azure OpenAI assistant service\n",
    "pdf_file_path = \"resources/employees.pdf\"\n",
    "\n",
    "\n",
    "with open(pdf_file_path, \"rb\") as file:\n",
    "    file = await client.files.create(file=file, purpose=\"assistants\")\n",
    "\n",
    "vector_store = await client.vector_stores.create(\n",
    "    name=\"step4_assistant_file_search\",\n",
    "    file_ids=[file.id],\n",
    ")\n",
    "\n",
    "# 3. Create file search tool with uploaded resources\n",
    "file_search_tool, file_search_tool_resources = AzureAssistantAgent.configure_file_search_tool(vector_store.id)\n",
    "\n",
    "# 4. Create the assistant on the Azure OpenAI service with the file search tool\n",
    "definition = await client.beta.assistants.create(\n",
    "    model=ai_agent_settings.model_deployment_name,\n",
    "    instructions=\"Find answers to the user's questions in the provided file.\",\n",
    "    name=\"FileSearch\",\n",
    "    tools=file_search_tool,\n",
    "    tool_resources=file_search_tool_resources,\n",
    ")\n",
    "\n",
    "# 5. Create a Semantic Kernel agent for the Azure OpenAI assistant\n",
    "agent = AzureAssistantAgent(\n",
    "    client=client,\n",
    "    definition=definition,\n",
    ")\n",
    "\n",
    "# 6. Create a new thread for use with the assistant\n",
    "# If no thread is provided, a new thread will be\n",
    "# created and returned with the initial response\n",
    "thread: AssistantAgentThread = None\n",
    "\n",
    "try:\n",
    "    for user_input in USER_INPUTS:\n",
    "        print(f\"# User: '{user_input}'\")\n",
    "        # 7. Invoke the agent for the current thread and print the response\n",
    "        async for response in agent.invoke(messages=user_input, thread=thread):\n",
    "            print(f\"# Agent: {response}\")\n",
    "            thread = response.thread\n",
    "finally:\n",
    "    # 9. Clean up the resources\n",
    "    await client.files.delete(file.id)\n",
    "    await client.vector_stores.delete(vector_store.id)\n",
    "    await client.beta.threads.delete(thread.id)\n",
    "    await client.beta.assistants.delete(agent.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d8159",
   "metadata": {},
   "source": [
    "## 🧪 Case 3.5 BingGrounding with Azure AI Agent (SK)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d58fbb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: '가장 최근에 서울 도심에 새로 오픈한 핫플레이스는?'\n",
      "Function Call:> bing_grounding with arguments: {'requesturl': 'https://api.bing.microsoft.com/v7.0/search?q=가장 최근에 서울 도심에 새로 오픈한 핫플레이스 2025년 6월', 'response_metadata': \"{'market': 'ko-KR', 'num_docs_retrieved': 3, 'num_docs_actually_used': 3}\"}\n",
      "# SKBingGroundingAgent:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "2025년 6월 현재 서울 도심에서 주목받는 새로운 핫플레이스 몇 곳을 소개합니다:\n",
       "\n",
       "1. **노량진수산시장 현대화단지**: 기존 수산시장을 완전히 현대화하여 깔끔하고 쾌적한 환경에서 신선한 해산물을 직접 고르고 식사할 수 있는 장소입니다【3:0†source】.\n",
       "\n",
       "2. **홍대 미디어 아트 갤러리**: 미디어 아트를 중심으로 한 전시와 공연 공간이 새롭게 오픈하여 예술적 영감을 주는 장소로 주목받고 있습니다【3:0†source】.\n",
       "\n",
       "3. **여의도 한강공원 루프탑 카페**: 한강의 멋진 야경을 즐기며 음료와 디저트를 맛볼 수 있는 루프탑 공간이 새로운 명소로 떠올랐습니다【3:0†source】.\n",
       "\n",
       "이 외에도 성수동의 다양한 갤러리형 카페, 잠실 석촌호수 근처의 뷰 카페 등이 계속해서 트렌드의 중심에 있으며, 한남동이나 익선동과 같은 문화적 개성의 동네도 꾸준히 인기를 끌고 있습니다【3:1†source】."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation :> https://peppercorntc.com/2025%EB%85%84-%EC%84%9C%EC%9A%B8%ED%95%AB%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-10%EA%B3%B3-%EB%86%93%EC%B9%98%EB%A9%B4-%ED%9B%84%ED%9A%8C/, source=【3:0†source】, with start_index=133 and end_index=145\n",
      "Annotation :> https://peppercorntc.com/2025%EB%85%84-%EC%84%9C%EC%9A%B8%ED%95%AB%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-10%EA%B3%B3-%EB%86%93%EC%B9%98%EB%A9%B4-%ED%9B%84%ED%9A%8C/, source=【3:0†source】, with start_index=229 and end_index=241\n",
      "Annotation :> https://peppercorntc.com/2025%EB%85%84-%EC%84%9C%EC%9A%B8%ED%95%AB%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-10%EA%B3%B3-%EB%86%93%EC%B9%98%EB%A9%B4-%ED%9B%84%ED%9A%8C/, source=【3:0†source】, with start_index=322 and end_index=334\n",
      "Annotation :> https://presentlive.tistory.com/entry/%F0%9F%93%8C-2025%EB%85%84-%EC%84%9C%EC%9A%B8-%ED%95%AB%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-TOP-5-%E2%80%93-%EC%9A%94%EC%A6%98-%EA%B0%80%EC%9E%A5-%ED%95%AB%ED%95%9C-%EA%B3%B3, source=【3:1†source】, with start_index=441 and end_index=453\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.agents.models import BingGroundingTool, MessageRole\n",
    "from azure.identity.aio import DefaultAzureCredential\n",
    "\n",
    "from semantic_kernel.agents import AzureAIAgent, AzureAIAgentSettings, AzureAIAgentThread\n",
    "from semantic_kernel.contents import (\n",
    "    AnnotationContent,\n",
    "    ChatMessageContent,\n",
    "    FunctionCallContent,\n",
    "    FunctionResultContent,\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "\n",
    "BING_GROUNDING_PROJECT_ENDPOINT = os.getenv(\"BING_GROUNDING_PROJECT_ENDPOINT\")\n",
    "# BING_GROUNDING_CONNECTION_ID = os.getenv(\"BING_GROUNDING_CONNECTION_ID\")\n",
    "BING_GROUNDING_CONNECTION_NAME = os.getenv(\"BING_GROUNDING_CONNECTION_NAME\")\n",
    "BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME = os.getenv(\"BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME\")\n",
    "BING_GROUNDING_MAX_RESULTS = int(os.getenv(\"BING_GROUNDING_MAX_RESULTS\", 3))\n",
    "BING_GROUNDING_MARKET = os.getenv(\"BING_GROUNDING_MARKET\", \"ko-KR\")\n",
    "BING_GROUNDING_SET_LANG = os.getenv(\"BING_GROUNDING_SET_LANG\", \"ko-KR\")\n",
    "    \n",
    "ai_agent_settings = AzureAIAgentSettings()\n",
    "\n",
    "async def handle_intermediate_steps(message: ChatMessageContent) -> None:\n",
    "    for item in message.items or []:\n",
    "        if isinstance(item, FunctionResultContent):\n",
    "            print(f\"Function Result:> {item.result} for function: {item.name}\")\n",
    "        elif isinstance(item, FunctionCallContent):\n",
    "            print(f\"Function Call:> {item.name} with arguments: {item.arguments}\")\n",
    "        else:\n",
    "            print(f\"{item}\")\n",
    "\n",
    "async with (\n",
    "        DefaultAzureCredential() as creds,\n",
    "        AzureAIAgent.create_client(credential=creds) as client,\n",
    "    ):\n",
    "        # 1. Enter your Bing Grounding Connection Name\n",
    "        bing_connection = await client.connections.get(name=BING_GROUNDING_CONNECTION_NAME)\n",
    "        conn_id = bing_connection.id\n",
    "\n",
    "        # 2. Initialize agent bing tool and add the connection id\n",
    "        bing = BingGroundingTool(connection_id=conn_id, market=BING_GROUNDING_MARKET, set_lang=BING_GROUNDING_SET_LANG, count=int(BING_GROUNDING_MAX_RESULTS))\n",
    "\n",
    "        # 3. Create an agent with Bing grounding on the Azure AI agent service\n",
    "        agent_definition = await client.agents.create_agent(\n",
    "            name=\"SKBingGroundingAgent\",\n",
    "            instructions=\"Use the Bing grounding tool to answer the user's question.\",\n",
    "            model=AzureAIAgentSettings().model_deployment_name,\n",
    "            tools=bing.definitions,\n",
    "        )\n",
    "\n",
    "        # 4. Create a Semantic Kernel agent for the Azure AI agent\n",
    "        agent = AzureAIAgent(\n",
    "            client=client,\n",
    "            definition=agent_definition,\n",
    "        )\n",
    "\n",
    "        # 5. Create a thread for the agent\n",
    "        # If no thread is provided, a new thread will be\n",
    "        # created and returned with the initial response\n",
    "        thread: AzureAIAgentThread | None = None\n",
    "        \n",
    "        SEARCH_GEN_USER_PROMPT_TEMPLATE = \"\"\"\n",
    "            please provide as rich and specific an answer and reference links as possible for `{query}`.\n",
    "            Today is {current_date}. Results should be based on the recent information available.\n",
    "        \"\"\"\n",
    "    \n",
    "        SEARCH_GEN_USER_PROMPT = PromptTemplate(\n",
    "            template=SEARCH_GEN_USER_PROMPT_TEMPLATE,\n",
    "            input_variables=[\"query\",\"current_date\"]\n",
    "        )\n",
    "        \n",
    "        current_date = datetime.now(tz=pytz.timezone(\"Asia/Seoul\")).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        try:\n",
    "            USER_INPUT = \"가장 최근에 서울 도심에 새로 오픈한 핫플레이스는?\"\n",
    "            print(f\"# User: '{USER_INPUT}'\")\n",
    "            # 6. Invoke the agent for the specified thread for response\n",
    "            async for response in agent.invoke(\n",
    "                messages=SEARCH_GEN_USER_PROMPT.format(\n",
    "                    query=USER_INPUT,\n",
    "                    current_date=current_date\n",
    "                ), thread=thread, on_intermediate_message=handle_intermediate_steps\n",
    "            ):\n",
    "                print(f\"# {response.name}:\")\n",
    "                display(Markdown(response.message.content))\n",
    "                thread = response.thread\n",
    "\n",
    "                # 7. Show annotations\n",
    "                if any(isinstance(item, AnnotationContent) for item in response.items):\n",
    "                    for annotation in response.items:\n",
    "                        if isinstance(annotation, AnnotationContent):\n",
    "                            print(\n",
    "                                f\"Annotation :> {annotation.url}, source={annotation.quote}, with \"\n",
    "                                f\"start_index={annotation.start_index} and end_index={annotation.end_index}\"\n",
    "                            )\n",
    "        finally:\n",
    "            # 8. Cleanup: Delete the thread and agent\n",
    "            await thread.delete() if thread else None\n",
    "            await client.agents.delete_agent(agent.id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6697a76b",
   "metadata": {},
   "source": [
    "## 🧪 Case 3.6 BingGrounding with Azure AI Agent (Vanila)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725c19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent, ID: asst_Ig5WOsYOCGGu5iHH8dCRRm8Y\n",
      "Created thread, ID: thread_EiknvevrOdXAv0qmDmXmxA0l\n",
      "# User: '가장 최근에 서울 도심에 새로 오픈한 핫플레이스는?'\n",
      "Created message, ID: msg_BBtZhrsSeBpunqhdhRDU49ZA\n",
      "Run finished with status: RunStatus.COMPLETED\n",
      "Role: MessageRole.AGENT\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "가장 최근에 서울 도심에 새로 오픈한 핫플레이스와 관련해 2025년 현재 서울에서 주목받는 여러 핫플레이스를 소개하겠습니다.\n",
       "\n",
       "1. 성수동:  \n",
       "- '오르에르'라는 베이글 전문 카페가 최근 인기를 끌고 있습니다. 성수동은 공장지대에서 감각적인 카페와 갤러리가 밀집한 트렌디한 지역으로 변모했으며, '오르에르'는 아침 일찍 오픈해 쫀득한 베이글과 크림치즈 등이 인기 메뉴입니다.\n",
       "\n",
       "2. 한남동:  \n",
       "- '어니언'이라는 카페 겸 브런치 맛집이 명성을 얻고 있습니다. 빈티지한 분위기의 오래된 주택을 개조해 아보카도 토스트와 바질 페스토 파스타 등이 주력 메뉴입니다.\n",
       "\n",
       "3. 연남동:  \n",
       "- 일식과 퓨전을 접목한 '연남토마'가 최근 입소문을 타고 있습니다. 명란 크림 우동과 연어 덮밥 등이 인기이며, 일본식 선술집 분위기가 특징입니다.\n",
       "\n",
       "4. 이태원:  \n",
       "- 비건 디저트 전문 카페 '플랜트'가 건강과 환경을 생각하는 트렌드에 맞춰 각광받고 있습니다. 코코넛 초콜릿 케이크와 당근 케이크 등이 대표 메뉴입니다.\n",
       "\n",
       "5. 여의도:  \n",
       "- 독일식 빵과 브런치 카페 '더베이커스테이블'이 최근 주목받고 있습니다. 호밀빵 샌드위치와 프레첼 등이 인기 메뉴이며, 한강공원 근처여서 식사 후 산책하기 좋습니다.\n",
       "\n",
       "서울 도심의 새로운 핫플레이스들은 각 지역 특성을 잘 반영하여 지속 가능성, 개성, 건강 트렌드 등을 반영한 공간들이 많습니다. 특히 성수동과 한남동, 연남동, 이태원, 여의도 지역이 2025년에도 활발히 새 명소들이 생겨나고 있으며, 각각 독특한 분위기와 맛집을 갖추고 있습니다.\n",
       "\n",
       "더 자세한 내용과 위치, 메뉴, 운영시간 등은 아래 링크에서 확인할 수 있습니다.  \n",
       "- 2025년 서울 핫플레이스 추천 및 상세 가이드: https://ifif2y.tistory.com/entry/2025%EB%85%84-%EC%84%9C%EC%9A%B8-%ED%95%AB%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-%EC%B6%94%EC%B2%9C-%ED%8A%B8%EB%A0%8C%EB%94%94%ED%95%9C-%EB%AA%85%EC%86%8C%EC%99%80-%EC%88%A8%EC%9D%80-%EB%B3%B4%EC%84%9D  \n",
       "- 서울 핫플레이스 TOP 5 및 가볼 만한 곳: https://blog.naver.com/PostView.naver?blogId=trendreviewer&logNo=223719780932  \n",
       "\n",
       "요약하자면, 2025년 현재 성수동의 '오르에르', 한남동 '어니언', 연남동 '연남토마', 이태원의 '플랜트', 여의도의 '더베이커스테이블' 등이 서울 도심에서 새로 오픈해 인기 있는 핫플레이스로 꼽힙니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.agents.models import BingGroundingTool, MessageRole\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Create an Azure AI Client from an endpoint, copied from your Azure AI Foundry project.\n",
    "# You need to login to Azure subscription via Azure CLI and set the environment variables\n",
    "BING_GROUNDING_PROJECT_ENDPOINT = os.getenv(\"BING_GROUNDING_PROJECT_ENDPOINT\")\n",
    "\n",
    "BING_GROUNDING_CONNECTION_ID = os.getenv(\"BING_GROUNDING_CONNECTION_ID\")\n",
    "BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME = os.getenv(\"BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME\")\n",
    "BING_GROUNDING_MAX_RESULTS = int(os.getenv(\"BING_GROUNDING_MAX_RESULTS\", 3))\n",
    "BING_GROUNDING_MARKET = os.getenv(\"BING_GROUNDING_MARKET\", \"ko-KR\")\n",
    "BING_GROUNDING_SET_LANG = os.getenv(\"BING_GROUNDING_SET_LANG\", \"ko-KR\")\n",
    "\n",
    "\n",
    "# Create an AIProjectClient instance\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=BING_GROUNDING_PROJECT_ENDPOINT,\n",
    "    credential=DefaultAzureCredential(),  # Use Azure Default Credential for authentication\n",
    ")\n",
    "\n",
    "# Initialize the Bing Grounding tool\n",
    "bing = BingGroundingTool(connection_id=BING_GROUNDING_CONNECTION_ID)\n",
    "\n",
    "with project_client:\n",
    "    # Create an agent with the Bing Grounding tool\n",
    "    agent = project_client.agents.create_agent(\n",
    "        model=BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME,  # Model deployment name\n",
    "        name=\"VanilaBingGroundingAgent\",  # Name of the agent\n",
    "        instructions=\"You are a helpful agent\",  # Instructions for the agent\n",
    "        tools=bing.definitions,  # Attach the Bing Grounding tool\n",
    "    )\n",
    "    print(f\"Created agent, ID: {agent.id}\")\n",
    "    \n",
    "    # Create a thread for communication\n",
    "    thread = project_client.agents.threads.create()\n",
    "    print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "\n",
    "    SEARCH_GEN_USER_PROMPT_TEMPLATE = \"\"\"\n",
    "        please provide as rich and specific an answer and reference links as possible for `{query}`.\n",
    "        Today is {current_date}. Results should be based on the recent information available.\n",
    "    \"\"\"\n",
    "\n",
    "    SEARCH_GEN_USER_PROMPT = PromptTemplate(\n",
    "        template=SEARCH_GEN_USER_PROMPT_TEMPLATE,\n",
    "        input_variables=[\"query\",\"current_date\"]\n",
    "    )\n",
    "\n",
    "    current_date = datetime.now(tz=pytz.timezone(\"Asia/Seoul\")).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    USER_INPUT = \"가장 최근에 서울 도심에 새로 오픈한 핫플레이스는?\"\n",
    "    print(f\"# User: '{USER_INPUT}'\")\n",
    "\n",
    "    message = project_client.agents.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=MessageRole.USER,  # Role of the message sender\n",
    "        content=SEARCH_GEN_USER_PROMPT.format(\n",
    "            query=USER_INPUT,\n",
    "            current_date=current_date\n",
    "        )\n",
    "    )\n",
    "    print(f\"Created message, ID: {message['id']}\")\n",
    "\n",
    "    # Create and process an agent run\n",
    "    run = project_client.agents.runs.create_and_process(thread_id=thread.id, agent_id=agent.id)\n",
    "    print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "    # Check if the run failed\n",
    "    if run.status == \"failed\":\n",
    "        print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "    # Fetch and log all messages\n",
    "    message = project_client.agents.messages.get_last_message_by_role(thread_id=thread.id, role=MessageRole.AGENT)\n",
    "    \n",
    "    print(f\"Role: {message.role}\")\n",
    "    # Extract the text content from the message\n",
    "    if isinstance(message.content, list):\n",
    "        for content_item in message.content:\n",
    "            if content_item.get('type') == 'text':\n",
    "                display(Markdown(content_item['text']['value']))\n",
    "    else:\n",
    "        display(Markdown(message.content))\n",
    "\n",
    "    # Delete the agent when done\n",
    "    project_client.agents.delete_agent(agent.id)\n",
    "    print(\"Deleted agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a94755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d7ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_agentlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
