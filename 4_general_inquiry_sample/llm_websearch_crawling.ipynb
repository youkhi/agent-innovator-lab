{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "138d2cb2",
   "metadata": {},
   "source": [
    "# LLM with Web Search and Crawl\n",
    "\n",
    "Code to crawl the top n pages of a Google search result and serve them to LLM in order to utilize rich context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5623f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True) \n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "CHAT_COMPLETIONS_MODEL = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13ea70",
   "metadata": {},
   "source": [
    "bs4 or scrapy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "897bb2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import scrapy\n",
    "from bs4 import BeautifulSoup\n",
    "import httpx\n",
    "import asyncio\n",
    "from urllib.parse import urljoin\n",
    "from azure.ai.projects.models import MessageRole, BingGroundingTool\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\")\n",
    "BING_GROUNDING_PROJECT_CONNECTION_STRING = os.getenv(\"BING_GROUNDING_PROJECT_CONNECTION_STRING\")\n",
    "BING_GROUNDING_AGENT_ID = os.getenv(\"BING_GROUNDING_AGENT_ID\")\n",
    "BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME = os.getenv(\"BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME\")\n",
    "BING_GROUNDING_CONNECTION_NAME = os.getenv(\"BING_GROUNDING_CONNECTION_NAME\")\n",
    "# Web search mode: \"google\" or \"bing\"\n",
    "# it can be changed when users want to use different search engine\n",
    "WEB_SEARCH_MODE = os.getenv(\"WEB_SEARCH_MODE\")\n",
    "\n",
    "def extract_text_and_tables_by_bs4(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # Extract main text\n",
    "    paragraphs = [p.get_text().strip() for p in soup.find_all(\"p\") if p.get_text().strip()]\n",
    "    text = \"\\n\".join(paragraphs)\n",
    "    return text\n",
    "\n",
    "\n",
    "async def extract_text_and_tables_async(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=3, follow_redirects=True) as client:\n",
    "        try:\n",
    "            response = await client.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            # Handle 302 redirect manually if follow_redirects fails\n",
    "            if e.response.status_code == 302 and \"location\" in e.response.headers:\n",
    "                redirect_url = e.response.headers[\"location\"]\n",
    "                if not redirect_url.startswith(\"http\"):\n",
    "                    # handle relative redirects\n",
    "                    redirect_url = urljoin(url, redirect_url)\n",
    "                try:\n",
    "                    response = await client.get(redirect_url, headers=headers)\n",
    "                    response.raise_for_status()\n",
    "                except Exception as e2:\n",
    "                    print(f\"Redirect request failed: {e2}\")\n",
    "                    return \"\"\n",
    "            else:\n",
    "                print(f\"Request failed: {e}\")\n",
    "                return \"\"\n",
    "        except httpx.HTTPError as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "        selector = scrapy.Selector(text=response.text)\n",
    "        paragraphs = [p.strip() for p in selector.css('p::text').getall() if p.strip()]\n",
    "        text = \"\\n\".join(paragraphs)\n",
    "        return text\n",
    "\n",
    "async def add_context_async(top_urls = []):\n",
    "    async def gather_contexts():\n",
    "        tasks = [extract_text_and_tables_async(url) for url in top_urls]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return results\n",
    "    return await gather_contexts()\n",
    "\n",
    "def google_search(query, num=5, search_type=\"web\"):\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"key\": GOOGLE_API_KEY,\n",
    "        \"cx\": GOOGLE_CSE_ID,\n",
    "        \"num\": num, \n",
    "        \"locale\": \"ko\",  # í•œêµ­ì–´ë¡œ ê²€ìƒ‰\n",
    "        \"siteSearch\": \"samsung.com\",\n",
    "        \"siteSearchFilter\": \"e\"\n",
    "    }\n",
    "    \n",
    "    if search_type == \"image\":\n",
    "        params[\"searchType\"] = \"image\"\n",
    "        \n",
    "    response = requests.get(url, params=params)\n",
    "    results = response.json()\n",
    "    return results.get(\"items\", [])\n",
    "\n",
    "def bing_grounding_search(query, num=5, search_type=\"web\"):\n",
    "    try:\n",
    "        creds = DefaultAzureCredential()\n",
    "        \n",
    "        project_client = AIProjectClient.from_connection_string(\n",
    "            credential=creds,\n",
    "            conn_str=BING_GROUNDING_PROJECT_CONNECTION_STRING,\n",
    "        )\n",
    "        \n",
    "        agent_id = BING_GROUNDING_AGENT_ID\n",
    "        \n",
    "        if not agent_id:\n",
    "            print(\"BING_GROUNDING_AGENT_ID is not set. Create new agent...\")\n",
    "            connection_name = BING_GROUNDING_CONNECTION_NAME\n",
    "            \n",
    "            bing_connection = project_client.connections.get(\n",
    "                connection_name=connection_name,\n",
    "            )\n",
    "            conn_id = bing_connection.id\n",
    "            \n",
    "            bing = BingGroundingTool(connection_id=conn_id)\n",
    "            \n",
    "            \n",
    "            agent = project_client.agents.create_agent(\n",
    "                model=BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME,\n",
    "                name=\"temporary-bing-agent\",\n",
    "                instructions=\"\"\"\n",
    "                    Search for product information and images exclusively about Samsung products. Get all contents from the website as much as you can. Don't include the url link in the response.\n",
    "                    Only respond with information from trusted sources: samsung.com.\n",
    "                    Prioritize data from samsung.com whenever available to ensure accuracy and reliability.\n",
    "                    If information is not found on samsung.com, supplement with tistory.com, but always indicate the source.\n",
    "                    Avoid using data from any other websites or unverified sources.\n",
    "                \"\"\",\n",
    "                tools=bing.definitions,\n",
    "                headers={\"x-ms-enable-preview\": \"true\"}\n",
    "            )\n",
    "            agent_id = agent.id\n",
    "            print(f\"New agent created. Agent ID: {agent_id}\")\n",
    "        else:\n",
    "            print(f\"Existing agent ID: {agent_id}\")\n",
    "            try:\n",
    "                agent = project_client.agents.get_agent(agent_id)\n",
    "            except Exception as agent_error:\n",
    "                print(f\"Failed to retrieve agent: {agent_error}\")\n",
    "                return []\n",
    "\n",
    "        thread = project_client.agents.create_thread()\n",
    "        \n",
    "        message = project_client.agents.create_message(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=f\"Search the web for: {query}. Return only the top {num} most relevant results as a list.\",\n",
    "        )\n",
    "\n",
    "        print(f\"Message created, ID: {message.id}\")\n",
    "\n",
    "        run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=agent.id)\n",
    "        \n",
    "        if run.status == \"failed\":\n",
    "            print(f\"Execution failed: {run.last_error}\")\n",
    "            return []\n",
    "        print(f\"Run completed successfully. Status: {run.status}\")\n",
    "        results = []\n",
    "        response_message = project_client.agents.list_messages(thread_id=thread.id).get_last_message_by_role(\n",
    "            MessageRole.AGENT\n",
    "        )\n",
    "        if response_message.url_citation_annotations:\n",
    "            # Extract content text and annotations\n",
    "            print(response_message)\n",
    "            if response_message.content:\n",
    "                for content_item in response_message[\"content\"]:\n",
    "                    if content_item[\"type\"] == \"text\":\n",
    "                        text_content = content_item[\"text\"][\"value\"]\n",
    "                        print(\"Extracted Text Content:\")\n",
    "                        print(text_content)\n",
    "                        results.append({\"content\": text_content})\n",
    "            \n",
    "            if response_message.url_citation_annotations:\n",
    "                for annotation in response_message.url_citation_annotations:\n",
    "                    if annotation[\"type\"] == \"url_citation\":\n",
    "                        url_citation = annotation[\"url_citation\"]\n",
    "                        url = url_citation[\"url\"]\n",
    "                        title = url_citation[\"title\"]\n",
    "                        # set the results same as google json format\n",
    "                        results.append({\"url_citation\":{\"link\": url, \"title\": title}})\n",
    "\n",
    "        if not BING_GROUNDING_AGENT_ID and hasattr(agent, 'id'):\n",
    "            try:\n",
    "                print(f\"Deleting temporary agent with ID: {agent.id}\")\n",
    "                project_client.agents.delete_agent(agent.id)\n",
    "                \n",
    "            except Exception as delete_error:\n",
    "                print(f\"Error deleting agent: {delete_error}\")\n",
    "\n",
    "        return results if results else []\n",
    "    except Exception as e:\n",
    "        print(f\"Bing Grounding error : {e}\")\n",
    "        return []\n",
    "\n",
    "def web_search(query, num=5, search_type=\"web\"):\n",
    "    \"\"\"í™˜ê²½ ë³€ìˆ˜ì— ë”°ë¼ Google Search API ë˜ëŠ” Bing Groundingì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ìˆ˜í–‰\"\"\"\n",
    "    \n",
    "    if WEB_SEARCH_MODE == \"bing\":\n",
    "        print(f\"Bing Grounding ê²€ìƒ‰ ì‚¬ìš©: {query}\")\n",
    "        try:\n",
    "            return bing_grounding_search(query, num, search_type)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Bing Grounding ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    else:\n",
    "        print(f\"Google Search API ì‚¬ìš©: {query}\")\n",
    "        return google_search(query, num, search_type)\n",
    "\n",
    "       \n",
    "QUERY_REWRITE_PROMPT = \"\"\"\n",
    "            <<ì§€ì‹œë¬¸>>\n",
    "            ë„ˆëŠ” êµ¬ê¸€ ê²€ìƒ‰ê³¼ LLM ì§ˆì˜ ìµœì í™” ì „ë¬¸ê°€ì•¼. ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ì„ ë‘ ê°€ì§€ ëª©ì ì— ë§ê²Œ ì¬ì‘ì„±í•´.\n",
    "\n",
    "            1. Web Searchìš© Query Rewrite:\n",
    "            - ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì‹¤ì œ ê²€ìƒ‰ ì—”ì§„ ê²€ìƒ‰ì°½ì— ì…ë ¥í•  ìˆ˜ ìˆë„ë¡, ëª…í™•í•˜ê³  ê°„ê²°í•œ í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ì‹¬ì˜ ê²€ìƒ‰ì–´ë¡œ ì¬ì‘ì„±í•´.\n",
    "            - ë¶ˆí•„ìš”í•œ ë¬¸ì¥, ë§¥ë½ ì„¤ëª…ì€ ë¹¼ê³ , ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë§Œë“¤ì–´.\n",
    "            - í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì‚¬ìš©í•´ ê²€ìƒ‰ì˜ ì •í™•ë„ë¥¼ ë†’ì—¬.\n",
    "\n",
    "            2. LLM Queryìš© Rewrite:\n",
    "            - ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ LLMì´ ë” ì˜ ì´í•´í•˜ê³  ë‹µë³€í•  ìˆ˜ ìˆë„ë¡, ë§¥ë½ê³¼ ì˜ë„ë¥¼ ëª…í™•íˆ ë“œëŸ¬ë‚´ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì¬ì‘ì„±í•´.\n",
    "            - í•„ìš”í•œ ê²½ìš° ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ì„¸ë¶€ ì¡°ê±´ì„ í¬í•¨í•´ì„œ ì§ˆë¬¸ì˜ ëª©ì ì´ ë¶„ëª…íˆ ë“œëŸ¬ë‚˜ë„ë¡ ë§Œë“¤ì–´.\n",
    "            - LLMì´ ë‹µë³€ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ í•µì‹¬ ë‹¨ì–´ë¥¼ ë°˜ë³µ ì‚¬ìš©í•´.\n",
    "\n",
    "            <<ì˜ˆì‹œ>>\n",
    "            * ì§ˆë¬¸: ì‚¼ì„±ì „ì ì œí’ˆ ì¤‘ 2êµ¬ ë§ê³  ë‹¤ë¥¸ ì¸ë•ì…˜ ì¶”ì²œí•´ì¤˜\n",
    "            * ì›¹ ê²€ìƒ‰ìš© ì¬ì‘ì„±: ì‚¼ì„±ì „ì 3êµ¬ ì´ìƒ ì¸ë•ì…˜ ì¶”ì²œ\n",
    "            * LLM ë‹µë³€ìš© ì¬ì‘ì„±: ì‚¼ì„±ì „ì ì¸ë•ì…˜ ì¤‘ 2êµ¬ ëª¨ë¸ì´ ì•„ë‹Œ, 3êµ¬ ì´ìƒ ë˜ëŠ” ë‹¤ì–‘í•œ í™”êµ¬ ìˆ˜ë¥¼ ê°€ì§„ ë‹¤ë¥¸ ì¸ë•ì…˜ ì œí’ˆì„ ì¶”ì²œí•´ ì£¼ì„¸ìš”. ê° ëª¨ë¸ì˜ ì£¼ìš” ê¸°ëŠ¥ê³¼ ì¥ì ë„ í•¨ê»˜ ì•Œë ¤ì£¼ì„¸ìš”.\n",
    "\n",
    "            <<ì§ˆë¬¸>>\n",
    "            {user_query}\n",
    "\n",
    "            <<ì¶œë ¥í¬ë§·>>\n",
    "            ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì´ json í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´.\n",
    "            {\"web_search\": \"ì›¹ ê²€ìƒ‰ìš© ì¬ì‘ì„±\", \"llm_query\": \"LLM ë‹µë³€ìš© ì¬ì‘ì„±\"}\n",
    "        \"\"\"     \n",
    "  \n",
    "def rewrite_query_for_search_and_llm(query, client: AzureOpenAI):\n",
    "        response = client.chat.completions.create(\n",
    "            model=CHAT_COMPLETIONS_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": QUERY_REWRITE_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ],\n",
    "            temperature=0.8,\n",
    "            max_tokens=300,\n",
    "            response_format= {\"type\": \"json_object\"},\n",
    "        )\n",
    "        \n",
    "        return json.loads(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69b6f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "#TODO ë‚ ì”¨ë‚˜ ë‰´ìŠ¤, ê¸°íƒ€ ë‹¤ë¥¸ íŠ¹ì •ì •ë³´ëŠ” Function Call\n",
    "# inputs = [\"ë‚ ì”¨, ë‰´ìŠ¤\"] ##\n",
    "\n",
    "async def process_web_search_call(RESULTS_COUNT, input):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    contexts = [] \n",
    "    url_citations= []\n",
    "    print(f\"Original Input: {input}\")\n",
    "    \n",
    "    query_rewrite = rewrite_query_for_search_and_llm(input, client)\n",
    "    print(f\"Web Search Query: {query_rewrite['web_search']}\")\n",
    "    print(f\"LLM Query: {query_rewrite['llm_query']}\")\n",
    "\n",
    "    results = web_search(query_rewrite['web_search'], RESULTS_COUNT)\n",
    "    \n",
    "    if WEB_SEARCH_MODE == \"bing\" and results and isinstance(results, list) and len(results) > 0:\n",
    "        print(f\"Web Search Results: {len(results)}\")\n",
    "        contexts = [results[i][\"content\"] for i in range(len(results)) if \"content\" in results[i]]\n",
    "        url_citations = [results[i][\"url_citation\"] for i in range(len(results)) if \"url_citation\" in results[i]]\n",
    "        \n",
    "        # top_urls = [results[i][\"link\"] for i in range(len(results))]\n",
    "        # contexts = await add_context_async(top_urls)\n",
    "    elif WEB_SEARCH_MODE == \"google\" and results and isinstance(results, list) and len(results) > 0:\n",
    "        print(f\"Web Search Results: {len(results)}\")\n",
    "        top_urls = [results[i][\"link\"] for i in range(len(results))]\n",
    "        contexts = await add_context_async(top_urls)\n",
    "    \n",
    "    else:\n",
    "        print(\"No results found or invalid response from web_search.\")\n",
    "        contexts = [] \n",
    "        url_citations= []\n",
    "\n",
    "    # for i, context in enumerate(contexts):\n",
    "    #     print(f\"Context {i+1}: {context}...\")  # Print first 1000 chars of each context\n",
    "    #     print(\"\\n--- End of Context ---\\n\")\n",
    "\n",
    "    now = datetime.now()\n",
    "    year = now.year\n",
    "    month = now.month\n",
    "    day = now.day\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "        ë„ˆëŠ” ì‚¼ì„±ì „ì ì œí’ˆ ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì±—ë´‡ì´ì•¼. \n",
    "        ë‹µë³€ì€ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì´ëª¨ì§€ë¥¼ 1~2ê°œ í¬í•¨í•´ì„œ ì‘ì„±í•´ì¤˜. \n",
    "        contexts, url_citationsë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ í’ë¶€í•˜ê²Œ ë‹µë³€ì„ í•´ì•¼í•´. \n",
    "        ë§í¬ë¥¼ ì¶”ê°€í• ë•ŒëŠ” ì›¹ê²€ìƒ‰ì—ì„œ ì œê³µí•œ url_citationsì„ ê¸°ì¤€ìœ¼ë¡œ í•¨ê»˜ í¬í•¨í•´ì¤˜. \n",
    "        ì‚¬ìš©ìê°€ ì§ˆë¬¸í•œ ë‚´ìš©ì— ëŒ€í•´ ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•´ì•¼ í•´. contextsê°€ ë¶€ì¡±í•˜ë©´ ìµœì†Œí•œì˜ ì•ˆë‚´ë§Œ í•´ì¤˜. \n",
    "    \"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "        ë„ˆëŠ” ì•„ë˜ ì œê³µí•˜ëŠ” ì›¹ê²€ìƒ‰ì—ì„œ ê²€ìƒ‰í•œ contextsë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•´ì•¼ í•´. \n",
    "        í˜„ì¬ëŠ” {year}ë…„ {month}ì›” {day}ì¼ì´ë¯€ë¡œ ìµœì‹ ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ í•´ì¤˜.\n",
    "        ì›¹ê²€ìƒ‰ì—ì„œ ì œê³µí•œ contexts: {contexts}\n",
    "        ì›¹ê²€ìƒ‰ì—ì„œ ì œê³µí•œ url_citations: {url_citations}\n",
    "        ì§ˆë¬¸: {query_rewrite['llm_query']}\n",
    "        \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=CHAT_COMPLETIONS_MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                 {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        top_p=0.9,\n",
    "        max_tokens=1500\n",
    "    )\n",
    "\n",
    "    display(Markdown(response.choices[0].message.content))\n",
    "    end_time = time.time()\n",
    "    print(f\"elapsed time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4999ff08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bing Grounding ê²€ìƒ‰ ì‚¬ìš©: ë¶€ëª¨ë‹˜ì—ê²Œ ì„ ë¬¼í•˜ê³  ì‹¶ì€ë° ì‚¼ì„±ì „ì TV ì¶”ì²œí•´ì¤˜\n",
      "Original Input: ë¶€ëª¨ë‹˜ì—ê²Œ ì„ ë¬¼í•˜ê³  ì‹¶ì€ë° ì‚¼ì„±ì „ì TV ì¶”ì²œí•´ì¤˜\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Search Query: ì‚¼ì„±ì „ì TV ì¶”ì²œ\n",
      "LLM Query: ë¶€ëª¨ë‹˜ì—ê²Œ ì„ ë¬¼í•˜ê¸° ì¢‹ì€ ì‚¼ì„±ì „ì TV ëª¨ë¸ì„ ì¶”ì²œí•´ ì£¼ì„¸ìš”. ê° ëª¨ë¸ì˜ ì£¼ìš” ê¸°ëŠ¥ê³¼ ì¥ì ë„ í•¨ê»˜ ì„¤ëª…í•´ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.\n",
      "Bing Grounding ê²€ìƒ‰ ì‚¬ìš©: ì‚¼ì„±ì „ì TV ì¶”ì²œ\n",
      "Existing agent ID: asst_1BNu5p4Wv52Cload5HkW3ZBa\n",
      "Message created, ID: msg_o3bwuHb7sbIGhFDynSWBwJeu\n",
      "Run completed successfully. Status: RunStatus.COMPLETED\n",
      "No results found or invalid response from web_search.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "í˜„ì¬ ì œê³µëœ ì •ë³´ê°€ ë¶€ì¡±í•´ êµ¬ì²´ì ì¸ ëª¨ë¸ì— ëŒ€í•œ ì„¸ë¶€ ì‚¬í•­ì„ ì œê³µí•  ìˆ˜ëŠ” ì—†ì§€ë§Œ, ë¶€ëª¨ë‹˜ì—ê²Œ ì„ ë¬¼í•˜ê¸° ì¢‹ì€ ì‚¼ì„±ì „ì TV ëª¨ë¸ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì¶”ì²œì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸŒŸ\n",
       "\n",
       "### ì¶”ì²œ ëª¨ë¸: ì‚¼ì„± QLED TV\n",
       "\n",
       "1. **QLED 4K TV**\n",
       "   - **ì£¼ìš” ê¸°ëŠ¥**: \n",
       "     - QLED ê¸°ìˆ ë¡œ ë” ìƒìƒí•˜ê³  ë„“ì€ ìƒ‰ ì˜ì—­ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
       "     - 4K í•´ìƒë„ë¡œ ë›°ì–´ë‚œ í™”ì§ˆì„ êµ¬í˜„í•˜ë©° HDR ì§€ì›ìœ¼ë¡œ ì–´ë‘ìš´ ì¥ë©´ê³¼ ë°ì€ ì¥ë©´ì—ì„œ ë”ìš± ê¹Šì´ ìˆëŠ” ëª…ì•”ì„ ê²½í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
       "   - **ì¥ì **:\n",
       "     - ì‹œì²­í•˜ëŠ” ê°ë„ì— ë”°ë¼ ìƒ‰ìƒì´ ì™œê³¡ë˜ì§€ ì•Šì•„ ê°€ì¡± ëª¨ë‘ê°€ í¸ì•ˆí•˜ê²Œ ì¦ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
       "     - ìŠ¤ë§ˆíŠ¸ ê¸°ëŠ¥ì´ íƒ‘ì¬ë˜ì–´ ìˆì–´ Netflix, YouTube ë“± ë‹¤ì–‘í•œ ìŠ¤íŠ¸ë¦¬ë° ì„œë¹„ìŠ¤ë¥¼ ì‰½ê²Œ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
       "\n",
       "2. **Neo QLED TV**\n",
       "   - **ì£¼ìš” ê¸°ëŠ¥**: \n",
       "     - Mini LED ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ë”ìš± ì„¬ì„¸í•œ ë°ê¸° ì¡°ì ˆì´ ê°€ëŠ¥í•˜ê³ , ê¹Šì€ ë¸”ë™ê³¼ ë°ì€ í™”ì´íŠ¸ë¥¼ ë™ì‹œì— í‘œí˜„í•©ë‹ˆë‹¤.\n",
       "     - ê³ ê¸‰ AI í”„ë¡œì„¸ì„œë¡œ ìµœì í™”ëœ í™”ì§ˆì„ ì œê³µí•©ë‹ˆë‹¤.\n",
       "   - **ì¥ì **:\n",
       "     - ëª°ì…ê° ìˆëŠ” ì‹œì²­ ê²½í—˜ì„ ì œê³µí•˜ì—¬ ì˜í™”ë‚˜ ë“œë¼ë§ˆë¥¼ ì¦ê¸°ëŠ” ë° ì í•©í•©ë‹ˆë‹¤.\n",
       "     - ìŠ¬ë¦¼í•œ ë””ìì¸ìœ¼ë¡œ ì–´ë–¤ ê³µê°„ì—ì„œë„ ì˜ ì–´ìš¸ë¦½ë‹ˆë‹¤.\n",
       "\n",
       "ë¶€ëª¨ë‹˜ì´ TVë¥¼ ì£¼ë¡œ ì–´ë–¤ ìš©ë„ë¡œ ì‚¬ìš©í•˜ì‹œëŠ”ì§€ì— ë”°ë¼ ì„ íƒì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ í†µí•´ í¸ë¦¬í•œ ì‚¬ìš©ì„±ì„ ì œê³µí•˜ë¯€ë¡œ, ì‚¼ì„± QLED TVë‚˜ Neo QLED TVëŠ” ë§¤ìš° ì¢‹ì€ ì„ íƒì´ ë  ê²ƒì…ë‹ˆë‹¤. ğŸ˜Š\n",
       "\n",
       "ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•˜ì‹œê±°ë‚˜ êµ¬ì²´ì ì¸ ëª¨ë¸ëª…ì„ ì›í•˜ì‹ ë‹¤ë©´, ìµœì‹  ì •ë³´ë¥¼ í™•ì¸í•˜ì‹œëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 18.91 seconds\n",
      "Bing Grounding ê²€ìƒ‰ ì‚¬ìš©: ì‚¼ì„±ì „ì 25ë…„ ì œí’ˆì´ ì‘ë…„ ëŒ€ë¹„ ì¢‹ì•„ì§„ê²ƒì€\n",
      "Original Input: ì‚¼ì„±ì „ì 25ë…„ ì œí’ˆì´ ì‘ë…„ ëŒ€ë¹„ ì¢‹ì•„ì§„ê²ƒì€\n",
      "Web Search Query: ì‚¼ì„±ì „ì 2025ë…„ ì œí’ˆ ì‘ë…„ ëŒ€ë¹„ ê°œì„  ì‚¬í•­\n",
      "LLM Query: ì‚¼ì„±ì „ìê°€ 2025ë…„ì— ì¶œì‹œí•œ ì œí’ˆì´ ì‘ë…„ ëª¨ë¸ ëŒ€ë¹„ ì–´ë–»ê²Œ ê°œì„ ë˜ì—ˆëŠ”ì§€, ì–´ë–¤ ê¸°ëŠ¥ì´ë‚˜ ì„±ëŠ¥ì´ ì¢‹ì•„ì¡ŒëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
      "Bing Grounding ê²€ìƒ‰ ì‚¬ìš©: ì‚¼ì„±ì „ì 2025ë…„ ì œí’ˆ ì‘ë…„ ëŒ€ë¹„ ê°œì„  ì‚¬í•­\n",
      "Existing agent ID: asst_1BNu5p4Wv52Cload5HkW3ZBa\n",
      "Message created, ID: msg_92vNJztHSA87oL2ZvYXFt4fN\n",
      "Run completed successfully. Status: RunStatus.COMPLETED\n",
      "No results found or invalid response from web_search.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "í˜„ì¬ ì œê³µëœ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ 2025ë…„ì— ì¶œì‹œëœ ì‚¼ì„±ì „ì ì œí’ˆì— ëŒ€í•œ êµ¬ì²´ì ì¸ ê°œì„  ì‚¬í•­ì´ë‚˜ ê¸°ëŠ¥, ì„±ëŠ¥ í–¥ìƒì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ğŸ§ \n",
       "\n",
       "í•˜ì§€ë§Œ ì‚¼ì„±ì „ìëŠ” ë§¤ë…„ ì‹ ì œí’ˆì„ í†µí•´ ê¸°ìˆ  í˜ì‹ ì„ ì„ ë³´ì´ë©°, ì¼ë°˜ì ìœ¼ë¡œ ì¹´ë©”ë¼ ì„±ëŠ¥, ë°°í„°ë¦¬ ìˆ˜ëª…, í”„ë¡œì„¸ì„œ ì„±ëŠ¥, ë””ìŠ¤í”Œë ˆì´ ê¸°ìˆ  ë“±ì´ í–¥ìƒë˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì •ë³´ëŠ” ì‚¼ì„±ì „ìì˜ ê³µì‹ ì›¹ì‚¬ì´íŠ¸ë‚˜ ê´€ë ¨ ë³´ë„ìë£Œë¥¼ ì°¸ê³ í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. \n",
       "\n",
       "ì¶”í›„ì— ë” ë§ì€ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë‹¤ë©´, íŠ¹ì • ì œí’ˆì— ëŒ€í•œ ì§ˆë¬¸ì„ ì£¼ì‹œë©´ ë” ë‚˜ì€ ì•ˆë‚´ë¥¼ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ˜Š"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 10.76 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RESULTS_COUNT = 3\n",
    "\n",
    "inputs = [\n",
    "    # \"ì‚¼ì„±ì „ì ì œí’ˆ ì¤‘ 2êµ¬ ë§ê³  ë‹¤ë¥¸ ì¸ë•ì…˜ ì¶”ì²œí•´ì¤˜\",\n",
    "    # \"ë¶€ëª¨ë‹˜ì—ê²Œ ì„ ë¬¼í•˜ê³  ì‹¶ì€ë° ì‚¼ì„±ì „ì TV ì¶”ì²œí•´ì¤˜\",\n",
    "    # \"ì‚¼ì„±ì „ì 25ë…„ ì œí’ˆì´ ì‘ë…„ ëŒ€ë¹„ ì¢‹ì•„ì§„ê²ƒì€\",\n",
    "    \"ì‚¼ì„±ì „ì JBLê³¼ í•˜ë§Œì¹´ëˆ ì°¨ì´ì ì´ ë­ì•¼\",\n",
    "    \"ê°¤ëŸ­ì‹œ ë²„ì¦ˆ ì´ì–´ë²„ë“œ í•œìª½ì„ ìƒˆë¡œ êµ¬ë§¤í–ˆëŠ”ë° í˜ì–´ë§ ì–´ë–»ê²Œ í•˜ë‚˜ìš”\",\n",
    "    \"ì‚¼ì„±ì „ì S25 ë¬´ê²Œê°€ S24ì™€ ë¹„êµ í–ˆì„ë•Œ ì–¼ë§ˆë‚˜ ì°¨ì´ë‚˜\"\n",
    "]\n",
    "\n",
    "\n",
    "# for input in inputs:\n",
    "#     WEB_SEARCH_MODE = \"google\"  \n",
    "#     print(f\"Google Search API ì‚¬ìš©: {input}\")\n",
    "#     await process_web_search_call(RESULTS_COUNT, input)\n",
    "\n",
    "WEB_SEARCH_MODE = \"bing\"\n",
    "\n",
    "for input in inputs:\n",
    "    print(f\"Bing Grounding ê²€ìƒ‰ ì‚¬ìš©: {input}\")\n",
    "    await process_web_search_call(RESULTS_COUNT, input)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5023d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_agentlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
