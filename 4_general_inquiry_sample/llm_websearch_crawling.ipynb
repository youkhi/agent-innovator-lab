{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "138d2cb2",
   "metadata": {},
   "source": [
    "# LLM with Web Search and Crawl\n",
    "\n",
    "Code to crawl the top n pages of a Google search result and serve them to LLM in order to utilize rich context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5623f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True) \n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "CHAT_COMPLETIONS_MODEL = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13ea70",
   "metadata": {},
   "source": [
    "bs4 or scrapy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "897bb2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import scrapy\n",
    "from bs4 import BeautifulSoup\n",
    "import httpx\n",
    "import asyncio\n",
    "from urllib.parse import urljoin\n",
    "from azure.ai.projects.models import MessageRole, BingGroundingTool\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\")\n",
    "BING_GROUNDING_PROJECT_CONNECTION_STRING = os.getenv(\"BING_GROUNDING_PROJECT_CONNECTION_STRING\")\n",
    "BING_GROUNDING_AGENT_ID = os.getenv(\"BING_GROUNDING_AGENT_ID\")\n",
    "BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME = os.getenv(\"BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME\")\n",
    "BING_GROUNDING_CONNECTION_NAME = os.getenv(\"BING_GROUNDING_CONNECTION_NAME\")\n",
    "# Web search mode: \"google\" or \"bing\"\n",
    "# it can be changed when users want to use different search engine\n",
    "WEB_SEARCH_MODE = os.getenv(\"WEB_SEARCH_MODE\")\n",
    "\n",
    "def extract_text_and_tables_by_bs4(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # Extract main text\n",
    "    paragraphs = [p.get_text().strip() for p in soup.find_all(\"p\") if p.get_text().strip()]\n",
    "    text = \"\\n\".join(paragraphs)\n",
    "    return text\n",
    "\n",
    "\n",
    "async def extract_text_and_tables_async(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=3, follow_redirects=True) as client:\n",
    "        try:\n",
    "            response = await client.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            # Handle 302 redirect manually if follow_redirects fails\n",
    "            if e.response.status_code == 302 and \"location\" in e.response.headers:\n",
    "                redirect_url = e.response.headers[\"location\"]\n",
    "                if not redirect_url.startswith(\"http\"):\n",
    "                    # handle relative redirects\n",
    "                    redirect_url = urljoin(url, redirect_url)\n",
    "                try:\n",
    "                    response = await client.get(redirect_url, headers=headers)\n",
    "                    response.raise_for_status()\n",
    "                except Exception as e2:\n",
    "                    print(f\"Redirect request failed: {e2}\")\n",
    "                    return \"\"\n",
    "            else:\n",
    "                print(f\"Request failed: {e}\")\n",
    "                return \"\"\n",
    "        except httpx.HTTPError as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "        selector = scrapy.Selector(text=response.text)\n",
    "        paragraphs = [p.strip() for p in selector.css('p::text').getall() if p.strip()]\n",
    "        text = \"\\n\".join(paragraphs)\n",
    "        return text\n",
    "\n",
    "async def add_context_async(top_urls = []):\n",
    "    async def gather_contexts():\n",
    "        tasks = [extract_text_and_tables_async(url) for url in top_urls]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return results\n",
    "    return await gather_contexts()\n",
    "\n",
    "def google_search(query, num=5, search_type=\"web\"):\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"key\": GOOGLE_API_KEY,\n",
    "        \"cx\": GOOGLE_CSE_ID,\n",
    "        \"num\": num, \n",
    "        \"locale\": \"ko\",  # í•œêµ­ì–´ë¡œ ê²€ìƒ‰\n",
    "        \"siteSearch\": \"samsung.com\",\n",
    "        \"siteSearchFilter\": \"e\"\n",
    "    }\n",
    "    \n",
    "    if search_type == \"image\":\n",
    "        params[\"searchType\"] = \"image\"\n",
    "        \n",
    "    response = requests.get(url, params=params)\n",
    "    results = response.json()\n",
    "    return results.get(\"items\", [])\n",
    "\n",
    "def bing_grounding_search(query, num=5, search_type=\"web\"):\n",
    "    try:\n",
    "        creds = DefaultAzureCredential()\n",
    "        \n",
    "        project_client = AIProjectClient.from_connection_string(\n",
    "            credential=creds,\n",
    "            conn_str=BING_GROUNDING_PROJECT_CONNECTION_STRING,\n",
    "        )\n",
    "        \n",
    "        agent_id = BING_GROUNDING_AGENT_ID\n",
    "        \n",
    "        if not agent_id:\n",
    "            print(\"BING_GROUNDING_AGENT_ID is not set. Create new agent...\")\n",
    "            connection_name = BING_GROUNDING_CONNECTION_NAME\n",
    "            \n",
    "            bing_connection = project_client.connections.get(\n",
    "                connection_name=connection_name,\n",
    "            )\n",
    "            conn_id = bing_connection.id\n",
    "            \n",
    "            bing = BingGroundingTool(connection_id=conn_id)\n",
    "            \n",
    "            \n",
    "            agent = project_client.agents.create_agent(\n",
    "                model=BING_GROUNDING_AGENT_MODEL_DEPLOYMENT_NAME,\n",
    "                name=\"temporary-bing-agent\",\n",
    "                instructions=\"\"\"\n",
    "                    Search for product information exclusively about Microsoft products.\n",
    "                    Only respond with information from trusted sources: microsoft.com and tistory.com.\n",
    "                    Prioritize data from microsoft.com whenever available to ensure accuracy and reliability.\n",
    "                    If information is not found on microsoft.com, supplement with tistory.com, but always indicate the source.\n",
    "                    Avoid using data from any other websites or unverified sources.\n",
    "                \"\"\",\n",
    "                tools=bing.definitions,\n",
    "                headers={\"x-ms-enable-preview\": \"true\"}\n",
    "            )\n",
    "            agent_id = agent.id\n",
    "            print(f\"New agent created. Agent ID: {agent_id}\")\n",
    "        else:\n",
    "            print(f\"Existing agent ID: {agent_id}\")\n",
    "            try:\n",
    "                agent = project_client.agents.get_agent(agent_id)\n",
    "            except Exception as agent_error:\n",
    "                print(f\"Failed to retrieve agent: {agent_error}\")\n",
    "                return []\n",
    "\n",
    "        thread = project_client.agents.create_thread()\n",
    "        \n",
    "        message = project_client.agents.create_message(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=f\"Search the web for: {query}. Return only the top {num} most relevant results as a list.\",\n",
    "        )\n",
    "\n",
    "        print(f\"Message created, ID: {message.id}\")\n",
    "\n",
    "        run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=agent.id)\n",
    "        \n",
    "        if run.status == \"failed\":\n",
    "            print(f\"Execution failed: {run.last_error}\")\n",
    "            return []\n",
    "        print(f\"Run completed successfully. Status: {run.status}\")\n",
    "        results = []\n",
    "        response_message = project_client.agents.list_messages(thread_id=thread.id).get_last_message_by_role(\n",
    "            MessageRole.AGENT\n",
    "        )\n",
    "        if response_message.url_citation_annotations:\n",
    "            # Extract content text and annotations\n",
    "            if response_message.content:\n",
    "                for content_item in response_message[\"content\"]:\n",
    "                    if content_item[\"type\"] == \"text\":\n",
    "                        text_content = content_item[\"text\"][\"value\"]\n",
    "                        print(\"Extracted Text Content:\")\n",
    "                        print(text_content)\n",
    "            for annotation in response_message.url_citation_annotations:\n",
    "                if annotation[\"type\"] == \"url_citation\":\n",
    "                    url_citation = annotation[\"url_citation\"]\n",
    "                    url = url_citation[\"url\"]\n",
    "                    title = url_citation[\"title\"]\n",
    "                    # set the results same as google json format\n",
    "                    results.append({\"link\": url, \"title\": title})\n",
    "\n",
    "        if not BING_GROUNDING_AGENT_ID and hasattr(agent, 'id'):\n",
    "            try:\n",
    "                project_client.agents.delete_agent(agent.id)\n",
    "                \n",
    "            except Exception as delete_error:\n",
    "                print(f\"Error deleting agent: {delete_error}\")\n",
    "\n",
    "        return results if results else []\n",
    "    except Exception as e:\n",
    "        print(f\"Bing Grounding error : {e}\")\n",
    "        return []\n",
    "\n",
    "def web_search(query, num=5, search_type=\"web\"):\n",
    "    \"\"\"í™˜ê²½ ë³€ìˆ˜ì— ë”°ë¼ Google Search API ë˜ëŠ” Bing Groundingì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ìˆ˜í–‰\"\"\"\n",
    "    \n",
    "    if WEB_SEARCH_MODE == \"bing\":\n",
    "        print(f\"Bing Grounding ê²€ìƒ‰ ì‚¬ìš©: {query}\")\n",
    "        try:\n",
    "            return bing_grounding_search(query, num, search_type)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Bing Grounding ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    else:\n",
    "        print(f\"Google Search API ì‚¬ìš©: {query}\")\n",
    "        return google_search(query, num, search_type)\n",
    "\n",
    "       \n",
    "QUERY_REWRITE_PROMPT = \"\"\"\n",
    "            <<ì§€ì‹œë¬¸>>\n",
    "            ë„ˆëŠ” êµ¬ê¸€ ê²€ìƒ‰ê³¼ LLM ì§ˆì˜ ìµœì í™” ì „ë¬¸ê°€ì•¼. ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ì„ ë‘ ê°€ì§€ ëª©ì ì— ë§ê²Œ ì¬ì‘ì„±í•´.\n",
    "\n",
    "            1. Web Searchìš© Query Rewrite:\n",
    "            - ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì‹¤ì œ ê²€ìƒ‰ ì—”ì§„ ê²€ìƒ‰ì°½ì— ì…ë ¥í•  ìˆ˜ ìˆë„ë¡, ëª…í™•í•˜ê³  ê°„ê²°í•œ í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ì‹¬ì˜ ê²€ìƒ‰ì–´ë¡œ ì¬ì‘ì„±í•´.\n",
    "            - ë¶ˆí•„ìš”í•œ ë¬¸ì¥, ë§¥ë½ ì„¤ëª…ì€ ë¹¼ê³ , ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë§Œë“¤ì–´.\n",
    "            - í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì‚¬ìš©í•´ ê²€ìƒ‰ì˜ ì •í™•ë„ë¥¼ ë†’ì—¬.\n",
    "\n",
    "            2. LLM Queryìš© Rewrite:\n",
    "            - ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ LLMì´ ë” ì˜ ì´í•´í•˜ê³  ë‹µë³€í•  ìˆ˜ ìˆë„ë¡, ë§¥ë½ê³¼ ì˜ë„ë¥¼ ëª…í™•íˆ ë“œëŸ¬ë‚´ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì¬ì‘ì„±í•´.\n",
    "            - í•„ìš”í•œ ê²½ìš° ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ì„¸ë¶€ ì¡°ê±´ì„ í¬í•¨í•´ì„œ ì§ˆë¬¸ì˜ ëª©ì ì´ ë¶„ëª…íˆ ë“œëŸ¬ë‚˜ë„ë¡ ë§Œë“¤ì–´.\n",
    "            - LLMì´ ë‹µë³€ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ í•µì‹¬ ë‹¨ì–´ë¥¼ ë°˜ë³µ ì‚¬ìš©í•´.\n",
    "\n",
    "            <<ì˜ˆì‹œ>>\n",
    "            * ì§ˆë¬¸: ì‚¼ì„±ì „ì ì œí’ˆ ì¤‘ 2êµ¬ ë§ê³  ë‹¤ë¥¸ ì¸ë•ì…˜ ì¶”ì²œí•´ì¤˜\n",
    "            * ì›¹ ê²€ìƒ‰ìš© ì¬ì‘ì„±: ì‚¼ì„±ì „ì 3êµ¬ ì´ìƒ ì¸ë•ì…˜ ì¶”ì²œ\n",
    "            * LLM ë‹µë³€ìš© ì¬ì‘ì„±: ì‚¼ì„±ì „ì ì¸ë•ì…˜ ì¤‘ 2êµ¬ ëª¨ë¸ì´ ì•„ë‹Œ, 3êµ¬ ì´ìƒ ë˜ëŠ” ë‹¤ì–‘í•œ í™”êµ¬ ìˆ˜ë¥¼ ê°€ì§„ ë‹¤ë¥¸ ì¸ë•ì…˜ ì œí’ˆì„ ì¶”ì²œí•´ ì£¼ì„¸ìš”. ê° ëª¨ë¸ì˜ ì£¼ìš” ê¸°ëŠ¥ê³¼ ì¥ì ë„ í•¨ê»˜ ì•Œë ¤ì£¼ì„¸ìš”.\n",
    "\n",
    "            <<ì§ˆë¬¸>>\n",
    "            {user_query}\n",
    "\n",
    "            <<ì¶œë ¥í¬ë§·>>\n",
    "            ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì´ json í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´.\n",
    "            {\"web_search\": \"ì›¹ ê²€ìƒ‰ìš© ì¬ì‘ì„±\", \"llm_query\": \"LLM ë‹µë³€ìš© ì¬ì‘ì„±\"}\n",
    "        \"\"\"     \n",
    "  \n",
    "def rewrite_query_for_search_and_llm(query, client: AzureOpenAI):\n",
    "        response = client.chat.completions.create(\n",
    "            model=CHAT_COMPLETIONS_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": QUERY_REWRITE_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ],\n",
    "            temperature=0.8,\n",
    "            max_tokens=300,\n",
    "            response_format= {\"type\": \"json_object\"},\n",
    "        )\n",
    "        \n",
    "        return json.loads(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69b6f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "#TODO ë‚ ì”¨ë‚˜ ë‰´ìŠ¤, ê¸°íƒ€ ë‹¤ë¥¸ íŠ¹ì •ì •ë³´ëŠ” Function Call\n",
    "# inputs = [\"ë‚ ì”¨, ë‰´ìŠ¤\"] ##\n",
    "\n",
    "async def process_web_search_call(RESULTS_COUNT, input):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Original Input: {input}\")\n",
    "    \n",
    "    query_rewrite = rewrite_query_for_search_and_llm(input, client)\n",
    "    print(f\"Web Search Query: {query_rewrite['web_search']}\")\n",
    "    print(f\"LLM Query: {query_rewrite['llm_query']}\")\n",
    "\n",
    "    results = web_search(query_rewrite['web_search'], RESULTS_COUNT)\n",
    "    if results and isinstance(results, list) and len(results) > 0:\n",
    "        print(f\"Web Search Results: {len(results)}\")\n",
    "        top_urls = [results[i][\"link\"] for i in range(len(results))]\n",
    "        contexts = await add_context_async(top_urls)\n",
    "    else:\n",
    "        print(\"No results found or invalid response from web_search.\")\n",
    "        contexts = []\n",
    "\n",
    "    # for i, context in enumerate(contexts):\n",
    "    #     print(f\"Context {i+1}: {context}...\")  # Print first 1000 chars of each context\n",
    "    #     print(\"\\n--- End of Context ---\\n\")\n",
    "\n",
    "    now = datetime.now()\n",
    "    year = now.year\n",
    "    month = now.month\n",
    "    day = now.day\n",
    "\n",
    "    system_prompt = \"ë„ˆëŠ” ì‚¼ì„±ì „ì ì œí’ˆ ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì±—ë´‡ì´ì•¼. ë‹µë³€ì€ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì´ëª¨ì§€ë¥¼ 1~2ê°œ í¬í•¨í•´ì„œ ì‘ì„±í•´ì¤˜.\"\n",
    "    user_prompt = f\"\"\"\n",
    "        ë„ˆëŠ” ì•„ë˜ ì œê³µí•˜ëŠ” ì›¹ê²€ìƒ‰ì—ì„œ ê²€ìƒ‰í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•´ì•¼ í•´. ì»¨í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ í’ë¶€í•˜ê²Œ ë‹µë³€ì„ í•´ì•¼í•´. \n",
    "        í˜„ì¬ëŠ” {year}ë…„ {month}ì›” {day}ì¼ì´ë¯€ë¡œ ìµœì‹ ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ í•´ì¤˜.\n",
    "        êµ¬ê¸€ì—ì„œ ì œê³µí•œ ì»¨í…ìŠ¤íŠ¸: {contexts}\n",
    "        ì§ˆë¬¸: {query_rewrite['llm_query']}\n",
    "        \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=CHAT_COMPLETIONS_MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                 {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        top_p=0.9,\n",
    "        max_tokens=1500\n",
    "    )\n",
    "\n",
    "    display(Markdown(response.choices[0].message.content))\n",
    "    end_time = time.time()\n",
    "    print(f\"elapsed time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4999ff08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Search API ì‚¬ìš©: ì‚¼ì„±ì „ì ì œí’ˆ ì¤‘ 2êµ¬ ë§ê³  ë‹¤ë¥¸ ì¸ë•ì…˜ ì¶”ì²œí•´ì¤˜\n",
      "Original Input: ì‚¼ì„±ì „ì ì œí’ˆ ì¤‘ 2êµ¬ ë§ê³  ë‹¤ë¥¸ ì¸ë•ì…˜ ì¶”ì²œí•´ì¤˜\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Search Query: ì‚¼ì„±ì „ì 3êµ¬ ì´ìƒ ì¸ë•ì…˜ ì¶”ì²œ\n",
      "LLM Query: ì‚¼ì„±ì „ì ì¸ë•ì…˜ ì¤‘ 2êµ¬ ëª¨ë¸ì´ ì•„ë‹Œ, 3êµ¬ ì´ìƒ ë˜ëŠ” ë‹¤ì–‘í•œ í™”êµ¬ ìˆ˜ë¥¼ ê°€ì§„ ë‹¤ë¥¸ ì¸ë•ì…˜ ì œí’ˆì„ ì¶”ì²œí•´ ì£¼ì„¸ìš”. ê° ëª¨ë¸ì˜ ì£¼ìš” ê¸°ëŠ¥ê³¼ ì¥ì ë„ í•¨ê»˜ ì•Œë ¤ì£¼ì„¸ìš”.\n",
      "Google Search API ì‚¬ìš©: ì‚¼ì„±ì „ì 3êµ¬ ì´ìƒ ì¸ë•ì…˜ ì¶”ì²œ\n",
      "Web Search Results: 3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ì‚¼ì„±ì „ìëŠ” ë‹¤ì–‘í•œ í™”êµ¬ ìˆ˜ë¥¼ ê°€ì§„ ì¸ë•ì…˜ ì œí’ˆì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. 3êµ¬ ì´ìƒì˜ ëª¨ë¸ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì œí’ˆë“¤ì´ ìˆìŠµë‹ˆë‹¤. \n",
       "\n",
       "### 1. ì‚¼ì„± ì¸ë•ì…˜ 4êµ¬ ëª¨ë¸ (NZ64K7757GK)\n",
       "- **ì£¼ìš” ê¸°ëŠ¥**:\n",
       "  - **Flex Zone**: ëŒ€í˜• íŒ¬ì´ë‚˜ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì¡°ë¦¬ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë„“ì€ ì¡°ë¦¬ ê³µê°„ ì œê³µ.\n",
       "  - **ìŠ¤ë§ˆíŠ¸ ì„¼ì„œ**: ìŒì‹ì˜ ì˜¨ë„ë¥¼ ê°ì§€í•˜ì—¬ ìµœì ì˜ ì¡°ë¦¬ ì˜¨ë„ë¡œ ìœ ì§€.\n",
       "  - **ë¹ ë¥¸ ê°€ì—´**: ë¶€í•˜ê°€ ìˆì„ ë•Œë„ ë¹ ë¥¸ ì‹œê°„ ì•ˆì— ê°•í•œ í™”ë ¥ì„ ì œê³µ.\n",
       "\n",
       "- **ì¥ì **:\n",
       "  - ë‹¤ì–‘í•œ ìš”ë¦¬ë¥¼ ë™ì‹œì— í•  ìˆ˜ ìˆì–´ íš¨ìœ¨ì ì¸ ì¡°ë¦¬ê°€ ê°€ëŠ¥.\n",
       "  - ì„¸ë ¨ëœ ë””ìì¸ìœ¼ë¡œ ì£¼ë°© ì¸í…Œë¦¬ì–´ì™€ ì˜ ì–´ìš¸ë¦¼. âœ¨\n",
       "\n",
       "### 2. ì‚¼ì„± ì¸ë•ì…˜ 5êµ¬ ëª¨ë¸ (NZ75K7777BK)\n",
       "- **ì£¼ìš” ê¸°ëŠ¥**:\n",
       "  - **Multi-Heat Zone**: ê°ê°ì˜ í™”êµ¬ì—ì„œ ë³„ë„ì˜ ì˜¨ë„ë¥¼ ì„¤ì • ê°€ëŠ¥, ë‹¤ì–‘í•œ ìš”ë¦¬ ìŠ¤íƒ€ì¼ ì§€ì›.\n",
       "  - **ìŠ¤ë§ˆíŠ¸ í„°ì¹˜ ì»¨íŠ¸ë¡¤**: ì§ê´€ì ì¸ í„°ì¹˜ ìŠ¤í¬ë¦°ìœ¼ë¡œ í¸ë¦¬í•œ ì¡°ì‘ ê°€ëŠ¥.\n",
       "  - **ì•ˆì „ ê¸°ëŠ¥**: ê³¼ì—´ ë°©ì§€ ë° ìë™ ì „ì› ì°¨ë‹¨ ê¸°ëŠ¥ìœ¼ë¡œ ì•ˆì „í•œ ì‚¬ìš©.\n",
       "\n",
       "- **ì¥ì **:\n",
       "  - ë‹¤ì¤‘ ìš”ë¦¬ ê¸°ëŠ¥ìœ¼ë¡œ ê°€ì¡± ì‹ì‚¬ë¥¼ ì‰½ê²Œ ì¤€ë¹„í•  ìˆ˜ ìˆìŒ.\n",
       "  - ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì¡°ë¦¬ê¸°êµ¬ ì‚¬ìš© ê°€ëŠ¥í•˜ì—¬ ì¡°ë¦¬ì˜ ìœ ì—°ì„± ì¦ê°€. ğŸ³\n",
       "\n",
       "ì´ ì™¸ì—ë„ ì‚¼ì„±ì „ìëŠ” ìµœì‹  ê¸°ìˆ ì´ ì ìš©ëœ ë‹¤ì–‘í•œ ì¸ë•ì…˜ ëª¨ë¸ì„ ë³´ìœ í•˜ê³  ìˆìœ¼ë‹ˆ, ì‚¬ìš©ìì˜ í•„ìš”ì— ë§ëŠ” ì œí’ˆì„ ì„ íƒí•˜ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 5.50 seconds\n",
      "Bing Grounding ê²€ìƒ‰ ì‚¬ìš©: ì‚¼ì„±ì „ì ì œí’ˆ ì¤‘ 2êµ¬ ë§ê³  ë‹¤ë¥¸ ì¸ë•ì…˜ ì¶”ì²œí•´ì¤˜\n",
      "Original Input: ì‚¼ì„±ì „ì ì œí’ˆ ì¤‘ 2êµ¬ ë§ê³  ë‹¤ë¥¸ ì¸ë•ì…˜ ì¶”ì²œí•´ì¤˜\n",
      "Web Search Query: ì‚¼ì„±ì „ì 3êµ¬ ì´ìƒ ì¸ë•ì…˜ ì¶”ì²œ\n",
      "LLM Query: ì‚¼ì„±ì „ì ì¸ë•ì…˜ ì¤‘ 2êµ¬ ëª¨ë¸ì´ ì•„ë‹Œ, 3êµ¬ ì´ìƒ ë˜ëŠ” ë‹¤ì–‘í•œ í™”êµ¬ ìˆ˜ë¥¼ ê°€ì§„ ë‹¤ë¥¸ ì¸ë•ì…˜ ì œí’ˆì„ ì¶”ì²œí•´ ì£¼ì„¸ìš”. ê° ëª¨ë¸ì˜ ì£¼ìš” ê¸°ëŠ¥ê³¼ ì¥ì ë„ í•¨ê»˜ ì•Œë ¤ì£¼ì„¸ìš”.\n",
      "Google Search API ì‚¬ìš©: ì‚¼ì„±ì „ì 3êµ¬ ì´ìƒ ì¸ë•ì…˜ ì¶”ì²œ\n",
      "Web Search Results: 3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ì‚¼ì„±ì „ìëŠ” ë‹¤ì–‘í•œ í™”êµ¬ ìˆ˜ë¥¼ ê°€ì§„ ì¸ë•ì…˜ ëª¨ë¸ì„ ì—¬ëŸ¬ ê°€ì§€ ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. 3êµ¬ ì´ìƒì˜ ì¸ë•ì…˜ ì œí’ˆ ì¤‘ ëª‡ ê°€ì§€ ì¶”ì²œ ëª¨ë¸ê³¼ ê·¸ ì£¼ìš” ê¸°ëŠ¥ì„ ì†Œê°œí• ê²Œìš”. ğŸ”¥ğŸ‘©â€ğŸ³\n",
       "\n",
       "### 1. ì‚¼ì„± ì¸ë•ì…˜ 4êµ¬ ëª¨ë¸ (ì˜ˆ: NZ84T9777K)\n",
       "- **ì£¼ìš” ê¸°ëŠ¥:**\n",
       "  - **Flex Zone:** í° ëƒ„ë¹„ë‚˜ íŒ¬ì„ ì‚¬ìš©í•˜ê¸°ì— ì í•©í•œ ë„“ì€ ì¡°ë¦¬ ê³µê°„.\n",
       "  - **ìŠ¤ë§ˆíŠ¸ ì„¼ì„œ:** ì˜¨ë„ ì¡°ì ˆ ë° ì¡°ë¦¬ ì‹œê°„ì„ ìë™ìœ¼ë¡œ ê´€ë¦¬í•´ì£¼ëŠ” ê¸°ëŠ¥.\n",
       "- **ì¥ì :**\n",
       "  - í•œ ë²ˆì— ì—¬ëŸ¬ ìš”ë¦¬ë¥¼ ì‰½ê²Œ í•  ìˆ˜ ìˆì–´ ì‹œê°„ ì ˆì•½ì— ìœ ë¦¬í•©ë‹ˆë‹¤.\n",
       "  - ì‚¬ìš©ì ì¹œí™”ì ì¸ í„°ì¹˜ íŒ¨ë„ë¡œ ì§ê´€ì ì¸ ì¡°ì‘ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
       "\n",
       "### 2. ì‚¼ì„± ì¸ë•ì…˜ 5êµ¬ ëª¨ë¸ (ì˜ˆ: NZ75T9977K)\n",
       "- **ì£¼ìš” ê¸°ëŠ¥:**\n",
       "  - **Virtual Flame Technology:** ì¸ë•ì…˜ì˜ ê°•ë ¥í•œ ì—´ì„ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œí•´ ìš”ë¦¬ ì‹œ ë”ìš± ì•ˆì „í•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥.\n",
       "  - **Wi-Fi ì—°ê²°:** ìŠ¤ë§ˆíŠ¸í°ê³¼ ì—°ë™í•´ ì›ê²©ìœ¼ë¡œ ì¡°ë¦¬ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
       "- **ì¥ì :**\n",
       "  - ë‹¤ì–‘í•œ í¬ê¸°ì˜ íŒ¬ê³¼ ëƒ„ë¹„ë¥¼ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ìˆì–´ ëŒ€ê°€ì¡±ì´ë‚˜ ì†ë‹˜ ì´ˆëŒ€ ì‹œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
       "  - ì„¸ë ¨ëœ ë””ìì¸ìœ¼ë¡œ ì£¼ë°© ì¸í…Œë¦¬ì–´ì™€ ì˜ ì–´ìš¸ë¦½ë‹ˆë‹¤.\n",
       "\n",
       "### 3. ì‚¼ì„± ì¸ë•ì…˜ 3êµ¬ + 1 êµ¬ì—­ í™•ì¥ ëª¨ë¸ (ì˜ˆ: NZ63T9777K)\n",
       "- **ì£¼ìš” ê¸°ëŠ¥:**\n",
       "  - **Zone Cooking:** ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì¡°ë¦¬ê¸°êµ¬ì— ë§ì¶° ì¡°ë¦¬ êµ¬ì—­ì„ ì¡°ì •í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥.\n",
       "  - **Power Boost:** ê¸‰ì†í•œ ì—´ ì¡°ì ˆë¡œ ë¹ ë¥¸ ì¡°ë¦¬ê°€ ê°€ëŠ¥.\n",
       "- **ì¥ì :**\n",
       "  - íŠ¹ì • êµ¬ì—­ì„ í™•ì¥í•´ í° ìš”ë¦¬ë¥¼ ë™ì‹œì— í•  ìˆ˜ ìˆì–´ ì‹¤ìš©ì ì…ë‹ˆë‹¤.\n",
       "  - ë‹¤ì–‘í•œ ìš”ë¦¬ ìŠ¤íƒ€ì¼ì— ì í•©í•œ ìœ ì—°ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
       "\n",
       "ì´ ì™¸ì—ë„ ì‚¼ì„±ì „ìëŠ” ë‹¤ì–‘í•œ ì¸ë•ì…˜ ëª¨ë¸ì„ í†µí•´ ì†Œë¹„ìë“¤ì˜ í•„ìš”ë¥¼ ì¶©ì¡±ì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤. í•„ìš”ì— ë”°ë¼ ì í•©í•œ ëª¨ë¸ì„ ì„ íƒí•˜ë©´ ìš”ë¦¬ì˜ ì¦ê±°ì›€ì„ í•œì¸µ ë†’ì¼ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”! ğŸ˜Š"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 12.55 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RESULTS_COUNT = 3\n",
    "\n",
    "inputs = [\n",
    "    \"ì‚¼ì„±ì „ì ì œí’ˆ ì¤‘ 2êµ¬ ë§ê³  ë‹¤ë¥¸ ì¸ë•ì…˜ ì¶”ì²œí•´ì¤˜\",\n",
    "    # \"ë¶€ëª¨ë‹˜ì—ê²Œ ì„ ë¬¼í•˜ê³  ì‹¶ì€ë° ì‚¼ì„±ì „ì TV ì¶”ì²œí•´ì¤˜\",\n",
    "    # \"ì‚¼ì„±ì „ì 25ë…„ ì œí’ˆì´ ì‘ë…„ ëŒ€ë¹„ ì¢‹ì•„ì§„ê²ƒì€\",\n",
    "    # \"ì‚¼ì„±ì „ì JBLê³¼ í•˜ë§Œì¹´ëˆ ì°¨ì´ì ì´ ë­ì•¼\",\n",
    "    # \"ê°¤ëŸ­ì‹œ ë²„ì¦ˆ ì´ì–´ë²„ë“œ í•œìª½ì„ ìƒˆë¡œ êµ¬ë§¤í–ˆëŠ”ë° í˜ì–´ë§ ì–´ë–»ê²Œ í•˜ë‚˜ìš”\",\n",
    "    # \"ì‚¼ì„±ì „ì S25 ë¬´ê²Œê°€ S24ì™€ ë¹„êµ í–ˆì„ë•Œ ì–¼ë§ˆë‚˜ ì°¨ì´ë‚˜\"\n",
    "]\n",
    "\n",
    "\n",
    "for input in inputs:\n",
    "    os.environ[\"WEB_SEARCH_MODE\"] = \"google\"  \n",
    "    print(f\"Google Search API ì‚¬ìš©: {input}\")\n",
    "    await process_web_search_call(RESULTS_COUNT, input)\n",
    "\n",
    "    os.environ[\"WEB_SEARCH_MODE\"] = \"bing\"\n",
    "\n",
    "for input in inputs:\n",
    "    print(f\"Bing Grounding ê²€ìƒ‰ ì‚¬ìš©: {input}\")\n",
    "    await process_web_search_call(RESULTS_COUNT, input)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5023d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_agentlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
